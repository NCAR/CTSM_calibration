{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67bb6a6e-2ee0-4a47-be25-6fdd0ff98262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, sys, toml, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from MOASMO_parameters import *\n",
    "from MOASMO_parameter_allbasin_emulator import *\n",
    "\n",
    "path_MOASMO = '/glade/u/home/guoqiang/CTSM_repos/ctsm_optz/MO-ASMO/src/'\n",
    "sys.path.append(path_MOASMO)\n",
    "import gp\n",
    "import NSGA2\n",
    "\n",
    "\n",
    "import os, sys, subprocess, time, toml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from MOASMO_parameter_allbasin_emulator import allbasin_emulator_train_and_optimize, allbasin_emulator_CV_traintest_and_optimize\n",
    "import run_multiple_paramsets_Derecho\n",
    "from multiprocessing import Pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8060f86f-983f-488d-aad4-a59ecd3bdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    infile_basin_info = f\"/glade/work/guoqiang/CTSM_CAMELS/data_mesh_surf/HillslopeHydrology/CAMELS_level1_basin_info.csv\"\n",
    "    infile_param_info = '/glade/u/home/guoqiang/CTSM_repos/CTSM_calibration/src/parameter/CTSM_CAMELS_SA_param_240202.csv'\n",
    "    infile_attr_foruse = '/glade/u/home/guoqiang/CTSM_repos/CTSM_calibration/data/camels_attributes_table_TrainModel.csv'\n",
    "    inpath_moasmo = \"/glade/campaign/cgd/tss/people/guoqiang/CTSM_CAMELS_proj/Calib_HH_MOASMO_bigrange\"\n",
    "    path_CTSM_case = f'/glade/work/guoqiang/CTSM_CAMELS/Calib_HH_MOASMO_bigrange'\n",
    "    ncpus = 36\n",
    "    iterend = 1\n",
    "    outpathname = 'LSE_spaceCV_PredictParam'\n",
    "    suffix = 'LSEspaceCV'\n",
    "    objfunc = 'normKGE'\n",
    "    numruns = 10\n",
    "    target_index = np.arange(627)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caaa7363-9c45-4677-ad78-7b17c39cedfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: /glade/campaign/cgd/tss/people/guoqiang/CTSM_CAMELS_proj/Calib_HH_MOASMO_bigrange/LSE_spaceCV_PredictParam/camels_627basin_attribute.pkl\n",
      "The number of attributes used: 27\n",
      "['mean_elev', 'mean_slope', 'area_gauges2', 'p_mean', 'pet_mean', 'aridity', 'p_seasonality', 'frac_snow', 'high_prec_freq', 'high_prec_dur', 'low_prec_freq', 'low_prec_dur', 'frac_forest', 'lai_max', 'lai_diff', 'dom_land_cover', 'dom_land_cover_frac', 'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity', 'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'carbonate_rocks_frac', 'geol_permeability']\n"
     ]
    }
   ],
   "source": [
    "    cv_num = 5\n",
    "    \n",
    "    outpath = f\"{inpath_moasmo}/{outpathname}\"\n",
    "    # os.makedirs(outpath, exist_ok=True)\n",
    "    \n",
    "    # Load data: same for all iterations\n",
    "    df_basin_info = pd.read_csv(infile_basin_info)\n",
    "    df_basin_info.index = np.arange(len(df_basin_info))\n",
    "    all_index = np.arange(len(df_basin_info))\n",
    "\n",
    "    # # divide into train/test index\n",
    "    # outfile = f'{outpath}/train_test_CV_indices.npz'\n",
    "    # if os.path.isfile(outfile):\n",
    "    #     dtmp = np.load(outfile, allow_pickle=True)\n",
    "    #     train_indices, test_indices = dtmp['train_indices'], dtmp['test_indices']\n",
    "    # else:\n",
    "    #     train_indices = []\n",
    "    #     test_indices = []\n",
    "    #     for i in range(cv_num):\n",
    "    #         ind1 = all_index[i::cv_num]\n",
    "    #         ind2 = np.setdiff1d(all_index, ind1)\n",
    "    #         test_indices.append(ind1)\n",
    "    #         train_indices.append(ind2)\n",
    "    #         print('test index', ind1)\n",
    "    #         print('train index', ind2)\n",
    "    #     # np.savez_compressed(outfile, test_indices=np.array(test_indices, dtype=object), train_indices=np.array(train_indices, dtype=object))\n",
    "    \n",
    "    # # information for all basins\n",
    "    # df_param_info = pd.read_csv(infile_param_info)\n",
    "\n",
    "    # file_defa_param = f'{outpath}/camels_{len(all_index)}basin_ctsm_defa_param.csv'\n",
    "    # df_param_defa = read_allbasin_defa_params(path_CTSM_case, infile_param_info, file_defa_param, all_index)\n",
    "\n",
    "    # file_param_lb = f'{outpath}/camels_{len(all_index)}basin_ctsm_all_param_lb.gz'\n",
    "    # file_param_ub = f'{outpath}/camels_{len(all_index)}basin_ctsm_all_param_ub.gz'\n",
    "    # df_param_lb, df_param_ub = load_basin_param_bounds(inpath_moasmo, df_param_defa, file_param_lb, file_param_ub, all_index, suffix)\n",
    "\n",
    "    file_camels_attribute = f'{outpath}/camels_{len(all_index)}basin_attribute.pkl'\n",
    "    df_att = read_camels_attributes(infile_basin_info, file_camels_attribute, all_index)\n",
    "    \n",
    "    df_att_foruse = pd.read_csv(infile_attr_foruse)\n",
    "    useattrs = list(df_att_foruse[df_att_foruse['att_Xie2021'].values]['Attribute_text'].values)\n",
    "    print(\"The number of attributes used:\", len(useattrs))\n",
    "    print(useattrs)\n",
    "\n",
    "    # # Load data: outputs from each iteration\n",
    "    # for iter in range(0, iterend):\n",
    "    #     file_all_param = f'{outpath}/camels_{len(all_index)}basin_ctsm_all_param_iter{iter}.gz'\n",
    "    #     file_all_metric = f'{outpath}/camels_{len(all_index)}basin_ctsm_all_metric_iter{iter}.gz'\n",
    "    #     file_all_basinid = f'{outpath}/camels_{len(all_index)}basin_ctsm_all_basinid_iter{iter}.gz'\n",
    "        \n",
    "    #     df_param_i, df_metric_i, df_basinid_i = load_all_basin_params_metrics(inpath_moasmo, df_param_defa, df_basin_info, iter, file_all_param, file_all_metric, file_all_basinid, all_index, suffix)\n",
    "        \n",
    "    #     df_basinid_i['iter'] = iter\n",
    "        \n",
    "    #     if iter == 0:\n",
    "    #         df_param = df_param_i\n",
    "    #         df_metric = df_metric_i\n",
    "    #         df_basinid = df_basinid_i\n",
    "    #     else:\n",
    "    #         df_param = pd.concat([df_param, df_param_i])\n",
    "    #         df_metric = pd.concat([df_metric, df_metric_i])\n",
    "    #         df_basinid = pd.concat([df_basinid, df_basinid_i])\n",
    "    \n",
    "    # df_param.index = np.arange(len(df_param))\n",
    "    # df_metric.index = np.arange(len(df_metric))\n",
    "    # df_basinid.index = np.arange(len(df_basinid))\n",
    "\n",
    "    # index = np.isnan(np.sum(df_metric.values, axis=1) + np.sum(df_param.values, axis=1))\n",
    "    # df_param = df_param[~index]\n",
    "    # df_metric = df_metric[~index]\n",
    "    # df_basinid = df_basinid[~index]\n",
    "    \n",
    "    # df_param.index = np.arange(len(df_param))\n",
    "    # df_metric.index = np.arange(len(df_metric))\n",
    "    # df_basinid.index = np.arange(len(df_basinid))\n",
    "    \n",
    "    # print('Number of nan samples:', np.sum(index))\n",
    "    # print(\"Number of original parameter sets:\", len(index))\n",
    "    # print(\"Number of final parameter sets:\", len(df_param))\n",
    "\n",
    "    # # Prepare model input and output\n",
    "    # df_input = df_param.copy()\n",
    "    # df_input[\"hru_id\"] = df_basinid[\"hru_id\"]\n",
    "    # df_input = df_input.merge(df_att[useattrs + [\"hru_id\"]], on=\"hru_id\", how=\"left\")\n",
    "    # df_input = df_input.drop([\"hru_id\"], axis=1)\n",
    "    \n",
    "    # inputnames = list(df_param.columns) + useattrs\n",
    "\n",
    "    # # One-hot encoding for categorical attributes\n",
    "    # for att in useattrs:\n",
    "    #     if df_input[att].dtype == \"object\":\n",
    "    #         print('Convert', att, 'to one-hot encoding')\n",
    "    #         enc = OneHotEncoder(sparse=False)\n",
    "    #         enc.fit(df_input[[att]])\n",
    "    #         encnames = [att + \"_\" + str(i) for i in range(len(enc.categories_[0]))]\n",
    "    #         print('New columns:', encnames)\n",
    "    #         df_enc = pd.DataFrame(enc.transform(df_input[[att]]), columns=encnames)\n",
    "    #         df_input = pd.concat([df_input, df_enc], axis=1)\n",
    "    #         df_input = df_input.drop([att], axis=1)\n",
    "    #         inputnames = [i for i in inputnames if i != att] + encnames\n",
    "\n",
    "    # x_all = df_input[inputnames].values.copy()\n",
    "    # print(\"Input shape:\", x_all.shape)\n",
    "\n",
    "\n",
    "    # # for cvind in range(len(train_indices)):\n",
    "    # for cvind in range(len(train_indices)):\n",
    "    #     train_index = train_indices[cvind]\n",
    "    #     test_index = test_indices[cvind]\n",
    "\n",
    "    #     train_index_allsample = df_basinid['basin_id'].isin(train_index).values\n",
    "    #     test_index_allsample = df_basinid['basin_id'].isin(test_index).values\n",
    "        \n",
    "    #     print('Train/test model')\n",
    "    #     print('Train index:', train_index)\n",
    "    #     print('Test index:', test_index)\n",
    "\n",
    "    #     if objfunc == 'normKGE':\n",
    "    #         print('Use normalized KGE as output')\n",
    "    #         df_output = df_metric.copy()\n",
    "    #         y_all = df_output[[\"kge\"]].values.copy()\n",
    "    #         y_all = y_all / (2 - y_all)  # Normalize KGE\n",
    "        \n",
    "    #         # Train a random forest emulator\n",
    "    #         outfile = f'{outpath}/RF_emulator_for_iter{iterend}_CVFold{cvind}.pkl'\n",
    "    #         outfile_eval = f'{outpath}/RF_emulator_for_iter{iterend}_CVFold{cvind}_eval.npz'\n",
    "    #         if not os.path.isfile(outfile):\n",
    "    #             with open(outfile, 'rb') as file:\n",
    "    #                 em_model = pickle.load(file)\n",
    "    #         else:\n",
    "    #             modelconfig = {'n_estimators': 100, 'random_state': 42, 'max_depth': 40}\n",
    "    #             em_model = RandomForestRegressor(**modelconfig, n_jobs=ncpus)\n",
    "    #             em_model.fit(x_all[train_index_allsample], y_all[train_index_allsample])\n",
    "    #             with open(outfile, 'wb') as file:\n",
    "    #                 pickle.dump(em_model, file)\n",
    "\n",
    "    #             # evaluate em_model on testing samples\n",
    "    #             y_test_pred = em_model.predict(x_all[test_index_allsample])\n",
    "    #             np.savez_compressed(outfile_eval, y_test_pred=y_test_pred, y_test=y_all[test_index_allsample], basin_id=df_basinid['basin_id'].values[test_index_allsample]) \n",
    "    \n",
    "    #     elif not objfunc == 'norm2err':\n",
    "    #         print('Use normalized mae and mmae as output')\n",
    "    #         # normalization is performed for each basin\n",
    "    #         df_output = df_metric.copy()\n",
    "    #         metvalues = df_output[['mae', 'max_mon_abs_err']].values.copy()\n",
    "    #         y_all = np.nan * metvalues\n",
    "    #         for i in range(len(df_basin_info)):\n",
    "    #             indi = df_basinid['basin_id'].values==i\n",
    "    #             di = metvalues[indi, :]\n",
    "    #             di = (di - np.nanmin(di,axis=0) ) / (np.nanmax(di,axis=0) - np.nanmin(di,axis=0) )\n",
    "    #             y_all[indi, :] = di\n",
    "            \n",
    "    #         # Train a random forest emulator\n",
    "    #         outfile = f'{outpath}/RF_emulator_2errOBJfunc_for_iter{iterend}_CVFold{cvind}.pkl'\n",
    "    #         outfile_eval = f'{outpath}/RF_emulator_2errOBJfunc_for_iter{iterend}_CVFold{cvind}_eval.npz'\n",
    "    #         if os.path.isfile(outfile):\n",
    "    #             with open(outfile, 'rb') as file:\n",
    "    #                 em_model = pickle.load(file)\n",
    "    #         else:\n",
    "    #             modelconfig = {'n_estimators': 200, 'random_state': 42, 'max_depth': 40}\n",
    "    #             em_model = RandomForestRegressor(**modelconfig, n_jobs=ncpus)\n",
    "    #             em_model.fit(x_all[train_index_allsample], y_all[train_index_allsample])\n",
    "    #             with open(outfile, 'wb') as file:\n",
    "    #                 pickle.dump(em_model, file)\n",
    "                    \n",
    "    #             # evaluate em_model on testing samples\n",
    "    #             y_test_pred = em_model.predict(x_all[test_index_allsample])\n",
    "    #             np.savez_compressed(outfile_eval, y_test_pred=y_test_pred, y_test=y_all[test_index_allsample], basin_id=df_basinid['basin_id'].values[test_index_allsample]) \n",
    "                \n",
    "        \n",
    "    #     # param_names = df_param_info['Parameter'].values\n",
    "    #     # parallel_process_basins(df_basinid[test_index_allsample], df_param_lb.iloc[test_index], df_param_ub.iloc[test_index], x_all[test_index_allsample], df_input[test_index_allsample], y_all[test_index_allsample], param_names, inputnames, em_model, inpath_moasmo, ncpus, numruns, iterend, test_index, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a423127-d5e5-42c8-a74b-1385a7a0a7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bcd059-a8e5-4e8f-8f58-7d3170857409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f451f1-8c19-4b18-935e-9f51c4207ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_CTSM_case_all = f'/glade/work/guoqiang/CTSM_CAMELS/Calib_HH_MOASMO_bigrange'\n",
    "# iter_end =  1\n",
    "# for tarbasin in target_index:\n",
    "#     config_file = f'{path_CTSM_case_all}/configuration/_level1-{tarbasin}_config_MOASMO.toml'\n",
    "#     config = toml.load(config_file)\n",
    "    \n",
    "#     # inputs\n",
    "#     file_parameter_list = config['file_calib_param']\n",
    "#     path_CTSM_base = config['path_CTSM_case']\n",
    "#     path_script_MOASMO = config['path_script_MOASMO']\n",
    "#     path_CTSM_source = config['path_CTSM_source']\n",
    "#     ref_streamflow = config['file_Qobs']\n",
    "    \n",
    "#     if 'add_flow_file' in config:\n",
    "#         add_flow_file = config['add_flow_file']\n",
    "#     else:\n",
    "#         add_flow_file = 'NA'\n",
    "    \n",
    "#     script_singlerun = f'{path_script_MOASMO}/run_one_paramset_Derecho.py'\n",
    "#     script_clone = f'{path_CTSM_source}/cime/scripts/create_clone'\n",
    "\n",
    "#     if config['path_calib'] == 'NA':\n",
    "#         path_MOASMOcalib = f'{path_CTSM_base}_MOASMOcalib'\n",
    "#     else:\n",
    "#         path_MOASMOcalib = config['path_calib']\n",
    "        \n",
    "#     # outputs\n",
    "#     path_paramset = f'{path_MOASMOcalib}/param_sets_{suffix}'\n",
    "#     path_submit = f'{path_MOASMOcalib}/run_model_{suffix}'\n",
    "#     path_archive = f'{path_MOASMOcalib}/ctsm_outputs_{suffix}'\n",
    "        \n",
    "#     os.makedirs(path_MOASMOcalib, exist_ok=True) \n",
    "    \n",
    "#     # evaluation period\n",
    "#     RUN_STARTDATE = config['RUN_STARTDATE']\n",
    "#     ignore_month = config['ignore_month']\n",
    "#     STOP_OPTION = config['STOP_OPTION']\n",
    "#     STOP_N = config['STOP_N']\n",
    "    \n",
    "#     if 'nonstandard_evaluation' in config:\n",
    "#         nonstandard_evaluation = config['nonstandard_evaluation']\n",
    "#     else:\n",
    "#         nonstandard_evaluation = 'NA'\n",
    "    \n",
    "#     # HPC job settings\n",
    "#     job_mode = config['job_mode']\n",
    "#     job_CTSMiteration = config['job_CTSMiteration']\n",
    "#     # job_controlMOASMO = config['job_controlMOASMO'] # not needed here\n",
    "    \n",
    "#     date_start = (pd.Timestamp(RUN_STARTDATE) + pd.offsets.DateOffset(months=ignore_month)).strftime('%Y-%m-%d') # ignor the first year when evaluating model\n",
    "#     if STOP_OPTION == 'nyears':\n",
    "#         date_end = (pd.Timestamp(RUN_STARTDATE) + pd.offsets.DateOffset(years=STOP_N)).strftime('%Y-%m-%d')\n",
    "#     elif STOP_OPTION == 'nmonths':\n",
    "#         date_end = (pd.Timestamp(RUN_STARTDATE) + pd.offsets.DateOffset(months=STOP_N)).strftime('%Y-%m-%d')\n",
    "#     else:\n",
    "#         sys.exit(f'STOP_OPTION must be nyears or nmonths. {STOP_OPTION} is not accepted.')\n",
    "\n",
    "#     # generate submission commands (note, this won't submit a real job on Derecho)\n",
    "#     run_multiple_paramsets_Derecho.generate_and_submit_multi_CTSM_runs(iter_end, path_submit, path_paramset, path_CTSM_base, \n",
    "#                                                                        path_archive, script_singlerun, script_clone, \n",
    "#                                                                        date_start, date_end, ref_streamflow, add_flow_file,\n",
    "#                                                                        job_CTSMiteration, job_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7122020-cd88-4e9c-a815-254e543133f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:npl-2024a-tgq]",
   "language": "python",
   "name": "conda-env-npl-2024a-tgq-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
