{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90140c5-6a50-456d-b816-88af72bb3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, subprocess, time, toml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from MOASMO_parameter_allbasin_emulator import allbasin_emulator_CV_traintest_and_optimize_2_ann\n",
    "import run_multiple_paramsets_Derecho\n",
    "from multiprocessing import Pool\n",
    "from MOASMO_parameter_allbasin_emulator import *\n",
    "\n",
    "iter_end = 1 # e.g., iter_end=2 means outputs from iter0 and iter1 will be used to generate new paprameters for iter 2\n",
    "ncpus = 5\n",
    "# ncpus = 20\n",
    "# iterend = 1\n",
    "numruns = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "442bd0e6-2fb3-40e1-91fe-82dc5dafb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "only_checkruns = False\n",
    "\n",
    "infile_basin_info = f\"/glade/work/guoqiang/CTSM_CAMELS/data_mesh_surf/HillslopeHydrology/CAMELS_level1_basin_info.csv\"\n",
    "infile_param_info = '/glade/u/home/guoqiang/CTSM_repos/CTSM_calibration/src/parameter/CTSM_CAMELS_calibparam_2410.csv'\n",
    "infile_attr_foruse = '/glade/u/home/guoqiang/CTSM_repos/CTSM_calibration/data/camels_attributes_table_TrainModel.csv'\n",
    "inpath_moasmo = \"/glade/campaign/cgd/tss/people/guoqiang/CTSM_CAMELS_proj/Calib_HH_emulator\"\n",
    "path_CTSM_case_all = f'/glade/work/guoqiang/CTSM_CAMELS/Calib_HH_emulator'\n",
    "\n",
    "# trainmode = 'trainbasin' # allbasin; trainbasin\n",
    "# trainmode = 'allbasin_2err'\n",
    "trainmode = 'spaceCV1'\n",
    "# trainmode = 'allbasin_50iter0'\n",
    "# trainmode = 'allbasin'\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# train on what basins?\n",
    "\n",
    "if trainmode == 'allbasin':\n",
    "    target_index = np.arange(627)\n",
    "    suffix = 'LSEnormKGE'\n",
    "    outpathname = 'LSE_allbasin'\n",
    "    objfunc = 'normKGE'\n",
    "elif trainmode == 'spaceCV1':\n",
    "    test_index = np.arange(0, 627, 5)\n",
    "    train_index = np.setdiff1d(np.arange(627), test_index)\n",
    "    target_index = train_index\n",
    "    outpathname = 'LSE_normNN_normKGECV0'\n",
    "    suffix = 'LSEnormNNnormKGECV0'\n",
    "    objfunc = 'normKGE'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e5b497b-f625-400b-bc3b-15f5fc5ecfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_basin_info, infile_param_info, infile_attr_foruse, inpath_moasmo, outpathname, path_CTSM_case, iterend, ncpus, suffix, train_index, numruns, objfunc = infile_basin_info, infile_param_info, infile_attr_foruse, inpath_moasmo, outpathname, path_CTSM_case_all, iter_end, ncpus, suffix, train_index, numruns, objfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53fccca6-4cd6-4d3d-bddc-53be9795fdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: /glade/campaign/cgd/tss/people/guoqiang/CTSM_CAMELS_proj/Calib_HH_emulator/LSE_normNN_normKGECV0/camels_basin_attribute_train_LSEnormNNnormKGECV0.pkl\n",
      "File exists: /glade/campaign/cgd/tss/people/guoqiang/CTSM_CAMELS_proj/Calib_HH_emulator/LSE_normNN_normKGECV0/camels_basin_attribute_test_LSEnormNNnormKGECV0.pkl\n",
      "The number of attributes used: 27\n",
      "['mean_elev', 'mean_slope', 'area_gauges2', 'p_mean', 'pet_mean', 'aridity', 'p_seasonality', 'frac_snow', 'high_prec_freq', 'high_prec_dur', 'low_prec_freq', 'low_prec_dur', 'frac_forest', 'lai_max', 'lai_diff', 'dom_land_cover', 'dom_land_cover_frac', 'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity', 'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'carbonate_rocks_frac', 'geol_permeability']\n",
      "Number of nan samples: 1079\n",
      "Number of original parameter sets: 100200\n",
      "Number of final parameter sets: 99121\n",
      "Convert dom_land_cover to one-hot encoding\n",
      "New columns: ['dom_land_cover_0', 'dom_land_cover_1', 'dom_land_cover_2', 'dom_land_cover_3', 'dom_land_cover_4', 'dom_land_cover_5', 'dom_land_cover_6', 'dom_land_cover_7', 'dom_land_cover_8', 'dom_land_cover_9', 'dom_land_cover_10', 'dom_land_cover_11']\n",
      "Input shape: (99121, 54)\n",
      "Train/test model\n",
      "Train index: [  1   2   3   4   6   7   8   9  11  12  13  14  16  17  18  19  21  22\n",
      "  23  24  26  27  28  29  31  32  33  34  36  37  38  39  41  42  43  44\n",
      "  46  47  48  49  51  52  53  54  56  57  58  59  61  62  63  64  66  67\n",
      "  68  69  71  72  73  74  76  77  78  79  81  82  83  84  86  87  88  89\n",
      "  91  92  93  94  96  97  98  99 101 102 103 104 106 107 108 109 111 112\n",
      " 113 114 116 117 118 119 121 122 123 124 126 127 128 129 131 132 133 134\n",
      " 136 137 138 139 141 142 143 144 146 147 148 149 151 152 153 154 156 157\n",
      " 158 159 161 162 163 164 166 167 168 169 171 172 173 174 176 177 178 179\n",
      " 181 182 183 184 186 187 188 189 191 192 193 194 196 197 198 199 201 202\n",
      " 203 204 206 207 208 209 211 212 213 214 216 217 218 219 221 222 223 224\n",
      " 226 227 228 229 231 232 233 234 236 237 238 239 241 242 243 244 246 247\n",
      " 248 249 251 252 253 254 256 257 258 259 261 262 263 264 266 267 268 269\n",
      " 271 272 273 274 276 277 278 279 281 282 283 284 286 287 288 289 291 292\n",
      " 293 294 296 297 298 299 301 302 303 304 306 307 308 309 311 312 313 314\n",
      " 316 317 318 319 321 322 323 324 326 327 328 329 331 332 333 334 336 337\n",
      " 338 339 341 342 343 344 346 347 348 349 351 352 353 354 356 357 358 359\n",
      " 361 362 363 364 366 367 368 369 371 372 373 374 376 377 378 379 381 382\n",
      " 383 384 386 387 388 389 391 392 393 394 396 397 398 399 401 402 403 404\n",
      " 406 407 408 409 411 412 413 414 416 417 418 419 421 422 423 424 426 427\n",
      " 428 429 431 432 433 434 436 437 438 439 441 442 443 444 446 447 448 449\n",
      " 451 452 453 454 456 457 458 459 461 462 463 464 466 467 468 469 471 472\n",
      " 473 474 476 477 478 479 481 482 483 484 486 487 488 489 491 492 493 494\n",
      " 496 497 498 499 501 502 503 504 506 507 508 509 511 512 513 514 516 517\n",
      " 518 519 521 522 523 524 526 527 528 529 531 532 533 534 536 537 538 539\n",
      " 541 542 543 544 546 547 548 549 551 552 553 554 556 557 558 559 561 562\n",
      " 563 564 566 567 568 569 571 572 573 574 576 577 578 579 581 582 583 584\n",
      " 586 587 588 589 591 592 593 594 596 597 598 599 601 602 603 604 606 607\n",
      " 608 609 611 612 613 614 616 617 618 619 621 622 623 624 626]\n"
     ]
    }
   ],
   "source": [
    "    outpath = f\"{inpath_moasmo}/{outpathname}\"\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "    # Load data: same for all iterations\n",
    "    df_basin_info = pd.read_csv(infile_basin_info)\n",
    "    df_basin_info.index = np.arange(len(df_basin_info))\n",
    "    all_index = np.arange(len(df_basin_info))\n",
    "\n",
    "    test_index = np.setdiff1d(all_index, train_index)\n",
    "\n",
    "    # information for all basins\n",
    "    df_param_info = pd.read_csv(infile_param_info)\n",
    "\n",
    "    file_defa_param = f'{outpath}/camels_ctsm_defa_param_train_{suffix}.csv'\n",
    "    df_param_defa_train = read_allbasin_defa_params(path_CTSM_case, infile_param_info, file_defa_param, train_index)\n",
    "    file_defa_param = f'{outpath}/camels_ctsm_defa_param_test_{suffix}.csv'\n",
    "    df_param_defa_test = read_allbasin_defa_params(path_CTSM_case, infile_param_info, file_defa_param, test_index)\n",
    "\n",
    "    file_param_lb = f'{outpath}/camels_ctsm_all_param_lb_train_{suffix}.gz'\n",
    "    file_param_ub = f'{outpath}/camels_ctsm_all_param_ub_train_{suffix}.gz'\n",
    "    \n",
    "    df_param_lb_train, df_param_ub_train = load_basin_param_bounds(inpath_moasmo, df_param_defa_train, file_param_lb, file_param_ub, train_index, suffix)\n",
    "    file_param_lb = f'{outpath}/camels_ctsm_all_param_lb_test_{suffix}.gz'\n",
    "    file_param_ub = f'{outpath}/camels_ctsm_all_param_ub_test_{suffix}.gz'\n",
    "    df_param_lb_test, df_param_ub_test = load_basin_param_bounds(inpath_moasmo, df_param_defa_test, file_param_lb, file_param_ub, test_index, suffix)\n",
    "\n",
    "    \n",
    "    file_camels_attribute = f'{outpath}/camels_basin_attribute_train_{suffix}.pkl'\n",
    "    df_att_train = read_camels_attributes(infile_basin_info, file_camels_attribute, train_index)\n",
    "    file_camels_attribute = f'{outpath}/camels_basin_attribute_test_{suffix}.pkl'\n",
    "    df_att_test = read_camels_attributes(infile_basin_info, file_camels_attribute, test_index)\n",
    "\n",
    "    df_att_foruse = pd.read_csv(infile_attr_foruse)\n",
    "    useattrs = list(df_att_foruse[df_att_foruse['att_Xie2021'].values]['Attribute_text'].values)\n",
    "    print(\"The number of attributes used:\", len(useattrs))\n",
    "    print(useattrs)\n",
    "\n",
    "\n",
    "    # Load data: outputs from each iteration from training basins\n",
    "    for iter in range(0, iterend):\n",
    "        file_all_param = f'{outpath}/camels_ctsm_all_param_train_{suffix}_iter{iter}.gz'\n",
    "        file_all_metric = f'{outpath}/camels_ctsm_all_metric_train_{suffix}_iter{iter}.gz'\n",
    "        file_all_basinid = f'{outpath}/camels_ctsm_all_basinid_train_{suffix}_iter{iter}.gz'\n",
    "\n",
    "        df_param_i, df_metric_i, df_basinid_i = load_all_basin_params_metrics(inpath_moasmo, df_param_defa_train,\n",
    "                                                                              df_basin_info, iter, file_all_param,\n",
    "                                                                              file_all_metric, file_all_basinid,\n",
    "                                                                              train_index, suffix)\n",
    "\n",
    "        df_basinid_i['iter'] = iter\n",
    "\n",
    "        if iter == 0:\n",
    "            df_param = df_param_i\n",
    "            df_metric = df_metric_i\n",
    "            df_basinid = df_basinid_i\n",
    "        else:\n",
    "            df_param = pd.concat([df_param, df_param_i])\n",
    "            df_metric = pd.concat([df_metric, df_metric_i])\n",
    "            df_basinid = pd.concat([df_basinid, df_basinid_i])\n",
    "\n",
    "    df_param.index = np.arange(len(df_param))\n",
    "    df_metric.index = np.arange(len(df_metric))\n",
    "    df_basinid.index = np.arange(len(df_basinid))\n",
    "\n",
    "    index = np.isnan(np.sum(df_metric.values, axis=1) + np.sum(df_param.values, axis=1))\n",
    "    df_param = df_param[~index]\n",
    "    df_metric = df_metric[~index]\n",
    "    df_basinid = df_basinid[~index]\n",
    "\n",
    "    df_param.index = np.arange(len(df_param))\n",
    "    df_metric.index = np.arange(len(df_metric))\n",
    "    df_basinid.index = np.arange(len(df_basinid))\n",
    "\n",
    "    print('Number of nan samples:', np.sum(index))\n",
    "    print(\"Number of original parameter sets:\", len(index))\n",
    "    print(\"Number of final parameter sets:\", len(df_param))\n",
    "\n",
    "\n",
    "    # One-hot encoding for categorical attributes\n",
    "    df_att = pd.concat([df_att_train, df_att_test])\n",
    "    df_att.index = np.arange(len(df_att))\n",
    "    df_att_use = df_att[useattrs + [\"hru_id\"]]\n",
    "    for att in useattrs:\n",
    "        if df_att_use[att].dtype == \"object\":\n",
    "            print('Convert', att, 'to one-hot encoding')\n",
    "            enc = OneHotEncoder(sparse_output=False)\n",
    "            enc.fit(df_att_use[[att]])\n",
    "            encnames = [att + \"_\" + str(i) for i in range(len(enc.categories_[0]))]\n",
    "            print('New columns:', encnames)\n",
    "            df_enc = pd.DataFrame(enc.transform(df_att_use[[att]]), columns=encnames)\n",
    "            df_att_use = pd.concat([df_att_use, df_enc], axis=1)\n",
    "            df_att_use = df_att_use.drop([att], axis=1)\n",
    "\n",
    "    df_att_use_train = df_att_use[:len(df_att_train)]\n",
    "    df_att_use_test = df_att_use[len(df_att_train):]\n",
    "    df_att_use_train.index = np.arange(len(df_att_use_train))\n",
    "    df_att_use_test.index = np.arange(len(df_att_use_test))\n",
    "    \n",
    "    useattrs = list(df_att_use_train.columns)\n",
    "    useattrs.remove('hru_id')\n",
    "\n",
    "    # Prepare model input and output\n",
    "    df_input = df_param.copy()\n",
    "    df_input[\"hru_id\"] = df_basinid[\"hru_id\"]\n",
    "    df_input = df_input.merge(df_att_use_train[useattrs + [\"hru_id\"]], on=\"hru_id\", how=\"left\")\n",
    "    df_input = df_input.drop([\"hru_id\"], axis=1)\n",
    "\n",
    "    inputnames = list(df_param.columns) + useattrs\n",
    "    x_all = df_input[inputnames].values.copy()\n",
    "    print(\"Input shape:\", x_all.shape)\n",
    "\n",
    "    print('Train/test model')\n",
    "    print('Train index:', train_index)\n",
    "\n",
    "    # Normalize the features\n",
    "    x_train_mean = np.mean(x_all, axis=0)\n",
    "    x_train_std = np.std(x_all, axis=0)    \n",
    "    x_all_scaled = (x_all - x_train_mean) / x_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4211968-c38c-45b5-bf9d-7fe24f84a39a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PytorchEnv]",
   "language": "python",
   "name": "conda-env-PytorchEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
