{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69bc3ac-6d00-4f61-9b0e-d0fafb8fa3be",
   "metadata": {},
   "source": [
    "# Can we reduce overfitting?\n",
    "An experiment using one basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad96659-aeae-49da-a6ac-0d191f2d5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, glob, pickle, toml, json, pickle, random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "sys.path.append('../../MOASMO_support')\n",
    "from MOASMO_parameter_allbasin_emulator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd09b5b-a13a-43e0-ab22-a191c7131a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data function, taken from ~/CTSM_repos/CTSM_calibration/src/MOASMO_support/MOASMO_parameter_allbasin_emulator.py\n",
    "def load_basin_data():\n",
    "    infile_basin_info = f\"/glade/work/guoqiang/CTSM_CAMELS/data_mesh_surf/HillslopeHydrology/CAMELS_level1_basin_info.csv\"\n",
    "    infile_param_info = '/glade/u/home/guoqiang/CTSM_repos/CTSM_calibration/src/parameter/CTSM_CAMELS_SA_param_240202.csv'\n",
    "    infile_attr_foruse = '/glade/u/home/guoqiang/CTSM_repos/CTSM_calibration/data/camels_attributes_table_TrainModel.csv'\n",
    "    inpath_moasmo = \"/glade/campaign/cgd/tss/people/guoqiang/CTSM_CAMELS_proj/Calib_HH_MOASMO_bigrange\"\n",
    "    path_CTSM_case = f'/glade/work/guoqiang/CTSM_CAMELS/Calib_HH_MOASMO_bigrange'\n",
    "    iterend = 1 # only read data from iter-0\n",
    "\n",
    "    outpath = f\"{inpath_moasmo}/allbasin_emulator\"\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "    \n",
    "    # Load data: same for all iterations\n",
    "    df_basin_info = pd.read_csv(infile_basin_info)\n",
    "    df_param_info = pd.read_csv(infile_param_info)\n",
    "    \n",
    "    file_defa_param = f'{outpath}/camels_627basin_ctsm_defa_param.csv'\n",
    "    df_param_defa = read_allbasin_defa_params(path_CTSM_case, infile_param_info, file_defa_param, len(df_basin_info))\n",
    "\n",
    "    file_param_lb = f'{outpath}/camels_627basin_ctsm_all_param_lb.gz'\n",
    "    file_param_ub = f'{outpath}/camels_627basin_ctsm_all_param_ub.gz'\n",
    "    df_param_lb, df_param_ub = load_basin_param_bounds(inpath_moasmo, df_param_defa, file_param_lb, file_param_ub)\n",
    "\n",
    "    file_camels_attribute = f'{outpath}/camels_627basin_attribute.pkl'\n",
    "    df_att = read_camels_attributes(infile_basin_info, file_camels_attribute)\n",
    "    \n",
    "    df_att_foruse = pd.read_csv(infile_attr_foruse)\n",
    "    useattrs = list(df_att_foruse[df_att_foruse['att_Xie2021'].values]['Attribute_text'].values)\n",
    "    print(\"The number of attributes used:\", len(useattrs))\n",
    "    print(useattrs)\n",
    "\n",
    "    # Load data: outputs from each iteration\n",
    "    for iter in range(0, iterend):\n",
    "        file_all_param = f'{outpath}/camels_627basin_ctsm_all_param_iter{iter}.gz'\n",
    "        file_all_metric = f'{outpath}/camels_627basin_ctsm_all_metric_iter{iter}.gz'\n",
    "        file_all_basinid = f'{outpath}/camels_627basin_ctsm_all_basinid_iter{iter}.gz'\n",
    "        \n",
    "        df_param_i, df_metric_i, df_basinid_i = load_all_basin_params_metrics(inpath_moasmo, df_param_defa, df_basin_info, iter, file_all_param, file_all_metric, file_all_basinid)\n",
    "        \n",
    "        df_basinid_i['iter'] = iter\n",
    "        \n",
    "        if iter == 0:\n",
    "            df_param = df_param_i\n",
    "            df_metric = df_metric_i\n",
    "            df_basinid = df_basinid_i\n",
    "        else:\n",
    "            df_param = pd.concat([df_param, df_param_i])\n",
    "            df_metric = pd.concat([df_metric, df_metric_i])\n",
    "            df_basinid = pd.concat([df_basinid, df_basinid_i])\n",
    "    \n",
    "    df_param.index = np.arange(len(df_param))\n",
    "    df_metric.index = np.arange(len(df_metric))\n",
    "    df_basinid.index = np.arange(len(df_basinid))\n",
    "\n",
    "    index = np.isnan(np.sum(df_metric.values, axis=1))\n",
    "    df_param = df_param[~index]\n",
    "    df_metric = df_metric[~index]\n",
    "    df_basinid = df_basinid[~index]\n",
    "    \n",
    "    df_param.index = np.arange(len(df_param))\n",
    "    df_metric.index = np.arange(len(df_metric))\n",
    "    df_basinid.index = np.arange(len(df_basinid))\n",
    "    \n",
    "    print('Number of nan samples:', np.sum(index))\n",
    "    print(\"Number of original parameter sets:\", len(index))\n",
    "    print(\"Number of final parameter sets:\", len(df_param))\n",
    "\n",
    "    return df_basin_info, df_param_info, df_param_defa, df_param_lb, df_param_ub, df_att, df_att_foruse, df_param, df_metric, df_basinid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02bbd3-a3bd-4238-a4a5-653faf3e8d98",
   "metadata": {},
   "source": [
    "# Load data and save data for model training and comparison\n",
    "Here I just load the outputs from LSE which has summarized outputs from individual basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68038acc-0101-4def-9fa0-43c2ac0f230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: /glade/campaign/cgd/tss/people/guoqiang/CTSM_CAMELS_proj/Calib_HH_MOASMO_bigrange/allbasin_emulator/camels_627basin_attribute.pkl\n",
      "The number of attributes used: 27\n",
      "['mean_elev', 'mean_slope', 'area_gauges2', 'p_mean', 'pet_mean', 'aridity', 'p_seasonality', 'frac_snow', 'high_prec_freq', 'high_prec_dur', 'low_prec_freq', 'low_prec_dur', 'frac_forest', 'lai_max', 'lai_diff', 'dom_land_cover', 'dom_land_cover_frac', 'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity', 'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'carbonate_rocks_frac', 'geol_permeability']\n",
      "Number of nan samples: 3309\n",
      "Number of original parameter sets: 250800\n",
      "Number of final parameter sets: 247491\n",
      "Number of basins: 627\n",
      "Number of all parameters: 27\n",
      "Number of all attributes: 62\n"
     ]
    }
   ],
   "source": [
    "# inpath = '/glade/campaign/cgd/tss/people/guoqiang/CTSM_CAMELS_proj/Calib_HH_MOASMO_bigrange/allbasin_emulator'\n",
    "\n",
    "# file_all_param = f'{inpath}/camels_627basin_ctsm_all_param_iter0.gz'\n",
    "# file_all_metric = f'{inpath}/camels_627basin_ctsm_all_meric_iter0.gz'\n",
    "# file_all_basinid = f'{inpath}/camels_627basin_ctsm_all_basinid_iter0.gz'\n",
    "\n",
    "# df_param = pd.read_csv(file_all_param, compression='gzip')\n",
    "# df_metric = pd.read_csv(file_all_metric, compression='gzip')\n",
    "# df_basinid = pd.read_csv(file_all_basinid, compression='gzip')\n",
    "\n",
    "df_basin_info, df_param_info, df_param_defa, df_param_lb, df_param_ub, df_att, df_att_foruse, df_param, df_metric, df_basinid = load_basin_data()\n",
    "print('Number of basins:', len(df_basin_info))\n",
    "print('Number of all parameters:', len(df_param_info))\n",
    "print('Number of all attributes:', len(df_att.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a53d8fd5-c165-4d98-a73b-5aa22d59431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath_moasmo = '/glade/campaign/cgd/tss/people/guoqiang/CTSM_CAMELS_proj/Calib_HH_MOASMO_bigrange'\n",
    "outpath_all = f'{inpath_moasmo}/LargeSampleEmulator_exps_out'\n",
    "os.makedirs(outpath_all, exist_ok=True)\n",
    "numbasin = len(df_basin_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068ed895-907c-4baf-869b-47d61739f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = 1  # Example: Use 4 CPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8a69a7-c7dc-4f39-8070-9160eb31c57a",
   "metadata": {},
   "source": [
    "# SSE train and CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dca7c5-5f69-48b1-b7a8-1f8954a0f8b8",
   "metadata": {},
   "source": [
    "## Train/Evalute RF model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d3fd63-0363-4ca3-af7e-4dd6dd6b9135",
   "metadata": {},
   "source": [
    "Key Parameters to Adjust to reduce overfitting\n",
    "\n",
    "n_estimators  \n",
    "Definition: The number of trees in the forest.  \n",
    "Adjustment: Increase the number of trees until the model's performance on the validation set stabilizes. More trees generally reduce variance.  \n",
    "\n",
    "max_depth  \n",
    "Definition: The maximum depth of each tree.  \n",
    "Adjustment: Limit the maximum depth of the trees. Shallower trees (smaller max_depth) can prevent the model from becoming too complex, thus reducing overfitting. For example, try setting max_depth=10.  \n",
    "\n",
    "min_samples_split  \n",
    "Definition: The minimum number of samples required to split an internal node.  \n",
    "Adjustment: Increase this value to require more samples at a node before it splits. This can prevent the model from learning overly specific patterns. For example, try setting min_samples_split=10.  \n",
    "\n",
    "min_samples_leaf  \n",
    "Definition: The minimum number of samples required to be at a leaf node.  \n",
    "Adjustment: Increase this value to ensure that each leaf has a sufficient number of samples, which can help prevent overfitting. For example, try setting min_samples_leaf=5.  \n",
    "\n",
    "max_features  \n",
    "Definition: The number of features to consider when looking for the best split.  \n",
    "Adjustment: Reduce this value to limit the number of features considered for each split. This increases the diversity among the trees, helping to reduce overfitting. For example, try setting max_features='sqrt' or max_features=0.3.  \n",
    "\n",
    "bootstrap  \n",
    "Definition: Whether bootstrap samples are used when building trees.  \n",
    "Adjustment: Ensure this is set to True to increase the diversity of the trees by sampling with replacement.  \n",
    "\n",
    "max_samples  \n",
    "Definition: The number of samples to draw from X to train each base estimator.  \n",
    "Adjustment: Set this to a fraction of the total number of samples to increase the diversity of the trees. For example, try setting max_samples=0.8.  \n",
    "\n",
    "Findings: some parameters like min_samples_split and max_samples can reduce overfitting, but they fail to improve the performance in the test period. Therefore, they are not used  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cfe7493d-f816-4c3d-8b3a-8bbee1f61c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05831038255420521 0.02150045959230613\n"
     ]
    }
   ],
   "source": [
    "# Parallel version\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def rf_emulator_cv(x, y, xlb_mean, xub_mean):\n",
    "\n",
    "    random.seed(1234567890)\n",
    "    np.random.seed(1234567890)\n",
    "    \n",
    "    n_splits = 5\n",
    "\n",
    "    cv_results = {}\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True) \n",
    "    kge_scores = np.nan * np.zeros([n_splits, y.shape[1]])\n",
    "\n",
    "\n",
    "    # normalize\n",
    "    x = (x - xlb_mean) / (xub_mean - xlb_mean)\n",
    "\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(kf.split(x), 1):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Initialize and train your GPR model here; adjust parameters as needed\n",
    "        sm = RandomForestRegressor()\n",
    "        sm.fit(x_train, y_train)\n",
    "        \n",
    "        # Store results\n",
    "        cv_results[fold_idx] = {\n",
    "                'train_index': train_index,\n",
    "                'test_index': test_index,\n",
    "                'y_train': np.squeeze(y_train),\n",
    "                'y_test': np.squeeze(y_test),\n",
    "                'y_test_pred': np.squeeze(sm.predict(x_test)),\n",
    "                'y_train_pred': np.squeeze(sm.predict(x_train)),\n",
    "            }\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "def process_basin(i):\n",
    "    indi = df_basinid['basin_id'].values == i\n",
    "    kgei = df_metric[indi]['kge'].values\n",
    "    kgei = kgei / (2 - kgei)\n",
    "    parami = df_param[indi].values\n",
    "\n",
    "    # only select useful params\n",
    "    lbi = df_param_lb.iloc[i].values\n",
    "    ubi = df_param_ub.iloc[i].values\n",
    "    induse = lbi != ubi\n",
    "    parami = parami[:, induse]\n",
    "    lbi = lbi[induse]\n",
    "    ubi = ubi[induse]\n",
    "\n",
    "    metrics_use = kgei[:, np.newaxis]\n",
    "\n",
    "    cv_results = rf_emulator_cv(parami, metrics_use, lbi, ubi)\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "def evaluate_cv(cv_results):\n",
    "    # evaluation\n",
    "    rmse_test = np.nan * np.zeros(len(cv_results))\n",
    "    rmse_train = np.nan * np.zeros(len(cv_results))\n",
    "    cc_test = np.nan * np.zeros(len(cv_results))\n",
    "    cc_train = np.nan * np.zeros(len(cv_results))\n",
    "    \n",
    "    for fold in range(1, len(cv_results)+1):\n",
    "        y_train, y_test, y_train_pred, y_test_pred = cv_results[fold]['y_train'], cv_results[fold]['y_test'], cv_results[fold]['y_train_pred'], cv_results[fold]['y_test_pred']\n",
    "        \n",
    "        # Evaluate the model using \n",
    "        rmse_test[fold - 1] = get_rmse(y_test, y_test_pred)\n",
    "        rmse_train[fold - 1] = get_rmse(y_train, y_train_pred)\n",
    "        cc_test[fold - 1] = get_cc(y_test, y_test_pred)\n",
    "        cc_train[fold - 1] = get_cc(y_train, y_train_pred)\n",
    "            \n",
    "    return rmse_test, rmse_train, cc_test, cc_train\n",
    "\n",
    "\n",
    "def get_rmse(d1, d2):\n",
    "    return ( np.nanmean( (d1-d2)**2 ) ) ** 0.5\n",
    "\n",
    "def get_cc(d1, d2):\n",
    "    ind = ~np.isnan(d1+d2)\n",
    "    return np.corrcoef(d1[ind], d2[ind])[0,1]\n",
    "\n",
    "i = 0\n",
    "cv_results = process_basin(i)\n",
    "rmse_test, rmse_train, cc_test, cc_train = evaluate_cv(cv_results)\n",
    "print(np.mean(rmse_test), np.mean(rmse_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80799d92-2123-4bef-aabd-90a5f268b42f",
   "metadata": {},
   "source": [
    "## Train/Evaluate MLP model\n",
    "\n",
    "Key Parameters to Adjust  \n",
    "\n",
    "hidden_layer_sizes  \n",
    "Definition: The number of neurons in each hidden layer.  \n",
    "Adjustment: Experiment with different sizes and numbers of hidden layers. More neurons and layers can capture more complex patterns but also increase the risk of overfitting. Try smaller sizes first, like (50,), (100,), or (100, 50).  \n",
    "\n",
    "activation  \n",
    "Definition: The activation function for the hidden layers.  \n",
    "Adjustment: Common choices are 'relu', 'tanh', and 'logistic'. 'relu' is often a good starting point. Experiment to see which activation function performs best for your data.  \n",
    "\n",
    "solver  \n",
    "Definition: The optimization algorithm.  \n",
    "Adjustment: Common solvers are 'adam', 'sgd', and 'lbfgs'. 'adam' is a popular choice due to its efficiency and good performance. However, 'sgd' with appropriate learning rate schedules and momentum can also be effective.  \n",
    "\n",
    "alpha  \n",
    "Definition: L2 regularization parameter (also known as weight decay).  \n",
    "Adjustment: Increase alpha to add more regularization, which can help prevent overfitting. Common values to try are 0.0001, 0.001, 0.01, and 0.1.  \n",
    "\n",
    "learning_rate  \n",
    "Definition: The learning rate for weight updates.  \n",
    "Adjustment: Smaller learning rates can improve convergence but may require more epochs. 'constant', 'invscaling', and 'adaptive' are common options. 'adaptive' can adjust the learning rate dynamically based on performance.  \n",
    "\n",
    "learning_rate_init  \n",
    "Definition: Initial learning rate.  \n",
    "Adjustment: Typical values range from 0.001 to 0.1. Start with 0.001 and adjust based on performance.  \n",
    "\n",
    "max_iter  \n",
    "Definition: Maximum number of iterations.  \n",
    "Adjustment: Ensure this is large enough for the model to converge. Values like 200, 500, or 1000 are common starting points.  \n",
    "\n",
    "early_stopping  \n",
    "Definition: Whether to stop training when the validation score is not improving.  \n",
    "Adjustment: Set this to True to enable early stopping, which can prevent overfitting by stopping training once the validation score stops improving.  \n",
    "\n",
    "validation_fraction  \n",
    "Definition: The proportion of training data to set aside as validation set for early stopping.  \n",
    "Adjustment: Typically set to 0.1 (i.e., 10% of the training data).   \n",
    "\n",
    "Additional Techniques  \n",
    "Dropout  \n",
    "Description: A regularization technique where randomly selected neurons are ignored during training.  \n",
    "Implementation: Unfortunately, sklearn's MLP does not support dropout. To use dropout, consider using deep learning libraries like Keras or TensorFlow.  \n",
    "\n",
    "Data Augmentation  \n",
    "Description: Increase the diversity of the training data by applying transformations.  \n",
    "Implementation: Common in image processing, less so in tabular data, but you can still consider generating synthetic samples if appropriate.  \n",
    "\n",
    "\n",
    "Finding: \n",
    "\n",
    "Increase alpha to 0.2 to 0.8 reduces overfitting and improves performance. But overfitting is still large  \n",
    "early_stopping + validation_fraction reduces overfitting but overfitting is still large and the test performance is worse  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e7b2caf-e43b-4044-b18b-c055217a78aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056014210596499955 0.04126194164261151\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parallel version\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def mlp_emulator_cv(x, y, xlb_mean, xub_mean):\n",
    "\n",
    "    random.seed(1234567890)\n",
    "    np.random.seed(1234567890)\n",
    "    \n",
    "    n_splits = 5\n",
    "\n",
    "    cv_results = {}\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True) \n",
    "    kge_scores = np.nan * np.zeros([n_splits, y.shape[1]])\n",
    "\n",
    "\n",
    "    # normalize\n",
    "    x = (x - xlb_mean) / (xub_mean - xlb_mean)\n",
    "\n",
    "    for fold_idx, (train_index, test_index) in enumerate(kf.split(x), 1):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Initialize and train your GPR model here; adjust parameters as needed\n",
    "        sm = make_pipeline(StandardScaler(), MLPRegressor(hidden_layer_sizes=(2000,) ,  alpha=0.9))\n",
    "        sm.fit(x_train, y_train)\n",
    "\n",
    "        # Store results\n",
    "        cv_results[fold_idx] = {\n",
    "                'train_index': train_index,\n",
    "                'test_index': test_index,\n",
    "                'y_train': np.squeeze(y_train),\n",
    "                'y_test': np.squeeze(y_test),\n",
    "                'y_test_pred': np.squeeze(sm.predict(x_test)),\n",
    "                'y_train_pred': np.squeeze(sm.predict(x_train)),\n",
    "            }\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "def process_basin(i):\n",
    "    indi = df_basinid['basin_id'].values == i\n",
    "    kgei = df_metric[indi]['kge'].values\n",
    "    kgei = kgei / (2 - kgei)\n",
    "    parami = df_param[indi].values\n",
    "\n",
    "    # only select useful params\n",
    "    lbi = df_param_lb.iloc[i].values\n",
    "    ubi = df_param_ub.iloc[i].values\n",
    "    induse = lbi != ubi\n",
    "    parami = parami[:, induse]\n",
    "    lbi = lbi[induse]\n",
    "    ubi = ubi[induse]\n",
    "\n",
    "    metrics_use = kgei[:, np.newaxis]\n",
    "\n",
    "    cv_results = mlp_emulator_cv(parami, metrics_use, lbi, ubi)\n",
    "    return cv_results\n",
    "\n",
    "i = 0\n",
    "cv_results = process_basin(i)\n",
    "rmse_test, rmse_train, cc_test, cc_train = evaluate_cv(cv_results)\n",
    "print(np.mean(rmse_test), np.mean(rmse_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60f31a-6302-4d85-b9df-db3fe627f617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:npl-2024a-tgq]",
   "language": "python",
   "name": "conda-env-npl-2024a-tgq-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
