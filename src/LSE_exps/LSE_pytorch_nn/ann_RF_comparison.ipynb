{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0832a8d8-8f46-4e05-a18b-d910d1bbc6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, sys, toml, pickle, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from os import path\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "path_MOASMO = '/glade/u/home/guoqiang/CTSM_repos/ctsm_optz/MO-ASMO/src/'\n",
    "sys.path.append(path_MOASMO)\n",
    "import gp\n",
    "import NSGA2\n",
    "\n",
    "\n",
    "import os, sys, subprocess, time, toml\n",
    "\n",
    "sys.path.append(\"/glade/u/home/mozhgana/mywork/model_calibration/src/moasmo_test/\")\n",
    "sys.path.append(\"/glade/u/home/mozhgana/mywork/model_calibration/src/moasmo_test/allbasin_emulator\")\n",
    "from MOASMO_parameter_allbasin_emulator import *\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "sys.path.append(\"/glade/u/home/guoqiang/CTSM_repos/ctsm_optz/MO-ASMO/src\")\n",
    "# import NSGA2\n",
    "\n",
    "\n",
    "from MOASMO_parameters import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ef008-1b20-4c54-8ee9-4a52bff0a9db",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "328a96ac-d5de-4773-9631-9fd01ff304ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_basin_info = '/glade/work/guoqiang/CTSM_CAMELS/data_mesh_surf/HillslopeHydrology/CAMELS_level1_basin_info.csv'\n",
    "infile_param_info = '/glade/u/home/mozhgana/mywork/model_calibration/src/moasmo_test/param_file_tpl.csv'\n",
    "infile_attr_foruse = '/glade/u/home/guoqiang/CTSM_repos/CTSM_calibration/data/camels_attributes_table_TrainModel.csv'\n",
    "inpath_moasmo = '/glade/campaign/cgd/tss/people/mozhgana/projects/SUMMA_Calib'\n",
    "\n",
    "CV=1\n",
    "ncpus = 1\n",
    "iterend = 1\n",
    "outpathname = 'LSE_spaceCV_PredictParam_4X_ann'\n",
    "suffix = f'LSEspaceCV_ann_{CV}'\n",
    "objfunc = 'normKGE'\n",
    "numruns = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca70c71-7449-4117-8871-597faba71f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV=1\n",
    "outpath = '/glade/campaign/cgd/tss/people/mozhgana/projects/SUMMA_Calib/LSE_spaceCV_PredictParam'\n",
    "# divide into train/test index\n",
    "outfile = f'{outpath}/train_test_CV_indices.npz'\n",
    "if os.path.isfile(outfile):\n",
    "    dtmp = np.load(outfile, allow_pickle=True)\n",
    "    train_indices, test_indices = dtmp['train_indices'], dtmp['test_indices']\n",
    "\n",
    "train_index= train_indices[CV-1]\n",
    "test_index= test_indices[CV-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0fd2e9f-af47-4d5e-93a0-c3c31763494e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: /glade/campaign/cgd/tss/people/mozhgana/projects/SUMMA_Calib/LSE_spaceCV_PredictParam_4X_ann/camels_basin_attribute_train_LSEspaceCV_ann_1.pkl\n",
      "File exists: /glade/campaign/cgd/tss/people/mozhgana/projects/SUMMA_Calib/LSE_spaceCV_PredictParam_4X_ann/camels_basin_attribute_test_LSEspaceCV_ann_1.pkl\n",
      "The number of attributes used: 27\n",
      "['mean_elev', 'mean_slope', 'area_gauges2', 'p_mean', 'pet_mean', 'aridity', 'p_seasonality', 'frac_snow', 'high_prec_freq', 'high_prec_dur', 'low_prec_freq', 'low_prec_dur', 'frac_forest', 'lai_max', 'lai_diff', 'dom_land_cover', 'dom_land_cover_frac', 'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity', 'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'carbonate_rocks_frac', 'geol_permeability']\n",
      "Number of nan samples: 406\n",
      "Number of original parameter sets: 200400\n",
      "Number of final parameter sets: 199994\n",
      "Convert dom_land_cover to one-hot encoding\n",
      "New columns: ['dom_land_cover_0', 'dom_land_cover_1', 'dom_land_cover_2', 'dom_land_cover_3', 'dom_land_cover_4', 'dom_land_cover_5', 'dom_land_cover_6', 'dom_land_cover_7', 'dom_land_cover_8', 'dom_land_cover_9', 'dom_land_cover_10', 'dom_land_cover_11']\n",
      "Input shape: (199994, 52)\n",
      "Train/test model\n",
      "Train index: [  1   2   3   4   6   7   8   9  11  12  13  14  16  17  18  19  21  22\n",
      "  23  24  26  27  28  29  31  32  33  34  36  37  38  39  41  42  43  44\n",
      "  46  47  48  49  51  52  53  54  56  57  58  59  61  62  63  64  66  67\n",
      "  68  69  71  72  73  74  76  77  78  79  81  82  83  84  86  87  88  89\n",
      "  91  92  93  94  96  97  98  99 101 102 103 104 106 107 108 109 111 112\n",
      " 113 114 116 117 118 119 121 122 123 124 126 127 128 129 131 132 133 134\n",
      " 136 137 138 139 141 142 143 144 146 147 148 149 151 152 153 154 156 157\n",
      " 158 159 161 162 163 164 166 167 168 169 171 172 173 174 176 177 178 179\n",
      " 181 182 183 184 186 187 188 189 191 192 193 194 196 197 198 199 201 202\n",
      " 203 204 206 207 208 209 211 212 213 214 216 217 218 219 221 222 223 224\n",
      " 226 227 228 229 231 232 233 234 236 237 238 239 241 242 243 244 246 247\n",
      " 248 249 251 252 253 254 256 257 258 259 261 262 263 264 266 267 268 269\n",
      " 271 272 273 274 276 277 278 279 281 282 283 284 286 287 288 289 291 292\n",
      " 293 294 296 297 298 299 301 302 303 304 306 307 308 309 311 312 313 314\n",
      " 316 317 318 319 321 322 323 324 326 327 328 329 331 332 333 334 336 337\n",
      " 338 339 341 342 343 344 346 347 348 349 351 352 353 354 356 357 358 359\n",
      " 361 362 363 364 366 367 368 369 371 372 373 374 376 377 378 379 381 382\n",
      " 383 384 386 387 388 389 391 392 393 394 396 397 398 399 401 402 403 404\n",
      " 406 407 408 409 411 412 413 414 416 417 418 419 421 422 423 424 426 427\n",
      " 428 429 431 432 433 434 436 437 438 439 441 442 443 444 446 447 448 449\n",
      " 451 452 453 454 456 457 458 459 461 462 463 464 466 467 468 469 471 472\n",
      " 473 474 476 477 478 479 481 482 483 484 486 487 488 489 491 492 493 494\n",
      " 496 497 498 499 501 502 503 504 506 507 508 509 511 512 513 514 516 517\n",
      " 518 519 521 522 523 524 526 527 528 529 531 532 533 534 536 537 538 539\n",
      " 541 542 543 544 546 547 548 549 551 552 553 554 556 557 558 559 561 562\n",
      " 563 564 566 567 568 569 571 572 573 574 576 577 578 579 581 582 583 584\n",
      " 586 587 588 589 591 592 593 594 596 597 598 599 601 602 603 604 606 607\n",
      " 608 609 611 612 613 614 616 617 618 619 621 622 623 624 626]\n",
      "Use normalized KGE as output\n"
     ]
    }
   ],
   "source": [
    "suffix_defa_source = 'LSEnormKGE'\n",
    "\n",
    "outpath = f\"{inpath_moasmo}/{outpathname}\"\n",
    "os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "# Load data: same for all iterations\n",
    "df_basin_info = pd.read_csv(infile_basin_info)\n",
    "df_basin_info.index = np.arange(len(df_basin_info))\n",
    "all_index = np.arange(len(df_basin_info))\n",
    "\n",
    "test_index = np.setdiff1d(all_index, train_index)\n",
    "\n",
    "# information for all basins\n",
    "df_param_info = pd.read_csv(infile_param_info)\n",
    "\n",
    "file_defa_param = f'{outpath}/camels_summa_defa_param_train_{suffix}.csv'\n",
    "df_param_defa_train = read_allbasin_defa_params(infile_param_info, file_defa_param, Basin_list, train_index)\n",
    "\n",
    "file_defa_param = f'{outpath}/camels_summa_defa_param_test_{suffix}.csv'\n",
    "df_param_defa_test = read_allbasin_defa_params(infile_param_info, file_defa_param, Basin_list, test_index)\n",
    "\n",
    "file_param_lb = f'{outpath}/camels_summa_all_param_lb_train_{suffix}.gz'\n",
    "file_param_ub = f'{outpath}/camels_summa_all_param_ub_train_{suffix}.gz'\n",
    "\n",
    "df_param_lb_train, df_param_ub_train = load_basin_param_bounds(inpath_moasmo, df_param_defa_train, file_param_lb, file_param_ub)\n",
    "\n",
    "file_param_lb = f'{outpath}/camels_summa_all_param_lb_test_{suffix}.gz'\n",
    "file_param_ub = f'{outpath}/camels_summa_all_param_ub_test_{suffix}.gz'\n",
    "df_param_lb_test, df_param_ub_test = load_basin_param_bounds(inpath_moasmo, df_param_defa_test, file_param_lb, file_param_ub)\n",
    "\n",
    "\n",
    "file_camels_attribute = f'{outpath}/camels_basin_attribute_train_{suffix}.pkl'\n",
    "df_att_train = read_camels_attributes(infile_basin_info, file_camels_attribute, train_index)\n",
    "file_camels_attribute = f'{outpath}/camels_basin_attribute_test_{suffix}.pkl'\n",
    "df_att_test = read_camels_attributes(infile_basin_info, file_camels_attribute, test_index)\n",
    "\n",
    "df_att_foruse = pd.read_csv(infile_attr_foruse)\n",
    "useattrs = list(df_att_foruse[df_att_foruse['att_Xie2021'].values]['Attribute_text'].values)\n",
    "print(\"The number of attributes used:\", len(useattrs))\n",
    "print(useattrs)\n",
    "\n",
    "\n",
    "suffixtest = suffix+'test'\n",
    "\n",
    "# Load data: outputs from each iteration from training basins\n",
    "for iter in range(0, iterend):\n",
    "    file_all_param = f'{outpath}/camels_summa_all_param_train_{suffix}_iter{iter}.gz'\n",
    "    file_all_metric = f'{outpath}/camels_summa_all_metric_train_{suffix}_iter{iter}.gz'\n",
    "    file_all_basinid = f'{outpath}/camels_summa_all_basinid_train_{suffix}_iter{iter}.gz'\n",
    "\n",
    "    file_all_param_test = f'{outpath}/camels_summa_all_param_test_{suffix}_iter{iter}.gz'\n",
    "    file_all_metric_test = f'{outpath}/camels_summa_all_metric_test_{suffix}_iter{iter}.gz'\n",
    "    file_all_basinid_test = f'{outpath}/camels_summa_all_basinid_test_{suffix}_iter{iter}.gz'\n",
    "\n",
    "    if iter == 0:\n",
    "\n",
    "        df_param_i, df_metric_i, df_basinid_i = load_all_basin_params_metrics(inpath_moasmo, infile_param_info, df_param_defa_train,\n",
    "                                                                              df_basin_info, iter, file_all_param,\n",
    "                                                                              file_all_metric, file_all_basinid,\n",
    "                                                                              train_index, suffix_defa_source)\n",
    "    \n",
    "        df_param_i_test, df_metric_i_test, df_basinid_i_test = load_all_basin_params_metrics(inpath_moasmo, infile_param_info, df_param_defa_test,\n",
    "                                                                              df_basin_info, iter, file_all_param_test,\n",
    "                                                                              file_all_metric_test, file_all_basinid_test,\n",
    "                                                                              test_index, suffix_defa_source)\n",
    "    else:\n",
    "        df_param_i, df_metric_i, df_basinid_i = load_all_basin_params_metrics(inpath_moasmo, infile_param_info, df_param_defa_train,\n",
    "                                                                              df_basin_info, iter, file_all_param,\n",
    "                                                                              file_all_metric, file_all_basinid,\n",
    "                                                                              train_index, suffix)\n",
    "    \n",
    "        df_param_i_test, df_metric_i_test, df_basinid_i_test = load_all_basin_params_metrics(inpath_moasmo, infile_param_info, df_param_defa_test,\n",
    "                                                                              df_basin_info, iter, file_all_param_test,\n",
    "                                                                              file_all_metric_test, file_all_basinid_test,\n",
    "                                                                              test_index, suffixtest)\n",
    "\n",
    "    df_basinid_i['iter'] = iter\n",
    "    df_basinid_i_test['iter'] = iter\n",
    "\n",
    "    if iter == 0:\n",
    "        df_param = df_param_i\n",
    "        df_metric = df_metric_i\n",
    "        df_basinid = df_basinid_i\n",
    "        \n",
    "        df_param_test = df_param_i_test\n",
    "        df_metric_test = df_metric_i_test\n",
    "        df_basinid_test = df_basinid_i_test\n",
    "    else:\n",
    "        df_param = pd.concat([df_param, df_param_i])\n",
    "        df_metric = pd.concat([df_metric, df_metric_i])\n",
    "        df_basinid = pd.concat([df_basinid, df_basinid_i])\n",
    "        \n",
    "        df_param_test = pd.concat([df_param_test, df_param_i_test])\n",
    "        df_metric_test = pd.concat([df_metric_test, df_metric_i_test])        \n",
    "        df_basinid_test = pd.concat([df_basinid_test, df_basinid_i_test])\n",
    "\n",
    "df_param = df_param.apply(pd.to_numeric, errors='coerce')\n",
    "df_param_test = df_param_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "df_param.index = np.arange(len(df_param))\n",
    "df_metric.index = np.arange(len(df_metric))\n",
    "df_basinid.index = np.arange(len(df_basinid))\n",
    "\n",
    "df_param_test.index = np.arange(len(df_param_test))\n",
    "df_metric_test.index = np.arange(len(df_metric_test))\n",
    "df_basinid_test.index = np.arange(len(df_basinid_test))\n",
    "\n",
    "\n",
    "index = np.isnan(np.sum(df_metric.values, axis=1) + np.sum(df_param.values, axis=1))\n",
    "df_param = df_param[~index]\n",
    "df_metric = df_metric[~index]\n",
    "df_basinid = df_basinid[~index]\n",
    "\n",
    "index_test = np.isnan(np.sum(df_metric_test.values, axis=1) + np.sum(df_param_test.values, axis=1))\n",
    "df_param_test = df_param_test[~index_test]\n",
    "df_metric_test = df_metric_test[~index_test]\n",
    "df_basinid_test = df_basinid_test[~index_test]\n",
    "\n",
    "\n",
    "df_param.index = np.arange(len(df_param))\n",
    "df_metric.index = np.arange(len(df_metric))\n",
    "df_basinid.index = np.arange(len(df_basinid))\n",
    "\n",
    "df_param_test.index = np.arange(len(df_param_test))\n",
    "df_metric_test.index = np.arange(len(df_metric_test))\n",
    "df_basinid_test.index = np.arange(len(df_basinid_test))\n",
    "\n",
    "\n",
    "print('Number of nan samples:', np.sum(index))\n",
    "print(\"Number of original parameter sets:\", len(index))\n",
    "print(\"Number of final parameter sets:\", len(df_param))\n",
    "\n",
    "\n",
    "# One-hot encoding for categorical attributes\n",
    "df_att = pd.concat([df_att_train, df_att_test])\n",
    "df_att.index = np.arange(len(df_att))\n",
    "df_att_use = df_att[useattrs + [\"hru_id\"]]\n",
    "\n",
    "for att in useattrs:\n",
    "    if df_att_use[att].dtype == \"object\":\n",
    "        print('Convert', att, 'to one-hot encoding')\n",
    "        enc = OneHotEncoder(sparse_output=False)\n",
    "        enc.fit(df_att_use[[att]])\n",
    "        encnames = [att + \"_\" + str(i) for i in range(len(enc.categories_[0]))]\n",
    "        print('New columns:', encnames)\n",
    "        df_enc = pd.DataFrame(enc.transform(df_att_use[[att]]), columns=encnames)\n",
    "        df_att_use = pd.concat([df_att_use, df_enc], axis=1)\n",
    "        df_att_use = df_att_use.drop([att], axis=1)\n",
    "\n",
    "df_att_use_train = df_att_use[:len(df_att_train)]\n",
    "df_att_use_test = df_att_use[len(df_att_train):]\n",
    "df_att_use_train.index = np.arange(len(df_att_use_train))\n",
    "df_att_use_test.index = np.arange(len(df_att_use_test))\n",
    "\n",
    "useattrs = list(df_att_use_train.columns)\n",
    "useattrs.remove('hru_id')\n",
    "\n",
    "# Prepare model input and output\n",
    "df_input = df_param.copy()\n",
    "df_input[\"hru_id\"] = df_basinid[\"basin_name\"]\n",
    "df_input = df_input.merge(df_att_use_train[useattrs + [\"hru_id\"]], on=\"hru_id\", how=\"left\")\n",
    "df_input = df_input.drop([\"hru_id\"], axis=1)\n",
    "\n",
    "inputnames = list(df_param.columns) + useattrs\n",
    "x_all = df_input[inputnames].values.copy()\n",
    "print(\"Input shape:\", x_all.shape)\n",
    "\n",
    "print('Train/test model')\n",
    "print('Train index:', train_index)\n",
    "\n",
    "# divide samples into training and validation sets (70% vs 30%)\n",
    "hru_idu = np.unique(df_basinid[\"basin_name\"].values)\n",
    "index_val_tmp = np.linspace(0, len(hru_idu)-1, int(len(hru_idu) * 0.3 )).astype(int)\n",
    "index_train_tmp = np.setdiff1d(np.arange(len(hru_idu)), index_val_tmp)\n",
    "hru_idu_train = hru_idu[index_train_tmp]\n",
    "hru_idu_val = hru_idu[index_val_tmp]\n",
    "\n",
    "index_train = df_basinid[\"basin_name\"].isin(hru_idu_train)\n",
    "index_val = df_basinid[\"basin_name\"].isin(hru_idu_val)\n",
    "\n",
    "x_train, x_val = x_all[index_train, :], x_all[index_val, :]\n",
    "\n",
    "# Normalize the features\n",
    "# x_train_mean = np.mean(x_train, axis=0)\n",
    "# x_train_std = np.std(x_train, axis=0)\n",
    "x_train_mean = np.mean(x_all, axis=0)\n",
    "x_train_std = np.std(x_all, axis=0)\n",
    "x_train_scaled = (x_train - x_train_mean) / x_train_std\n",
    "x_val_scaled = (x_val - x_train_mean) / x_train_std\n",
    "\n",
    "\n",
    "if objfunc == 'normKGE':\n",
    "    print('Use normalized KGE as output')\n",
    "    df_output = df_metric.copy()\n",
    "    y_all = df_output[[\"kge\"]].values.copy()\n",
    "    y_all = y_all / (2 - y_all)  # Normalize KGE\n",
    "    y_train, y_val = y_all[index_train], y_all[index_val]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df885066-cb6a-4091-baa1-f9c6ba16475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath_all = '/glade/derecho/scratch/guoqiang/LSEexps'\n",
    "num_cpus = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05813a-777a-46ee-aa21-4f71de72c835",
   "metadata": {},
   "source": [
    "# spatial cv index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870b75e2-5a41-4898-89de-78c82ebb9edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import GroupKFold,  KFold\n",
    "def get_group_cv_indices(df_basinid, nfold=5, rndseed=1234567890):\n",
    "\n",
    "    random.seed(rndseed)\n",
    "    np.random.seed(rndseed)\n",
    "\n",
    "    # Initialize GroupKFold\n",
    "    group_kfold = GroupKFold(n_splits=nfold)\n",
    "\n",
    "    # Dictionary to store train and test indices\n",
    "    group_cv_indices = {}\n",
    "\n",
    "    # Cross-validation process to save indices\n",
    "    # does not depend on random seeds\n",
    "    for fold, (train_index, test_index) in enumerate(group_kfold.split(df_basinid, groups=df_basinid['basin_num'])):\n",
    "        \n",
    "        train_basin = np.unique(df_basinid.iloc[train_index]['basin_id'].values)\n",
    "        test_basin = np.unique(df_basinid.iloc[test_index]['basin_id'].values)\n",
    "\n",
    "        group_cv_indices[fold] = {'train_index': train_index, 'test_index': test_index, 'train_basin': train_basin, 'test_basin': test_basin} \n",
    "    return group_cv_indices\n",
    "\n",
    "# Get the indices\n",
    "nfold = 5\n",
    "group_cv_indices = get_group_cv_indices(df_basinid, nfold)\n",
    "\n",
    "# plot test basin batches\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "for fold in range(len(group_cv_indices)):\n",
    "    test_basin = group_cv_indices[fold]['test_basin']\n",
    "    lat = df_basin_info.iloc[test_basin]['lat_cen'].values\n",
    "    lon = df_basin_info.iloc[test_basin]['lon_cen'].values\n",
    "    plt.scatter(lon, lat, 15, label=f'fold {fold}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7e00a-14c9-49ff-9a60-d2b86e5264cf",
   "metadata": {},
   "source": [
    "# Sklearn Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "febb9438-be4b-4bd1-897c-04c5f40ef1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation functions\n",
    "# evaluate\n",
    "\n",
    "def evaluate_cv(cv_results):\n",
    "    # evaluation\n",
    "    rmse_test = np.nan * np.zeros(len(cv_results))\n",
    "    rmse_train = np.nan * np.zeros(len(cv_results))\n",
    "    cc_test = np.nan * np.zeros(len(cv_results))\n",
    "    cc_train = np.nan * np.zeros(len(cv_results))\n",
    "    \n",
    "    for fold in range(len(cv_results)):\n",
    "        y_train, y_test, y_train_pred, y_test_pred = cv_results[fold]['y_train'], cv_results[fold]['y_test'], cv_results[fold]['y_train_pred'], cv_results[fold]['y_test_pred']\n",
    "        \n",
    "        # Evaluate the model using \n",
    "        rmse_test[fold] = get_rmse(y_test, y_test_pred)\n",
    "        rmse_train[fold] = get_rmse(y_train, y_train_pred)\n",
    "        cc_test[fold] = get_cc(y_test, y_test_pred)\n",
    "        cc_train[fold] = get_cc(y_train, y_train_pred)\n",
    "            \n",
    "    return rmse_test, rmse_train, cc_test, cc_train\n",
    "\n",
    "\n",
    "def get_rmse(d1, d2):\n",
    "    d1, d2 = np.squeeze(d1), np.squeeze(d2)\n",
    "    return ( np.nanmean( (d1-d2)**2 ) ) ** 0.5\n",
    "\n",
    "def get_cc(d1, d2):\n",
    "    d1, d2 = np.squeeze(d1), np.squeeze(d2)\n",
    "    ind = ~np.isnan(d1+d2)\n",
    "    return np.corrcoef(d1[ind], d2[ind])[0,1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a03ebb5-ce6f-456b-ad1b-1732eb789794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def rf_run_cross_validation_parallel(cv_indices, x_all, y_all, ncpus, modelconfig={'n_estimators': 100, 'random_state': 42}):\n",
    "\n",
    "    if True:\n",
    "\n",
    "        rf_cv_results = {}\n",
    "        \n",
    "        # Cross-validation loop\n",
    "        for fold, indices in cv_indices.items():\n",
    "        \n",
    "            # print(f\"RF Fold {fold}\")\n",
    "        \n",
    "            train_index = indices['train_index']\n",
    "            test_index = indices['test_index']\n",
    "            \n",
    "            x_train, x_test = x_all[train_index], x_all[test_index]\n",
    "            y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "            \n",
    "            # Initialize a new RandomForestRegressor for each fold\n",
    "            rf_model = RandomForestRegressor(**modelconfig, n_jobs=ncpus)\n",
    "            \n",
    "            # Train the model\n",
    "            rf_model.fit(x_train, y_train)\n",
    "        \n",
    "            # Predict on the test set\n",
    "            y_test_pred = rf_model.predict(x_test)\n",
    "            y_train_pred = rf_model.predict(x_train)\n",
    "\n",
    "            if y_test_pred.ndim == 1 and y_test.ndim == 2:\n",
    "                y_test_pred = y_test_pred[:, np.newaxis]\n",
    "                y_train_pred = y_train_pred[:, np.newaxis]\n",
    "            \n",
    "            \n",
    "            # Store results\n",
    "            rf_cv_results[fold] = {\n",
    "                # 'model': rf_model,\n",
    "                'train_index': train_index,\n",
    "                'test_index': test_index,\n",
    "                'y_train': y_train,\n",
    "                'y_test': y_test,\n",
    "                'y_test_pred': y_test_pred,\n",
    "                'y_train_pred': y_train_pred,\n",
    "            }\n",
    "        \n",
    "    return rf_cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4feb176-df7d-412f-b057-477e6deb6441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outfile = f'{outpath_all}/LSE_RF_normKGE_groupCV_estimates.pkl'\n",
    "\n",
    "if os.path.isfile(outfile):\n",
    "    with open(outfile, 'rb') as file:\n",
    "        rf_group_cv_results = pickle.load(file)    \n",
    "else:\n",
    "    modelconfig = {'n_estimators': 100, 'random_state': 42, 'max_depth': 40}\n",
    "    rf_group_cv_results = rf_run_cross_validation_parallel(group_cv_indices, x_all, y_all, num_cpus, modelconfig)\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(rf_group_cv_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8854ad45-41fa-4455-8f8b-8b13fd76463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24391602268490745 0.020253727711481387 0.6810679989599152 0.9981226722210224\n"
     ]
    }
   ],
   "source": [
    "outfile = f'{outpath_all}/LSE_RF_normKGE_groupCV_evaluation.npz'\n",
    "\n",
    "if os.path.isfile(outfile):\n",
    "    dtmp = np.load(outfile)\n",
    "    rf_group_rmse_train = dtmp['rf_group_rmse_train']\n",
    "    rf_group_rmse_test = dtmp['rf_group_rmse_test']\n",
    "    rf_group_cc_train = dtmp['rf_group_cc_train']\n",
    "    rf_group_cc_test = dtmp['rf_group_cc_test']\n",
    "\n",
    "else:\n",
    "    rf_group_rmse_test, rf_group_rmse_train, rf_group_cc_test, rf_group_cc_train = evaluate_cv(rf_group_cv_results)\n",
    "    np.savez_compressed(outfile, rf_group_rmse_train=rf_group_rmse_train, rf_group_rmse_test=rf_group_rmse_test, rf_group_cc_train=rf_group_cc_train, rf_group_cc_test=rf_group_cc_test)\n",
    "\n",
    "print(np.mean(rf_group_rmse_test), np.mean(rf_group_rmse_train), np.mean(rf_group_cc_test), np.mean(rf_group_cc_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ac616-015b-4d99-b142-6f491368d69e",
   "metadata": {},
   "source": [
    "# Sklearn ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af9b81be-73a6-4536-9103-3fab7b9c539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bpnn_train_and_evaluate_fold(args):\n",
    "    fold, indices, x_all, y_all, modelconfig = args\n",
    "    \n",
    "    train_index = indices['train_index']\n",
    "    test_index = indices['test_index']\n",
    "    \n",
    "    x_train, x_test = x_all[train_index], x_all[test_index]\n",
    "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "    \n",
    "    # Initialize scalers\n",
    "    x_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "\n",
    "    # Normalize the input features\n",
    "    x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "    x_test_scaled = x_scaler.transform(x_test)\n",
    "    \n",
    "    # Normalize the target values\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Initialize a new MLPRegressor for each fold\n",
    "    bp_model = MLPRegressor(**modelconfig)\n",
    "    \n",
    "    # Train the model\n",
    "    bp_model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_test_pred_scaled = bp_model.predict(x_test_scaled)\n",
    "    y_train_pred_scaled = bp_model.predict(x_train_scaled)\n",
    "\n",
    "    # Inverse transform the predicted outputs\n",
    "    y_test_pred = y_scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_train_pred = y_scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    if y_test_pred.ndim == 1 and y_test.ndim == 2:\n",
    "        y_test_pred = y_test_pred[:, np.newaxis]\n",
    "        y_train_pred = y_train_pred[:, np.newaxis]\n",
    "    \n",
    "\n",
    "    # Store results\n",
    "    fold_result = {\n",
    "        'train_index': train_index,\n",
    "        'test_index': test_index,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'y_test_pred': y_test_pred,\n",
    "        'y_train_pred': y_train_pred,\n",
    "    }\n",
    "\n",
    "    return fold, fold_result\n",
    "\n",
    "\n",
    "\n",
    "def bpnn_run_cross_validation_parallel(cv_indices, x_all, y_all, ncpus, modelconfig={'hidden_layer_sizes': (100,), 'max_iter': 2000, 'alpha': 0.001, 'random_state': 42, 'early_stopping': True}):\n",
    "\n",
    "    with Pool(processes=ncpus) as pool:\n",
    "        # Prepare arguments for each fold\n",
    "        args = [(fold, indices, x_all, y_all, modelconfig) for fold, indices in cv_indices.items()]\n",
    "            \n",
    "        # Parallelize the cross-validation loop\n",
    "        results = pool.map(bpnn_train_and_evaluate_fold, args)\n",
    "        \n",
    "        # Collect results\n",
    "        ann_cv_results = {fold: result for fold, result in results}\n",
    "\n",
    "    return ann_cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "170b0d9d-1a3e-45fb-bd5b-54c49aa83fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26663739529229324 0.05850480211377039 0.6875870176010697 0.9838342425395421\n"
     ]
    }
   ],
   "source": [
    "outfile = f'{outpath_all}/LSE_MLP_normKGE_groupCV_estimates.pkl'\n",
    "\n",
    "if os.path.isfile(outfile):\n",
    "    with open(outfile, 'rb') as file:\n",
    "        mlp_group_cv_results = pickle.load(file)\n",
    "else:\n",
    "    modelconfig = {'hidden_layer_sizes': (200,), 'max_iter': 1000, 'alpha': 0.0001, 'random_state': 42, 'early_stopping': True, 'validation_fraction': 0.2}\n",
    "    mlp_group_cv_results = bpnn_run_cross_validation_parallel(group_cv_indices, x_all, y_all, 5, modelconfig)\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_group_cv_results, file)\n",
    "\n",
    "outfile = f'{outpath_all}/LSE_MLP_normKGE_groupCV_evaluation.npz'\n",
    "\n",
    "if os.path.isfile(outfile):\n",
    "    dtmp = np.load(outfile)\n",
    "    mlp_group_rmse_train = dtmp['mlp_group_rmse_train']\n",
    "    mlp_group_rmse_test = dtmp['mlp_group_rmse_test']\n",
    "    mlp_group_cc_train = dtmp['mlp_group_cc_train']\n",
    "    mlp_group_cc_test = dtmp['mlp_group_cc_test']\n",
    "\n",
    "else:\n",
    "    mlp_group_rmse_test, mlp_group_rmse_train, mlp_group_cc_test, mlp_group_cc_train = evaluate_cv(mlp_group_cv_results)\n",
    "    np.savez_compressed(outfile, mlp_group_rmse_train=mlp_group_rmse_train, mlp_group_rmse_test=mlp_group_rmse_test, mlp_group_cc_train=mlp_group_cc_train, mlp_group_cc_test=mlp_group_cc_test)\n",
    "\n",
    "print(np.mean(mlp_group_rmse_test), np.mean(mlp_group_rmse_train), np.mean(mlp_group_cc_test), np.mean(mlp_group_cc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cafca29-6e3f-48af-993f-e63679fc2f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23495897691096657 0.04710168880948058 0.7155299584565977 0.9895216038321492\n"
     ]
    }
   ],
   "source": [
    "suffix = 2\n",
    "\n",
    "outfile = f'{outpath_all}/LSE_MLP_normKGE_groupCV_estimates{suffix}.pkl'\n",
    "\n",
    "if os.path.isfile(outfile):\n",
    "    with open(outfile, 'rb') as file:\n",
    "        mlp_group_cv_results = pickle.load(file)\n",
    "else:\n",
    "    modelconfig = {'hidden_layer_sizes': (100,100,), 'max_iter': 1000, 'alpha': 0.0001, 'random_state': 42, 'early_stopping': True, 'validation_fraction': 0.2}\n",
    "    mlp_group_cv_results = bpnn_run_cross_validation_parallel(group_cv_indices, x_all, y_all, 5, modelconfig)\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_group_cv_results, file)\n",
    "\n",
    "outfile = f'{outpath_all}/LSE_MLP_normKGE_groupCV_evaluation{suffix}.npz'\n",
    "\n",
    "if os.path.isfile(outfile):\n",
    "    dtmp = np.load(outfile)\n",
    "    mlp_group_rmse_train = dtmp['mlp_group_rmse_train']\n",
    "    mlp_group_rmse_test = dtmp['mlp_group_rmse_test']\n",
    "    mlp_group_cc_train = dtmp['mlp_group_cc_train']\n",
    "    mlp_group_cc_test = dtmp['mlp_group_cc_test']\n",
    "\n",
    "else:\n",
    "    mlp_group_rmse_test, mlp_group_rmse_train, mlp_group_cc_test, mlp_group_cc_train = evaluate_cv(mlp_group_cv_results)\n",
    "    np.savez_compressed(outfile, mlp_group_rmse_train=mlp_group_rmse_train, mlp_group_rmse_test=mlp_group_rmse_test, mlp_group_cc_train=mlp_group_cc_train, mlp_group_cc_test=mlp_group_cc_test)\n",
    "\n",
    "print(np.mean(mlp_group_rmse_test), np.mean(mlp_group_rmse_train), np.mean(mlp_group_cc_test), np.mean(mlp_group_cc_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1b1ba-9056-47b6-8fae-51666b2e6226",
   "metadata": {},
   "source": [
    "# Pytorch ANN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8ce2ba0-7be7-400a-9b58-6d8875ca8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the neural network model outside the function\n",
    "class SimpleNN1(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 200)\n",
    "        self.fc2 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    # Add a predict method similar to sklearn's\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            predictions = self.forward(X)\n",
    "        return predictions.numpy()  # Convert the predictions back to numpy\n",
    "\n",
    "\n",
    "class SimpleNN2(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN2, self).__init__()\n",
    "        # Update to two hidden layers with 100 units each\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the two hidden layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Final output\n",
    "\n",
    "    # Add a predict method similar to sklearn's\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            predictions = self.forward(X)\n",
    "        return predictions.numpy()  # Convert the predictions back to numpy\n",
    "\n",
    "def train_nn_model_pytorch(x_train_scaled, y_train, x_val_scaled, y_val, n_epochs=1000, patience=10, lr=0.001, model_file=\"model.pth\", method=1):\n",
    "    # Convert data to PyTorch tensors\n",
    "    x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    x_val_tensor = torch.tensor(x_val_scaled, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    if method==1:\n",
    "        model = SimpleNN1(input_size=x_train_tensor.shape[1])\n",
    "    elif method==2:\n",
    "        model = SimpleNN2(input_size=x_train_tensor.shape[1])\n",
    "    criterion = nn.MSELoss()  # We use MSELoss but will take the sqrt for RMSE\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_val_rmse = float('inf')\n",
    "    best_train_rmse = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_train_tensor)\n",
    "        train_loss = criterion(predictions, y_train_tensor)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate RMSE for training\n",
    "        train_rmse = torch.sqrt(train_loss).item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(x_val_tensor)\n",
    "            val_loss = criterion(val_predictions, y_val_tensor)\n",
    "            val_rmse = torch.sqrt(val_loss).item()  # Calculate RMSE for validation\n",
    "\n",
    "        # Early stopping logic based on validation RMSE\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_train_rmse = train_rmse  # Update best train RMSE when validation RMSE improves\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_file)  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    # Load the best model state before returning\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    # Output the best validation and training RMSE after training finishes\n",
    "    print(f'Best Training RMSE after training: {best_train_rmse}')\n",
    "    print(f'Best Validation RMSE after training: {best_val_rmse}')\n",
    "    print(f'Best Training MSE after training: {best_train_rmse**2}')\n",
    "    print(f'Best Validation MSE after training: {best_val_rmse**2}')\n",
    "\n",
    "    # Return the trained model, now with predict method\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfe2f4b5-f88f-4bb5-9405-827dff283174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Training RMSE after training: 0.15986594557762146\n",
      "Best Validation RMSE after training: 0.23128218948841095\n",
      "Best Training MSE after training: 0.025557120555427026\n",
      "Best Validation MSE after training: 0.05349145117455323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/guoqiang/tmp/ipykernel_40458/134617902.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train a random forest emulator\n",
    "outfile = f'{outpath_all}/Pytorch_ANN_emulator1'\n",
    "em_model = train_nn_model_pytorch(x_train_scaled, y_train, x_val_scaled, y_val, model_file=outfile, method=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8388c93-161f-4a8c-a3fb-d49a200f71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "train_index=group_cv_indices[fold]\n",
    "test_index=group_cv_indices[fold]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160daa1-df64-45c2-b5e3-a6d2274ce319",
   "metadata": {},
   "source": [
    "# Pytorch ANN, mimicing sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ef09d9e-2c89-4fee-adb1-abe29e619b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers=(100, 100)):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, 1))  # Output layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            predictions = self.forward(X)\n",
    "        return predictions.numpy()  # Convert the predictions back to numpy\n",
    "\n",
    "\n",
    "def train_nn_model_pytorch_fold(args):\n",
    "    fold, indices, x_all, y_all, n_epochs, patience, lr, model_file = args\n",
    "    \n",
    "    # Extract training and testing indices for the fold\n",
    "    train_index = indices['train_index']\n",
    "    test_index = indices['test_index']\n",
    "    \n",
    "    x_train, x_test = x_all[train_index], x_all[test_index]\n",
    "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "\n",
    "    # Initialize scalers for inputs and outputs\n",
    "    x_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "\n",
    "    # Normalize x_train and x_test\n",
    "    x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "    x_test_scaled = x_scaler.transform(x_test)\n",
    "\n",
    "    # Normalize y_train and y_test\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).view(-1, 1)\n",
    "    x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = SimpleNN(input_size=x_train_tensor.shape[1])\n",
    "    criterion = nn.MSELoss()  # We use MSELoss but will take the sqrt for RMSE\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_val_rmse = float('inf')\n",
    "    best_train_rmse = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop with internal random validation split\n",
    "    for epoch in range(n_epochs):\n",
    "        # Randomly split 10% of the training data as validation\n",
    "        x_train_fold, x_val_fold, y_train_fold, y_val_fold = train_test_split(\n",
    "            x_train_tensor, y_train_tensor, test_size=0.2, random_state=epoch\n",
    "        )\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_train_fold)\n",
    "        train_loss = criterion(predictions, y_train_fold)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate RMSE for training\n",
    "        train_rmse = torch.sqrt(train_loss).item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(x_val_fold)\n",
    "            val_loss = criterion(val_predictions, y_val_fold)\n",
    "            val_rmse = torch.sqrt(val_loss).item()  # Calculate RMSE for validation\n",
    "\n",
    "        # Early stopping logic based on validation RMSE\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_train_rmse = train_rmse  # Update best train RMSE when validation RMSE improves\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_file)  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Fold {fold}: Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    # Testing phase (on 20% test set)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions_scaled = model(x_test_tensor)\n",
    "        train_predictions_scaled = model(x_train_tensor)  # Predict on training set for train results\n",
    "\n",
    "    # Inverse transform the scaled predictions and true values for interpretability\n",
    "    y_test_pred = y_scaler.inverse_transform(test_predictions_scaled.numpy().flatten().reshape(-1, 1)).flatten()\n",
    "    y_train_pred = y_scaler.inverse_transform(train_predictions_scaled.numpy().flatten().reshape(-1, 1)).flatten()\n",
    "    y_test_true = y_test  # Original unscaled y_test values\n",
    "    y_train_true = y_train  # Original unscaled y_train values\n",
    "\n",
    "    # Store results\n",
    "    fold_result = {\n",
    "        'train_index': train_index,\n",
    "        'test_index': test_index,\n",
    "        'y_train': y_train_true,\n",
    "        'y_test': y_test_true,\n",
    "        'y_test_pred': y_test_pred,\n",
    "        'y_train_pred': y_train_pred,\n",
    "        'train_rmse': best_train_rmse,\n",
    "        'val_rmse': best_val_rmse,\n",
    "    }\n",
    "    return fold, fold_result\n",
    "\n",
    "\n",
    "def run_cross_validation_pytorch(cv_indices, x_all, y_all, ncpus, n_epochs=1000, patience=10, lr=0.001):\n",
    "    model_file = \"model_fold.pth\"\n",
    "    # Prepare arguments for each fold\n",
    "    args = [\n",
    "        (fold, indices, x_all, y_all, n_epochs, patience, lr, model_file)\n",
    "        for fold, indices in cv_indices.items()\n",
    "    ]\n",
    "\n",
    "    # Run cross-validation in parallel\n",
    "    with Pool(processes=ncpus) as pool:\n",
    "        results = pool.map(train_nn_model_pytorch_fold, args)\n",
    "\n",
    "    # Collect and organize results by fold\n",
    "    ann_cv_results = {fold: result for fold, result in results}\n",
    "\n",
    "    return ann_cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29a42b81-205d-4362-b151-c7870c36f7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12494849919975075 0.1426924282762719 0.9020818806453311 0.8965373676612751\n"
     ]
    }
   ],
   "source": [
    "outfile = f'{outpath_all}/LSE_PytorchNN_normKGE_groupCV_estimates.pkl'\n",
    "\n",
    "if os.path.isfile(outfile):\n",
    "    with open(outfile, 'rb') as file:\n",
    "        mlp_group_cv_results = pickle.load(file)\n",
    "else:\n",
    "    mlp_group_cv_results = run_cross_validation_pytorch(group_cv_indices, x_all, y_all, 5)\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_group_cv_results, file)\n",
    "\n",
    "outfile = f'{outpath_all}/LSE_PytorchNN_normKGE_groupCV_evaluation.npz'\n",
    "\n",
    "if os.path.isfile(outfile):\n",
    "    dtmp = np.load(outfile)\n",
    "    mlp_group_rmse_train = dtmp['mlp_group_rmse_train']\n",
    "    mlp_group_rmse_test = dtmp['mlp_group_rmse_test']\n",
    "    mlp_group_cc_train = dtmp['mlp_group_cc_train']\n",
    "    mlp_group_cc_test = dtmp['mlp_group_cc_test']\n",
    "\n",
    "else:\n",
    "    mlp_group_rmse_test, mlp_group_rmse_train, mlp_group_cc_test, mlp_group_cc_train = evaluate_cv(mlp_group_cv_results)\n",
    "    np.savez_compressed(outfile, mlp_group_rmse_train=mlp_group_rmse_train, mlp_group_rmse_test=mlp_group_rmse_test, mlp_group_cc_train=mlp_group_cc_train, mlp_group_cc_test=mlp_group_cc_test)\n",
    "\n",
    "print(np.mean(mlp_group_rmse_test), np.mean(mlp_group_rmse_train), np.mean(mlp_group_cc_test), np.mean(mlp_group_cc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb27e8-0c11-4e86-8eec-4267bf94d21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfb05954-4876-45a8-ac0f-48f13979aa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x153612085f90>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d1 = np.squeeze(mlp_group_cv_results[0]['y_test'])\n",
    "d2 = np.squeeze(mlp_group_cv_results[0]['y_test_pred'])\n",
    "d3 = np.squeeze(rf_group_cv_results[0]['y_test_pred'])\n",
    "\n",
    "plt.plot(d1)\n",
    "plt.plot(d2)\n",
    "plt.plot(d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae34d70-00b6-45d4-b553-3b3940dcfcf7",
   "metadata": {},
   "source": [
    "# Train Pytorch using all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75b02bc4-f05e-4fe5-98d1-46e650738b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers=(100, 100)):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, 1))  # Output layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            predictions = self.forward(X)\n",
    "        return predictions.numpy()  # Convert the predictions back to numpy\n",
    "\n",
    "\n",
    "def train_nn_model_pytorch(x_all, y_all, n_epochs=2000, patience=30, lr=0.0005, model_file=\"model.pth\"):\n",
    "    # Split the data into training and validation sets (10% for validation)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_all, y_all, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize scalers for inputs and outputs\n",
    "    x_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "\n",
    "    # Normalize x_train and x_val\n",
    "    x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "    x_val_scaled = x_scaler.transform(x_val)\n",
    "\n",
    "    # Normalize y_train and y_val\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).view(-1, 1)\n",
    "    x_val_tensor = torch.tensor(x_val_scaled, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = SimpleNN(input_size=x_train_tensor.shape[1])\n",
    "    criterion = nn.MSELoss()  # We use MSELoss but will calculate RMSE manually\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_val_rmse = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_train_tensor)\n",
    "        train_loss = criterion(predictions, y_train_tensor)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate train RMSE from model predictions\n",
    "        train_predictions = model.predict(x_train_scaled)\n",
    "        train_rmse = np.sqrt(np.mean((y_scaler.inverse_transform(train_predictions) - y_train) ** 2))\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model.predict(x_val_scaled)\n",
    "            val_rmse = np.sqrt(np.mean((y_scaler.inverse_transform(val_predictions) - y_val) ** 2))\n",
    "\n",
    "        # Print RMSE for each epoch\n",
    "        print(f'Epoch {epoch+1}, Training RMSE: {train_rmse:.4f}, Validation RMSE: {val_rmse:.4f}')\n",
    "\n",
    "        # Early stopping logic based on validation RMSE\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_train_rmse = train_rmse  # Update best train RMSE when validation RMSE improves\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_file)  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    # Load the best model state before returning\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    # Output the best validation and training RMSE after training finishes\n",
    "    print(f'Best Training RMSE after training: {best_train_rmse}')\n",
    "    print(f'Best Validation RMSE after training: {best_val_rmse}')\n",
    "\n",
    "    # Return the trained model, now with predict method\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e651de6-948a-4c2e-bd16-f98fbb4fa15c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training RMSE: 0.3285, Validation RMSE: 0.3274\n",
      "Epoch 2, Training RMSE: 0.3247, Validation RMSE: 0.3236\n",
      "Epoch 3, Training RMSE: 0.3211, Validation RMSE: 0.3199\n",
      "Epoch 4, Training RMSE: 0.3176, Validation RMSE: 0.3164\n",
      "Epoch 5, Training RMSE: 0.3141, Validation RMSE: 0.3129\n",
      "Epoch 6, Training RMSE: 0.3108, Validation RMSE: 0.3095\n",
      "Epoch 7, Training RMSE: 0.3075, Validation RMSE: 0.3062\n",
      "Epoch 8, Training RMSE: 0.3042, Validation RMSE: 0.3030\n",
      "Epoch 9, Training RMSE: 0.3010, Validation RMSE: 0.2998\n",
      "Epoch 10, Training RMSE: 0.2978, Validation RMSE: 0.2966\n",
      "Epoch 11, Training RMSE: 0.2947, Validation RMSE: 0.2935\n",
      "Epoch 12, Training RMSE: 0.2916, Validation RMSE: 0.2904\n",
      "Epoch 13, Training RMSE: 0.2885, Validation RMSE: 0.2873\n",
      "Epoch 14, Training RMSE: 0.2854, Validation RMSE: 0.2843\n",
      "Epoch 15, Training RMSE: 0.2824, Validation RMSE: 0.2813\n",
      "Epoch 16, Training RMSE: 0.2795, Validation RMSE: 0.2783\n",
      "Epoch 17, Training RMSE: 0.2766, Validation RMSE: 0.2754\n",
      "Epoch 18, Training RMSE: 0.2737, Validation RMSE: 0.2726\n",
      "Epoch 19, Training RMSE: 0.2710, Validation RMSE: 0.2699\n",
      "Epoch 20, Training RMSE: 0.2684, Validation RMSE: 0.2673\n",
      "Epoch 21, Training RMSE: 0.2659, Validation RMSE: 0.2648\n",
      "Epoch 22, Training RMSE: 0.2635, Validation RMSE: 0.2625\n",
      "Epoch 23, Training RMSE: 0.2613, Validation RMSE: 0.2603\n",
      "Epoch 24, Training RMSE: 0.2592, Validation RMSE: 0.2582\n",
      "Epoch 25, Training RMSE: 0.2573, Validation RMSE: 0.2563\n",
      "Epoch 26, Training RMSE: 0.2555, Validation RMSE: 0.2546\n",
      "Epoch 27, Training RMSE: 0.2538, Validation RMSE: 0.2530\n",
      "Epoch 28, Training RMSE: 0.2523, Validation RMSE: 0.2515\n",
      "Epoch 29, Training RMSE: 0.2509, Validation RMSE: 0.2501\n",
      "Epoch 30, Training RMSE: 0.2496, Validation RMSE: 0.2488\n",
      "Epoch 31, Training RMSE: 0.2483, Validation RMSE: 0.2476\n",
      "Epoch 32, Training RMSE: 0.2471, Validation RMSE: 0.2464\n",
      "Epoch 33, Training RMSE: 0.2459, Validation RMSE: 0.2452\n",
      "Epoch 34, Training RMSE: 0.2447, Validation RMSE: 0.2440\n",
      "Epoch 35, Training RMSE: 0.2434, Validation RMSE: 0.2428\n",
      "Epoch 36, Training RMSE: 0.2422, Validation RMSE: 0.2416\n",
      "Epoch 37, Training RMSE: 0.2410, Validation RMSE: 0.2403\n",
      "Epoch 38, Training RMSE: 0.2397, Validation RMSE: 0.2391\n",
      "Epoch 39, Training RMSE: 0.2385, Validation RMSE: 0.2379\n",
      "Epoch 40, Training RMSE: 0.2373, Validation RMSE: 0.2367\n",
      "Epoch 41, Training RMSE: 0.2361, Validation RMSE: 0.2355\n",
      "Epoch 42, Training RMSE: 0.2350, Validation RMSE: 0.2343\n",
      "Epoch 43, Training RMSE: 0.2339, Validation RMSE: 0.2332\n",
      "Epoch 44, Training RMSE: 0.2329, Validation RMSE: 0.2322\n",
      "Epoch 45, Training RMSE: 0.2320, Validation RMSE: 0.2312\n",
      "Epoch 46, Training RMSE: 0.2310, Validation RMSE: 0.2303\n",
      "Epoch 47, Training RMSE: 0.2301, Validation RMSE: 0.2294\n",
      "Epoch 48, Training RMSE: 0.2293, Validation RMSE: 0.2285\n",
      "Epoch 49, Training RMSE: 0.2285, Validation RMSE: 0.2277\n",
      "Epoch 50, Training RMSE: 0.2276, Validation RMSE: 0.2268\n",
      "Epoch 51, Training RMSE: 0.2268, Validation RMSE: 0.2260\n",
      "Epoch 52, Training RMSE: 0.2260, Validation RMSE: 0.2252\n",
      "Epoch 53, Training RMSE: 0.2252, Validation RMSE: 0.2244\n",
      "Epoch 54, Training RMSE: 0.2245, Validation RMSE: 0.2236\n",
      "Epoch 55, Training RMSE: 0.2237, Validation RMSE: 0.2228\n",
      "Epoch 56, Training RMSE: 0.2229, Validation RMSE: 0.2220\n",
      "Epoch 57, Training RMSE: 0.2221, Validation RMSE: 0.2212\n",
      "Epoch 58, Training RMSE: 0.2213, Validation RMSE: 0.2205\n",
      "Epoch 59, Training RMSE: 0.2206, Validation RMSE: 0.2197\n",
      "Epoch 60, Training RMSE: 0.2198, Validation RMSE: 0.2190\n",
      "Epoch 61, Training RMSE: 0.2191, Validation RMSE: 0.2182\n",
      "Epoch 62, Training RMSE: 0.2184, Validation RMSE: 0.2175\n",
      "Epoch 63, Training RMSE: 0.2177, Validation RMSE: 0.2168\n",
      "Epoch 64, Training RMSE: 0.2170, Validation RMSE: 0.2161\n",
      "Epoch 65, Training RMSE: 0.2163, Validation RMSE: 0.2155\n",
      "Epoch 66, Training RMSE: 0.2157, Validation RMSE: 0.2148\n",
      "Epoch 67, Training RMSE: 0.2150, Validation RMSE: 0.2141\n",
      "Epoch 68, Training RMSE: 0.2143, Validation RMSE: 0.2135\n",
      "Epoch 69, Training RMSE: 0.2137, Validation RMSE: 0.2128\n",
      "Epoch 70, Training RMSE: 0.2130, Validation RMSE: 0.2121\n",
      "Epoch 71, Training RMSE: 0.2124, Validation RMSE: 0.2115\n",
      "Epoch 72, Training RMSE: 0.2117, Validation RMSE: 0.2108\n",
      "Epoch 73, Training RMSE: 0.2111, Validation RMSE: 0.2102\n",
      "Epoch 74, Training RMSE: 0.2104, Validation RMSE: 0.2095\n",
      "Epoch 75, Training RMSE: 0.2098, Validation RMSE: 0.2089\n",
      "Epoch 76, Training RMSE: 0.2092, Validation RMSE: 0.2083\n",
      "Epoch 77, Training RMSE: 0.2085, Validation RMSE: 0.2076\n",
      "Epoch 78, Training RMSE: 0.2079, Validation RMSE: 0.2070\n",
      "Epoch 79, Training RMSE: 0.2072, Validation RMSE: 0.2063\n",
      "Epoch 80, Training RMSE: 0.2066, Validation RMSE: 0.2057\n",
      "Epoch 81, Training RMSE: 0.2059, Validation RMSE: 0.2050\n",
      "Epoch 82, Training RMSE: 0.2053, Validation RMSE: 0.2044\n",
      "Epoch 83, Training RMSE: 0.2046, Validation RMSE: 0.2037\n",
      "Epoch 84, Training RMSE: 0.2040, Validation RMSE: 0.2030\n",
      "Epoch 85, Training RMSE: 0.2033, Validation RMSE: 0.2024\n",
      "Epoch 86, Training RMSE: 0.2026, Validation RMSE: 0.2017\n",
      "Epoch 87, Training RMSE: 0.2020, Validation RMSE: 0.2010\n",
      "Epoch 88, Training RMSE: 0.2013, Validation RMSE: 0.2004\n",
      "Epoch 89, Training RMSE: 0.2006, Validation RMSE: 0.1997\n",
      "Epoch 90, Training RMSE: 0.2000, Validation RMSE: 0.1990\n",
      "Epoch 91, Training RMSE: 0.1993, Validation RMSE: 0.1984\n",
      "Epoch 92, Training RMSE: 0.1986, Validation RMSE: 0.1977\n",
      "Epoch 93, Training RMSE: 0.1979, Validation RMSE: 0.1970\n",
      "Epoch 94, Training RMSE: 0.1973, Validation RMSE: 0.1963\n",
      "Epoch 95, Training RMSE: 0.1966, Validation RMSE: 0.1956\n",
      "Epoch 96, Training RMSE: 0.1959, Validation RMSE: 0.1949\n",
      "Epoch 97, Training RMSE: 0.1952, Validation RMSE: 0.1943\n",
      "Epoch 98, Training RMSE: 0.1946, Validation RMSE: 0.1936\n",
      "Epoch 99, Training RMSE: 0.1939, Validation RMSE: 0.1929\n",
      "Epoch 100, Training RMSE: 0.1932, Validation RMSE: 0.1922\n",
      "Epoch 101, Training RMSE: 0.1925, Validation RMSE: 0.1915\n",
      "Epoch 102, Training RMSE: 0.1919, Validation RMSE: 0.1908\n",
      "Epoch 103, Training RMSE: 0.1912, Validation RMSE: 0.1901\n",
      "Epoch 104, Training RMSE: 0.1905, Validation RMSE: 0.1895\n",
      "Epoch 105, Training RMSE: 0.1899, Validation RMSE: 0.1888\n",
      "Epoch 106, Training RMSE: 0.1892, Validation RMSE: 0.1881\n",
      "Epoch 107, Training RMSE: 0.1885, Validation RMSE: 0.1875\n",
      "Epoch 108, Training RMSE: 0.1879, Validation RMSE: 0.1868\n",
      "Epoch 109, Training RMSE: 0.1872, Validation RMSE: 0.1861\n",
      "Epoch 110, Training RMSE: 0.1866, Validation RMSE: 0.1855\n",
      "Epoch 111, Training RMSE: 0.1859, Validation RMSE: 0.1848\n",
      "Epoch 112, Training RMSE: 0.1853, Validation RMSE: 0.1842\n",
      "Epoch 113, Training RMSE: 0.1846, Validation RMSE: 0.1835\n",
      "Epoch 114, Training RMSE: 0.1840, Validation RMSE: 0.1829\n",
      "Epoch 115, Training RMSE: 0.1834, Validation RMSE: 0.1822\n",
      "Epoch 116, Training RMSE: 0.1828, Validation RMSE: 0.1816\n",
      "Epoch 117, Training RMSE: 0.1821, Validation RMSE: 0.1810\n",
      "Epoch 118, Training RMSE: 0.1815, Validation RMSE: 0.1804\n",
      "Epoch 119, Training RMSE: 0.1809, Validation RMSE: 0.1797\n",
      "Epoch 120, Training RMSE: 0.1803, Validation RMSE: 0.1791\n",
      "Epoch 121, Training RMSE: 0.1797, Validation RMSE: 0.1785\n",
      "Epoch 122, Training RMSE: 0.1791, Validation RMSE: 0.1779\n",
      "Epoch 123, Training RMSE: 0.1785, Validation RMSE: 0.1773\n",
      "Epoch 124, Training RMSE: 0.1779, Validation RMSE: 0.1767\n",
      "Epoch 125, Training RMSE: 0.1773, Validation RMSE: 0.1761\n",
      "Epoch 126, Training RMSE: 0.1768, Validation RMSE: 0.1756\n",
      "Epoch 127, Training RMSE: 0.1762, Validation RMSE: 0.1750\n",
      "Epoch 128, Training RMSE: 0.1756, Validation RMSE: 0.1744\n",
      "Epoch 129, Training RMSE: 0.1750, Validation RMSE: 0.1739\n",
      "Epoch 130, Training RMSE: 0.1745, Validation RMSE: 0.1733\n",
      "Epoch 131, Training RMSE: 0.1739, Validation RMSE: 0.1727\n",
      "Epoch 132, Training RMSE: 0.1734, Validation RMSE: 0.1722\n",
      "Epoch 133, Training RMSE: 0.1728, Validation RMSE: 0.1716\n",
      "Epoch 134, Training RMSE: 0.1723, Validation RMSE: 0.1711\n",
      "Epoch 135, Training RMSE: 0.1718, Validation RMSE: 0.1706\n",
      "Epoch 136, Training RMSE: 0.1712, Validation RMSE: 0.1700\n",
      "Epoch 137, Training RMSE: 0.1707, Validation RMSE: 0.1695\n",
      "Epoch 138, Training RMSE: 0.1702, Validation RMSE: 0.1690\n",
      "Epoch 139, Training RMSE: 0.1697, Validation RMSE: 0.1685\n",
      "Epoch 140, Training RMSE: 0.1691, Validation RMSE: 0.1679\n",
      "Epoch 141, Training RMSE: 0.1686, Validation RMSE: 0.1674\n",
      "Epoch 142, Training RMSE: 0.1681, Validation RMSE: 0.1669\n",
      "Epoch 143, Training RMSE: 0.1676, Validation RMSE: 0.1664\n",
      "Epoch 144, Training RMSE: 0.1671, Validation RMSE: 0.1659\n",
      "Epoch 145, Training RMSE: 0.1666, Validation RMSE: 0.1654\n",
      "Epoch 146, Training RMSE: 0.1661, Validation RMSE: 0.1649\n",
      "Epoch 147, Training RMSE: 0.1656, Validation RMSE: 0.1644\n",
      "Epoch 148, Training RMSE: 0.1651, Validation RMSE: 0.1639\n",
      "Epoch 149, Training RMSE: 0.1646, Validation RMSE: 0.1635\n",
      "Epoch 150, Training RMSE: 0.1641, Validation RMSE: 0.1630\n",
      "Epoch 151, Training RMSE: 0.1636, Validation RMSE: 0.1625\n",
      "Epoch 152, Training RMSE: 0.1631, Validation RMSE: 0.1620\n",
      "Epoch 153, Training RMSE: 0.1627, Validation RMSE: 0.1615\n",
      "Epoch 154, Training RMSE: 0.1622, Validation RMSE: 0.1611\n",
      "Epoch 155, Training RMSE: 0.1617, Validation RMSE: 0.1606\n",
      "Epoch 156, Training RMSE: 0.1612, Validation RMSE: 0.1601\n",
      "Epoch 157, Training RMSE: 0.1607, Validation RMSE: 0.1597\n",
      "Epoch 158, Training RMSE: 0.1603, Validation RMSE: 0.1592\n",
      "Epoch 159, Training RMSE: 0.1598, Validation RMSE: 0.1588\n",
      "Epoch 160, Training RMSE: 0.1593, Validation RMSE: 0.1583\n",
      "Epoch 161, Training RMSE: 0.1589, Validation RMSE: 0.1579\n",
      "Epoch 162, Training RMSE: 0.1584, Validation RMSE: 0.1574\n",
      "Epoch 163, Training RMSE: 0.1580, Validation RMSE: 0.1570\n",
      "Epoch 164, Training RMSE: 0.1575, Validation RMSE: 0.1565\n",
      "Epoch 165, Training RMSE: 0.1570, Validation RMSE: 0.1561\n",
      "Epoch 166, Training RMSE: 0.1566, Validation RMSE: 0.1556\n",
      "Epoch 167, Training RMSE: 0.1561, Validation RMSE: 0.1552\n",
      "Epoch 168, Training RMSE: 0.1557, Validation RMSE: 0.1547\n",
      "Epoch 169, Training RMSE: 0.1553, Validation RMSE: 0.1543\n",
      "Epoch 170, Training RMSE: 0.1548, Validation RMSE: 0.1539\n",
      "Epoch 171, Training RMSE: 0.1544, Validation RMSE: 0.1534\n",
      "Epoch 172, Training RMSE: 0.1539, Validation RMSE: 0.1530\n",
      "Epoch 173, Training RMSE: 0.1535, Validation RMSE: 0.1526\n",
      "Epoch 174, Training RMSE: 0.1531, Validation RMSE: 0.1522\n",
      "Epoch 175, Training RMSE: 0.1526, Validation RMSE: 0.1517\n",
      "Epoch 176, Training RMSE: 0.1522, Validation RMSE: 0.1513\n",
      "Epoch 177, Training RMSE: 0.1518, Validation RMSE: 0.1509\n",
      "Epoch 178, Training RMSE: 0.1513, Validation RMSE: 0.1505\n",
      "Epoch 179, Training RMSE: 0.1509, Validation RMSE: 0.1501\n",
      "Epoch 180, Training RMSE: 0.1505, Validation RMSE: 0.1497\n",
      "Epoch 181, Training RMSE: 0.1501, Validation RMSE: 0.1492\n",
      "Epoch 182, Training RMSE: 0.1496, Validation RMSE: 0.1488\n",
      "Epoch 183, Training RMSE: 0.1492, Validation RMSE: 0.1484\n",
      "Epoch 184, Training RMSE: 0.1488, Validation RMSE: 0.1480\n",
      "Epoch 185, Training RMSE: 0.1484, Validation RMSE: 0.1476\n",
      "Epoch 186, Training RMSE: 0.1480, Validation RMSE: 0.1472\n",
      "Epoch 187, Training RMSE: 0.1475, Validation RMSE: 0.1468\n",
      "Epoch 188, Training RMSE: 0.1471, Validation RMSE: 0.1464\n",
      "Epoch 189, Training RMSE: 0.1467, Validation RMSE: 0.1460\n",
      "Epoch 190, Training RMSE: 0.1463, Validation RMSE: 0.1456\n",
      "Epoch 191, Training RMSE: 0.1459, Validation RMSE: 0.1452\n",
      "Epoch 192, Training RMSE: 0.1455, Validation RMSE: 0.1448\n",
      "Epoch 193, Training RMSE: 0.1451, Validation RMSE: 0.1444\n",
      "Epoch 194, Training RMSE: 0.1447, Validation RMSE: 0.1440\n",
      "Epoch 195, Training RMSE: 0.1443, Validation RMSE: 0.1436\n",
      "Epoch 196, Training RMSE: 0.1439, Validation RMSE: 0.1433\n",
      "Epoch 197, Training RMSE: 0.1435, Validation RMSE: 0.1429\n",
      "Epoch 198, Training RMSE: 0.1431, Validation RMSE: 0.1425\n",
      "Epoch 199, Training RMSE: 0.1427, Validation RMSE: 0.1421\n",
      "Epoch 200, Training RMSE: 0.1423, Validation RMSE: 0.1417\n",
      "Epoch 201, Training RMSE: 0.1419, Validation RMSE: 0.1413\n",
      "Epoch 202, Training RMSE: 0.1415, Validation RMSE: 0.1409\n",
      "Epoch 203, Training RMSE: 0.1411, Validation RMSE: 0.1406\n",
      "Epoch 204, Training RMSE: 0.1407, Validation RMSE: 0.1402\n",
      "Epoch 205, Training RMSE: 0.1403, Validation RMSE: 0.1398\n",
      "Epoch 206, Training RMSE: 0.1399, Validation RMSE: 0.1394\n",
      "Epoch 207, Training RMSE: 0.1395, Validation RMSE: 0.1391\n",
      "Epoch 208, Training RMSE: 0.1391, Validation RMSE: 0.1387\n",
      "Epoch 209, Training RMSE: 0.1388, Validation RMSE: 0.1383\n",
      "Epoch 210, Training RMSE: 0.1384, Validation RMSE: 0.1379\n",
      "Epoch 211, Training RMSE: 0.1380, Validation RMSE: 0.1376\n",
      "Epoch 212, Training RMSE: 0.1376, Validation RMSE: 0.1372\n",
      "Epoch 213, Training RMSE: 0.1372, Validation RMSE: 0.1368\n",
      "Epoch 214, Training RMSE: 0.1368, Validation RMSE: 0.1365\n",
      "Epoch 215, Training RMSE: 0.1365, Validation RMSE: 0.1361\n",
      "Epoch 216, Training RMSE: 0.1361, Validation RMSE: 0.1358\n",
      "Epoch 217, Training RMSE: 0.1357, Validation RMSE: 0.1354\n",
      "Epoch 218, Training RMSE: 0.1353, Validation RMSE: 0.1350\n",
      "Epoch 219, Training RMSE: 0.1350, Validation RMSE: 0.1347\n",
      "Epoch 220, Training RMSE: 0.1346, Validation RMSE: 0.1343\n",
      "Epoch 221, Training RMSE: 0.1342, Validation RMSE: 0.1340\n",
      "Epoch 222, Training RMSE: 0.1339, Validation RMSE: 0.1336\n",
      "Epoch 223, Training RMSE: 0.1335, Validation RMSE: 0.1333\n",
      "Epoch 224, Training RMSE: 0.1331, Validation RMSE: 0.1329\n",
      "Epoch 225, Training RMSE: 0.1328, Validation RMSE: 0.1326\n",
      "Epoch 226, Training RMSE: 0.1324, Validation RMSE: 0.1322\n",
      "Epoch 227, Training RMSE: 0.1320, Validation RMSE: 0.1319\n",
      "Epoch 228, Training RMSE: 0.1317, Validation RMSE: 0.1315\n",
      "Epoch 229, Training RMSE: 0.1313, Validation RMSE: 0.1312\n",
      "Epoch 230, Training RMSE: 0.1310, Validation RMSE: 0.1308\n",
      "Epoch 231, Training RMSE: 0.1306, Validation RMSE: 0.1305\n",
      "Epoch 232, Training RMSE: 0.1302, Validation RMSE: 0.1301\n",
      "Epoch 233, Training RMSE: 0.1299, Validation RMSE: 0.1298\n",
      "Epoch 234, Training RMSE: 0.1295, Validation RMSE: 0.1294\n",
      "Epoch 235, Training RMSE: 0.1292, Validation RMSE: 0.1291\n",
      "Epoch 236, Training RMSE: 0.1288, Validation RMSE: 0.1288\n",
      "Epoch 237, Training RMSE: 0.1285, Validation RMSE: 0.1284\n",
      "Epoch 238, Training RMSE: 0.1281, Validation RMSE: 0.1281\n",
      "Epoch 239, Training RMSE: 0.1278, Validation RMSE: 0.1278\n",
      "Epoch 240, Training RMSE: 0.1275, Validation RMSE: 0.1274\n",
      "Epoch 241, Training RMSE: 0.1271, Validation RMSE: 0.1271\n",
      "Epoch 242, Training RMSE: 0.1268, Validation RMSE: 0.1268\n",
      "Epoch 243, Training RMSE: 0.1264, Validation RMSE: 0.1264\n",
      "Epoch 244, Training RMSE: 0.1261, Validation RMSE: 0.1261\n",
      "Epoch 245, Training RMSE: 0.1258, Validation RMSE: 0.1258\n",
      "Epoch 246, Training RMSE: 0.1254, Validation RMSE: 0.1255\n",
      "Epoch 247, Training RMSE: 0.1251, Validation RMSE: 0.1252\n",
      "Epoch 248, Training RMSE: 0.1248, Validation RMSE: 0.1248\n",
      "Epoch 249, Training RMSE: 0.1244, Validation RMSE: 0.1245\n",
      "Epoch 250, Training RMSE: 0.1241, Validation RMSE: 0.1242\n",
      "Epoch 251, Training RMSE: 0.1238, Validation RMSE: 0.1239\n",
      "Epoch 252, Training RMSE: 0.1235, Validation RMSE: 0.1236\n",
      "Epoch 253, Training RMSE: 0.1231, Validation RMSE: 0.1233\n",
      "Epoch 254, Training RMSE: 0.1228, Validation RMSE: 0.1229\n",
      "Epoch 255, Training RMSE: 0.1225, Validation RMSE: 0.1226\n",
      "Epoch 256, Training RMSE: 0.1222, Validation RMSE: 0.1223\n",
      "Epoch 257, Training RMSE: 0.1219, Validation RMSE: 0.1220\n",
      "Epoch 258, Training RMSE: 0.1216, Validation RMSE: 0.1217\n",
      "Epoch 259, Training RMSE: 0.1212, Validation RMSE: 0.1214\n",
      "Epoch 260, Training RMSE: 0.1209, Validation RMSE: 0.1211\n",
      "Epoch 261, Training RMSE: 0.1206, Validation RMSE: 0.1208\n",
      "Epoch 262, Training RMSE: 0.1203, Validation RMSE: 0.1205\n",
      "Epoch 263, Training RMSE: 0.1200, Validation RMSE: 0.1202\n",
      "Epoch 264, Training RMSE: 0.1197, Validation RMSE: 0.1199\n",
      "Epoch 265, Training RMSE: 0.1194, Validation RMSE: 0.1196\n",
      "Epoch 266, Training RMSE: 0.1191, Validation RMSE: 0.1193\n",
      "Epoch 267, Training RMSE: 0.1188, Validation RMSE: 0.1190\n",
      "Epoch 268, Training RMSE: 0.1185, Validation RMSE: 0.1187\n",
      "Epoch 269, Training RMSE: 0.1182, Validation RMSE: 0.1184\n",
      "Epoch 270, Training RMSE: 0.1179, Validation RMSE: 0.1182\n",
      "Epoch 271, Training RMSE: 0.1176, Validation RMSE: 0.1179\n",
      "Epoch 272, Training RMSE: 0.1173, Validation RMSE: 0.1176\n",
      "Epoch 273, Training RMSE: 0.1170, Validation RMSE: 0.1173\n",
      "Epoch 274, Training RMSE: 0.1167, Validation RMSE: 0.1170\n",
      "Epoch 275, Training RMSE: 0.1165, Validation RMSE: 0.1168\n",
      "Epoch 276, Training RMSE: 0.1162, Validation RMSE: 0.1165\n",
      "Epoch 277, Training RMSE: 0.1159, Validation RMSE: 0.1162\n",
      "Epoch 278, Training RMSE: 0.1156, Validation RMSE: 0.1159\n",
      "Epoch 279, Training RMSE: 0.1153, Validation RMSE: 0.1157\n",
      "Epoch 280, Training RMSE: 0.1150, Validation RMSE: 0.1154\n",
      "Epoch 281, Training RMSE: 0.1148, Validation RMSE: 0.1151\n",
      "Epoch 282, Training RMSE: 0.1145, Validation RMSE: 0.1149\n",
      "Epoch 283, Training RMSE: 0.1142, Validation RMSE: 0.1146\n",
      "Epoch 284, Training RMSE: 0.1139, Validation RMSE: 0.1144\n",
      "Epoch 285, Training RMSE: 0.1137, Validation RMSE: 0.1141\n",
      "Epoch 286, Training RMSE: 0.1134, Validation RMSE: 0.1138\n",
      "Epoch 287, Training RMSE: 0.1131, Validation RMSE: 0.1136\n",
      "Epoch 288, Training RMSE: 0.1129, Validation RMSE: 0.1133\n",
      "Epoch 289, Training RMSE: 0.1126, Validation RMSE: 0.1131\n",
      "Epoch 290, Training RMSE: 0.1123, Validation RMSE: 0.1128\n",
      "Epoch 291, Training RMSE: 0.1121, Validation RMSE: 0.1126\n",
      "Epoch 292, Training RMSE: 0.1118, Validation RMSE: 0.1123\n",
      "Epoch 293, Training RMSE: 0.1116, Validation RMSE: 0.1121\n",
      "Epoch 294, Training RMSE: 0.1113, Validation RMSE: 0.1118\n",
      "Epoch 295, Training RMSE: 0.1110, Validation RMSE: 0.1116\n",
      "Epoch 296, Training RMSE: 0.1108, Validation RMSE: 0.1113\n",
      "Epoch 297, Training RMSE: 0.1105, Validation RMSE: 0.1111\n",
      "Epoch 298, Training RMSE: 0.1103, Validation RMSE: 0.1109\n",
      "Epoch 299, Training RMSE: 0.1100, Validation RMSE: 0.1106\n",
      "Epoch 300, Training RMSE: 0.1098, Validation RMSE: 0.1104\n",
      "Epoch 301, Training RMSE: 0.1095, Validation RMSE: 0.1101\n",
      "Epoch 302, Training RMSE: 0.1093, Validation RMSE: 0.1099\n",
      "Epoch 303, Training RMSE: 0.1090, Validation RMSE: 0.1097\n",
      "Epoch 304, Training RMSE: 0.1088, Validation RMSE: 0.1094\n",
      "Epoch 305, Training RMSE: 0.1086, Validation RMSE: 0.1092\n",
      "Epoch 306, Training RMSE: 0.1083, Validation RMSE: 0.1090\n",
      "Epoch 307, Training RMSE: 0.1081, Validation RMSE: 0.1088\n",
      "Epoch 308, Training RMSE: 0.1079, Validation RMSE: 0.1085\n",
      "Epoch 309, Training RMSE: 0.1076, Validation RMSE: 0.1083\n",
      "Epoch 310, Training RMSE: 0.1074, Validation RMSE: 0.1081\n",
      "Epoch 311, Training RMSE: 0.1072, Validation RMSE: 0.1079\n",
      "Epoch 312, Training RMSE: 0.1069, Validation RMSE: 0.1077\n",
      "Epoch 313, Training RMSE: 0.1067, Validation RMSE: 0.1074\n",
      "Epoch 314, Training RMSE: 0.1065, Validation RMSE: 0.1072\n",
      "Epoch 315, Training RMSE: 0.1063, Validation RMSE: 0.1070\n",
      "Epoch 316, Training RMSE: 0.1060, Validation RMSE: 0.1068\n",
      "Epoch 317, Training RMSE: 0.1058, Validation RMSE: 0.1066\n",
      "Epoch 318, Training RMSE: 0.1056, Validation RMSE: 0.1064\n",
      "Epoch 319, Training RMSE: 0.1054, Validation RMSE: 0.1061\n",
      "Epoch 320, Training RMSE: 0.1052, Validation RMSE: 0.1059\n",
      "Epoch 321, Training RMSE: 0.1049, Validation RMSE: 0.1057\n",
      "Epoch 322, Training RMSE: 0.1047, Validation RMSE: 0.1055\n",
      "Epoch 323, Training RMSE: 0.1045, Validation RMSE: 0.1053\n",
      "Epoch 324, Training RMSE: 0.1043, Validation RMSE: 0.1051\n",
      "Epoch 325, Training RMSE: 0.1041, Validation RMSE: 0.1049\n",
      "Epoch 326, Training RMSE: 0.1039, Validation RMSE: 0.1047\n",
      "Epoch 327, Training RMSE: 0.1037, Validation RMSE: 0.1045\n",
      "Epoch 328, Training RMSE: 0.1035, Validation RMSE: 0.1043\n",
      "Epoch 329, Training RMSE: 0.1033, Validation RMSE: 0.1041\n",
      "Epoch 330, Training RMSE: 0.1031, Validation RMSE: 0.1039\n",
      "Epoch 331, Training RMSE: 0.1029, Validation RMSE: 0.1037\n",
      "Epoch 332, Training RMSE: 0.1027, Validation RMSE: 0.1035\n",
      "Epoch 333, Training RMSE: 0.1025, Validation RMSE: 0.1033\n",
      "Epoch 334, Training RMSE: 0.1023, Validation RMSE: 0.1031\n",
      "Epoch 335, Training RMSE: 0.1021, Validation RMSE: 0.1029\n",
      "Epoch 336, Training RMSE: 0.1019, Validation RMSE: 0.1027\n",
      "Epoch 337, Training RMSE: 0.1017, Validation RMSE: 0.1026\n",
      "Epoch 338, Training RMSE: 0.1015, Validation RMSE: 0.1024\n",
      "Epoch 339, Training RMSE: 0.1013, Validation RMSE: 0.1022\n",
      "Epoch 340, Training RMSE: 0.1011, Validation RMSE: 0.1020\n",
      "Epoch 341, Training RMSE: 0.1009, Validation RMSE: 0.1018\n",
      "Epoch 342, Training RMSE: 0.1007, Validation RMSE: 0.1016\n",
      "Epoch 343, Training RMSE: 0.1006, Validation RMSE: 0.1014\n",
      "Epoch 344, Training RMSE: 0.1004, Validation RMSE: 0.1012\n",
      "Epoch 345, Training RMSE: 0.1002, Validation RMSE: 0.1011\n",
      "Epoch 346, Training RMSE: 0.1000, Validation RMSE: 0.1009\n",
      "Epoch 347, Training RMSE: 0.0998, Validation RMSE: 0.1007\n",
      "Epoch 348, Training RMSE: 0.0996, Validation RMSE: 0.1005\n",
      "Epoch 349, Training RMSE: 0.0995, Validation RMSE: 0.1003\n",
      "Epoch 350, Training RMSE: 0.0993, Validation RMSE: 0.1002\n",
      "Epoch 351, Training RMSE: 0.0991, Validation RMSE: 0.1000\n",
      "Epoch 352, Training RMSE: 0.0989, Validation RMSE: 0.0998\n",
      "Epoch 353, Training RMSE: 0.0988, Validation RMSE: 0.0996\n",
      "Epoch 354, Training RMSE: 0.0986, Validation RMSE: 0.0995\n",
      "Epoch 355, Training RMSE: 0.0984, Validation RMSE: 0.0993\n",
      "Epoch 356, Training RMSE: 0.0982, Validation RMSE: 0.0991\n",
      "Epoch 357, Training RMSE: 0.0981, Validation RMSE: 0.0989\n",
      "Epoch 358, Training RMSE: 0.0979, Validation RMSE: 0.0988\n",
      "Epoch 359, Training RMSE: 0.0977, Validation RMSE: 0.0986\n",
      "Epoch 360, Training RMSE: 0.0976, Validation RMSE: 0.0985\n",
      "Epoch 361, Training RMSE: 0.0974, Validation RMSE: 0.0983\n",
      "Epoch 362, Training RMSE: 0.0972, Validation RMSE: 0.0981\n",
      "Epoch 363, Training RMSE: 0.0971, Validation RMSE: 0.0980\n",
      "Epoch 364, Training RMSE: 0.0969, Validation RMSE: 0.0978\n",
      "Epoch 365, Training RMSE: 0.0968, Validation RMSE: 0.0977\n",
      "Epoch 366, Training RMSE: 0.0966, Validation RMSE: 0.0975\n",
      "Epoch 367, Training RMSE: 0.0964, Validation RMSE: 0.0973\n",
      "Epoch 368, Training RMSE: 0.0963, Validation RMSE: 0.0972\n",
      "Epoch 369, Training RMSE: 0.0961, Validation RMSE: 0.0970\n",
      "Epoch 370, Training RMSE: 0.0960, Validation RMSE: 0.0969\n",
      "Epoch 371, Training RMSE: 0.0958, Validation RMSE: 0.0967\n",
      "Epoch 372, Training RMSE: 0.0957, Validation RMSE: 0.0966\n",
      "Epoch 373, Training RMSE: 0.0955, Validation RMSE: 0.0964\n",
      "Epoch 374, Training RMSE: 0.0954, Validation RMSE: 0.0963\n",
      "Epoch 375, Training RMSE: 0.0952, Validation RMSE: 0.0961\n",
      "Epoch 376, Training RMSE: 0.0951, Validation RMSE: 0.0960\n",
      "Epoch 377, Training RMSE: 0.0949, Validation RMSE: 0.0959\n",
      "Epoch 378, Training RMSE: 0.0948, Validation RMSE: 0.0957\n",
      "Epoch 379, Training RMSE: 0.0947, Validation RMSE: 0.0956\n",
      "Epoch 380, Training RMSE: 0.0945, Validation RMSE: 0.0954\n",
      "Epoch 381, Training RMSE: 0.0944, Validation RMSE: 0.0953\n",
      "Epoch 382, Training RMSE: 0.0942, Validation RMSE: 0.0952\n",
      "Epoch 383, Training RMSE: 0.0941, Validation RMSE: 0.0950\n",
      "Epoch 384, Training RMSE: 0.0940, Validation RMSE: 0.0949\n",
      "Epoch 385, Training RMSE: 0.0938, Validation RMSE: 0.0948\n",
      "Epoch 386, Training RMSE: 0.0937, Validation RMSE: 0.0946\n",
      "Epoch 387, Training RMSE: 0.0935, Validation RMSE: 0.0945\n",
      "Epoch 388, Training RMSE: 0.0934, Validation RMSE: 0.0943\n",
      "Epoch 389, Training RMSE: 0.0933, Validation RMSE: 0.0942\n",
      "Epoch 390, Training RMSE: 0.0931, Validation RMSE: 0.0941\n",
      "Epoch 391, Training RMSE: 0.0930, Validation RMSE: 0.0940\n",
      "Epoch 392, Training RMSE: 0.0929, Validation RMSE: 0.0938\n",
      "Epoch 393, Training RMSE: 0.0928, Validation RMSE: 0.0937\n",
      "Epoch 394, Training RMSE: 0.0927, Validation RMSE: 0.0936\n",
      "Epoch 395, Training RMSE: 0.0926, Validation RMSE: 0.0935\n",
      "Epoch 396, Training RMSE: 0.0925, Validation RMSE: 0.0935\n",
      "Epoch 397, Training RMSE: 0.0924, Validation RMSE: 0.0933\n",
      "Epoch 398, Training RMSE: 0.0922, Validation RMSE: 0.0931\n",
      "Epoch 399, Training RMSE: 0.0920, Validation RMSE: 0.0930\n",
      "Epoch 400, Training RMSE: 0.0919, Validation RMSE: 0.0929\n",
      "Epoch 401, Training RMSE: 0.0919, Validation RMSE: 0.0928\n",
      "Epoch 402, Training RMSE: 0.0917, Validation RMSE: 0.0927\n",
      "Epoch 403, Training RMSE: 0.0915, Validation RMSE: 0.0925\n",
      "Epoch 404, Training RMSE: 0.0914, Validation RMSE: 0.0924\n",
      "Epoch 405, Training RMSE: 0.0914, Validation RMSE: 0.0923\n",
      "Epoch 406, Training RMSE: 0.0913, Validation RMSE: 0.0922\n",
      "Epoch 407, Training RMSE: 0.0911, Validation RMSE: 0.0921\n",
      "Epoch 408, Training RMSE: 0.0910, Validation RMSE: 0.0920\n",
      "Epoch 409, Training RMSE: 0.0909, Validation RMSE: 0.0919\n",
      "Epoch 410, Training RMSE: 0.0908, Validation RMSE: 0.0918\n",
      "Epoch 411, Training RMSE: 0.0907, Validation RMSE: 0.0917\n",
      "Epoch 412, Training RMSE: 0.0906, Validation RMSE: 0.0915\n",
      "Epoch 413, Training RMSE: 0.0905, Validation RMSE: 0.0914\n",
      "Epoch 414, Training RMSE: 0.0904, Validation RMSE: 0.0913\n",
      "Epoch 415, Training RMSE: 0.0903, Validation RMSE: 0.0912\n",
      "Epoch 416, Training RMSE: 0.0901, Validation RMSE: 0.0911\n",
      "Epoch 417, Training RMSE: 0.0900, Validation RMSE: 0.0910\n",
      "Epoch 418, Training RMSE: 0.0899, Validation RMSE: 0.0909\n",
      "Epoch 419, Training RMSE: 0.0898, Validation RMSE: 0.0908\n",
      "Epoch 420, Training RMSE: 0.0897, Validation RMSE: 0.0907\n",
      "Epoch 421, Training RMSE: 0.0896, Validation RMSE: 0.0906\n",
      "Epoch 422, Training RMSE: 0.0895, Validation RMSE: 0.0905\n",
      "Epoch 423, Training RMSE: 0.0894, Validation RMSE: 0.0904\n",
      "Epoch 424, Training RMSE: 0.0893, Validation RMSE: 0.0903\n",
      "Epoch 425, Training RMSE: 0.0892, Validation RMSE: 0.0902\n",
      "Epoch 426, Training RMSE: 0.0891, Validation RMSE: 0.0901\n",
      "Epoch 427, Training RMSE: 0.0890, Validation RMSE: 0.0900\n",
      "Epoch 428, Training RMSE: 0.0889, Validation RMSE: 0.0899\n",
      "Epoch 429, Training RMSE: 0.0888, Validation RMSE: 0.0898\n",
      "Epoch 430, Training RMSE: 0.0887, Validation RMSE: 0.0897\n",
      "Epoch 431, Training RMSE: 0.0886, Validation RMSE: 0.0896\n",
      "Epoch 432, Training RMSE: 0.0886, Validation RMSE: 0.0896\n",
      "Epoch 433, Training RMSE: 0.0885, Validation RMSE: 0.0895\n",
      "Epoch 434, Training RMSE: 0.0884, Validation RMSE: 0.0894\n",
      "Epoch 435, Training RMSE: 0.0883, Validation RMSE: 0.0893\n",
      "Epoch 436, Training RMSE: 0.0882, Validation RMSE: 0.0892\n",
      "Epoch 437, Training RMSE: 0.0881, Validation RMSE: 0.0891\n",
      "Epoch 438, Training RMSE: 0.0880, Validation RMSE: 0.0890\n",
      "Epoch 439, Training RMSE: 0.0879, Validation RMSE: 0.0889\n",
      "Epoch 440, Training RMSE: 0.0878, Validation RMSE: 0.0888\n",
      "Epoch 441, Training RMSE: 0.0877, Validation RMSE: 0.0887\n",
      "Epoch 442, Training RMSE: 0.0876, Validation RMSE: 0.0887\n",
      "Epoch 443, Training RMSE: 0.0875, Validation RMSE: 0.0886\n",
      "Epoch 444, Training RMSE: 0.0874, Validation RMSE: 0.0885\n",
      "Epoch 445, Training RMSE: 0.0874, Validation RMSE: 0.0884\n",
      "Epoch 446, Training RMSE: 0.0873, Validation RMSE: 0.0883\n",
      "Epoch 447, Training RMSE: 0.0872, Validation RMSE: 0.0882\n",
      "Epoch 448, Training RMSE: 0.0871, Validation RMSE: 0.0881\n",
      "Epoch 449, Training RMSE: 0.0870, Validation RMSE: 0.0880\n",
      "Epoch 450, Training RMSE: 0.0869, Validation RMSE: 0.0880\n",
      "Epoch 451, Training RMSE: 0.0868, Validation RMSE: 0.0879\n",
      "Epoch 452, Training RMSE: 0.0867, Validation RMSE: 0.0878\n",
      "Epoch 453, Training RMSE: 0.0867, Validation RMSE: 0.0877\n",
      "Epoch 454, Training RMSE: 0.0866, Validation RMSE: 0.0876\n",
      "Epoch 455, Training RMSE: 0.0865, Validation RMSE: 0.0876\n",
      "Epoch 456, Training RMSE: 0.0864, Validation RMSE: 0.0875\n",
      "Epoch 457, Training RMSE: 0.0863, Validation RMSE: 0.0874\n",
      "Epoch 458, Training RMSE: 0.0862, Validation RMSE: 0.0873\n",
      "Epoch 459, Training RMSE: 0.0861, Validation RMSE: 0.0872\n",
      "Epoch 460, Training RMSE: 0.0861, Validation RMSE: 0.0872\n",
      "Epoch 461, Training RMSE: 0.0860, Validation RMSE: 0.0871\n",
      "Epoch 462, Training RMSE: 0.0859, Validation RMSE: 0.0870\n",
      "Epoch 463, Training RMSE: 0.0858, Validation RMSE: 0.0869\n",
      "Epoch 464, Training RMSE: 0.0857, Validation RMSE: 0.0868\n",
      "Epoch 465, Training RMSE: 0.0856, Validation RMSE: 0.0868\n",
      "Epoch 466, Training RMSE: 0.0856, Validation RMSE: 0.0867\n",
      "Epoch 467, Training RMSE: 0.0855, Validation RMSE: 0.0866\n",
      "Epoch 468, Training RMSE: 0.0854, Validation RMSE: 0.0866\n",
      "Epoch 469, Training RMSE: 0.0854, Validation RMSE: 0.0865\n",
      "Epoch 470, Training RMSE: 0.0853, Validation RMSE: 0.0865\n",
      "Epoch 471, Training RMSE: 0.0853, Validation RMSE: 0.0865\n",
      "Epoch 472, Training RMSE: 0.0854, Validation RMSE: 0.0865\n",
      "Epoch 473, Training RMSE: 0.0853, Validation RMSE: 0.0864\n",
      "Epoch 474, Training RMSE: 0.0851, Validation RMSE: 0.0863\n",
      "Epoch 475, Training RMSE: 0.0849, Validation RMSE: 0.0860\n",
      "Epoch 476, Training RMSE: 0.0848, Validation RMSE: 0.0859\n",
      "Epoch 477, Training RMSE: 0.0848, Validation RMSE: 0.0859\n",
      "Epoch 478, Training RMSE: 0.0848, Validation RMSE: 0.0859\n",
      "Epoch 479, Training RMSE: 0.0846, Validation RMSE: 0.0858\n",
      "Epoch 480, Training RMSE: 0.0845, Validation RMSE: 0.0856\n",
      "Epoch 481, Training RMSE: 0.0844, Validation RMSE: 0.0856\n",
      "Epoch 482, Training RMSE: 0.0844, Validation RMSE: 0.0856\n",
      "Epoch 483, Training RMSE: 0.0843, Validation RMSE: 0.0855\n",
      "Epoch 484, Training RMSE: 0.0842, Validation RMSE: 0.0854\n",
      "Epoch 485, Training RMSE: 0.0841, Validation RMSE: 0.0853\n",
      "Epoch 486, Training RMSE: 0.0841, Validation RMSE: 0.0852\n",
      "Epoch 487, Training RMSE: 0.0840, Validation RMSE: 0.0852\n",
      "Epoch 488, Training RMSE: 0.0839, Validation RMSE: 0.0851\n",
      "Epoch 489, Training RMSE: 0.0838, Validation RMSE: 0.0850\n",
      "Epoch 490, Training RMSE: 0.0838, Validation RMSE: 0.0850\n",
      "Epoch 491, Training RMSE: 0.0837, Validation RMSE: 0.0849\n",
      "Epoch 492, Training RMSE: 0.0836, Validation RMSE: 0.0848\n",
      "Epoch 493, Training RMSE: 0.0836, Validation RMSE: 0.0848\n",
      "Epoch 494, Training RMSE: 0.0835, Validation RMSE: 0.0847\n",
      "Epoch 495, Training RMSE: 0.0834, Validation RMSE: 0.0846\n",
      "Epoch 496, Training RMSE: 0.0834, Validation RMSE: 0.0846\n",
      "Epoch 497, Training RMSE: 0.0833, Validation RMSE: 0.0845\n",
      "Epoch 498, Training RMSE: 0.0832, Validation RMSE: 0.0844\n",
      "Epoch 499, Training RMSE: 0.0831, Validation RMSE: 0.0844\n",
      "Epoch 500, Training RMSE: 0.0831, Validation RMSE: 0.0843\n",
      "Epoch 501, Training RMSE: 0.0830, Validation RMSE: 0.0842\n",
      "Epoch 502, Training RMSE: 0.0829, Validation RMSE: 0.0842\n",
      "Epoch 503, Training RMSE: 0.0829, Validation RMSE: 0.0841\n",
      "Epoch 504, Training RMSE: 0.0828, Validation RMSE: 0.0841\n",
      "Epoch 505, Training RMSE: 0.0827, Validation RMSE: 0.0840\n",
      "Epoch 506, Training RMSE: 0.0827, Validation RMSE: 0.0839\n",
      "Epoch 507, Training RMSE: 0.0826, Validation RMSE: 0.0839\n",
      "Epoch 508, Training RMSE: 0.0826, Validation RMSE: 0.0838\n",
      "Epoch 509, Training RMSE: 0.0825, Validation RMSE: 0.0837\n",
      "Epoch 510, Training RMSE: 0.0824, Validation RMSE: 0.0837\n",
      "Epoch 511, Training RMSE: 0.0824, Validation RMSE: 0.0836\n",
      "Epoch 512, Training RMSE: 0.0823, Validation RMSE: 0.0835\n",
      "Epoch 513, Training RMSE: 0.0822, Validation RMSE: 0.0835\n",
      "Epoch 514, Training RMSE: 0.0822, Validation RMSE: 0.0834\n",
      "Epoch 515, Training RMSE: 0.0821, Validation RMSE: 0.0834\n",
      "Epoch 516, Training RMSE: 0.0820, Validation RMSE: 0.0833\n",
      "Epoch 517, Training RMSE: 0.0820, Validation RMSE: 0.0832\n",
      "Epoch 518, Training RMSE: 0.0819, Validation RMSE: 0.0832\n",
      "Epoch 519, Training RMSE: 0.0819, Validation RMSE: 0.0831\n",
      "Epoch 520, Training RMSE: 0.0818, Validation RMSE: 0.0831\n",
      "Epoch 521, Training RMSE: 0.0817, Validation RMSE: 0.0830\n",
      "Epoch 522, Training RMSE: 0.0817, Validation RMSE: 0.0829\n",
      "Epoch 523, Training RMSE: 0.0816, Validation RMSE: 0.0829\n",
      "Epoch 524, Training RMSE: 0.0816, Validation RMSE: 0.0828\n",
      "Epoch 525, Training RMSE: 0.0815, Validation RMSE: 0.0828\n",
      "Epoch 526, Training RMSE: 0.0814, Validation RMSE: 0.0827\n",
      "Epoch 527, Training RMSE: 0.0814, Validation RMSE: 0.0826\n",
      "Epoch 528, Training RMSE: 0.0813, Validation RMSE: 0.0826\n",
      "Epoch 529, Training RMSE: 0.0813, Validation RMSE: 0.0825\n",
      "Epoch 530, Training RMSE: 0.0812, Validation RMSE: 0.0825\n",
      "Epoch 531, Training RMSE: 0.0811, Validation RMSE: 0.0824\n",
      "Epoch 532, Training RMSE: 0.0811, Validation RMSE: 0.0824\n",
      "Epoch 533, Training RMSE: 0.0810, Validation RMSE: 0.0823\n",
      "Epoch 534, Training RMSE: 0.0810, Validation RMSE: 0.0822\n",
      "Epoch 535, Training RMSE: 0.0809, Validation RMSE: 0.0822\n",
      "Epoch 536, Training RMSE: 0.0809, Validation RMSE: 0.0821\n",
      "Epoch 537, Training RMSE: 0.0808, Validation RMSE: 0.0821\n",
      "Epoch 538, Training RMSE: 0.0807, Validation RMSE: 0.0820\n",
      "Epoch 539, Training RMSE: 0.0807, Validation RMSE: 0.0819\n",
      "Epoch 540, Training RMSE: 0.0806, Validation RMSE: 0.0819\n",
      "Epoch 541, Training RMSE: 0.0806, Validation RMSE: 0.0818\n",
      "Epoch 542, Training RMSE: 0.0805, Validation RMSE: 0.0818\n",
      "Epoch 543, Training RMSE: 0.0805, Validation RMSE: 0.0817\n",
      "Epoch 544, Training RMSE: 0.0804, Validation RMSE: 0.0817\n",
      "Epoch 545, Training RMSE: 0.0804, Validation RMSE: 0.0816\n",
      "Epoch 546, Training RMSE: 0.0803, Validation RMSE: 0.0816\n",
      "Epoch 547, Training RMSE: 0.0802, Validation RMSE: 0.0815\n",
      "Epoch 548, Training RMSE: 0.0802, Validation RMSE: 0.0814\n",
      "Epoch 549, Training RMSE: 0.0801, Validation RMSE: 0.0814\n",
      "Epoch 550, Training RMSE: 0.0801, Validation RMSE: 0.0813\n",
      "Epoch 551, Training RMSE: 0.0800, Validation RMSE: 0.0813\n",
      "Epoch 552, Training RMSE: 0.0800, Validation RMSE: 0.0813\n",
      "Epoch 553, Training RMSE: 0.0800, Validation RMSE: 0.0813\n",
      "Epoch 554, Training RMSE: 0.0800, Validation RMSE: 0.0813\n",
      "Epoch 555, Training RMSE: 0.0801, Validation RMSE: 0.0814\n",
      "Epoch 556, Training RMSE: 0.0801, Validation RMSE: 0.0814\n",
      "Epoch 557, Training RMSE: 0.0802, Validation RMSE: 0.0815\n",
      "Epoch 558, Training RMSE: 0.0800, Validation RMSE: 0.0812\n",
      "Epoch 559, Training RMSE: 0.0797, Validation RMSE: 0.0810\n",
      "Epoch 560, Training RMSE: 0.0796, Validation RMSE: 0.0808\n",
      "Epoch 561, Training RMSE: 0.0796, Validation RMSE: 0.0808\n",
      "Epoch 562, Training RMSE: 0.0797, Validation RMSE: 0.0810\n",
      "Epoch 563, Training RMSE: 0.0796, Validation RMSE: 0.0809\n",
      "Epoch 564, Training RMSE: 0.0794, Validation RMSE: 0.0807\n",
      "Epoch 565, Training RMSE: 0.0793, Validation RMSE: 0.0806\n",
      "Epoch 566, Training RMSE: 0.0793, Validation RMSE: 0.0806\n",
      "Epoch 567, Training RMSE: 0.0793, Validation RMSE: 0.0806\n",
      "Epoch 568, Training RMSE: 0.0792, Validation RMSE: 0.0805\n",
      "Epoch 569, Training RMSE: 0.0791, Validation RMSE: 0.0804\n",
      "Epoch 570, Training RMSE: 0.0791, Validation RMSE: 0.0804\n",
      "Epoch 571, Training RMSE: 0.0791, Validation RMSE: 0.0804\n",
      "Epoch 572, Training RMSE: 0.0791, Validation RMSE: 0.0804\n",
      "Epoch 573, Training RMSE: 0.0790, Validation RMSE: 0.0802\n",
      "Epoch 574, Training RMSE: 0.0789, Validation RMSE: 0.0802\n",
      "Epoch 575, Training RMSE: 0.0789, Validation RMSE: 0.0802\n",
      "Epoch 576, Training RMSE: 0.0789, Validation RMSE: 0.0801\n",
      "Epoch 577, Training RMSE: 0.0788, Validation RMSE: 0.0801\n",
      "Epoch 578, Training RMSE: 0.0787, Validation RMSE: 0.0800\n",
      "Epoch 579, Training RMSE: 0.0787, Validation RMSE: 0.0800\n",
      "Epoch 580, Training RMSE: 0.0787, Validation RMSE: 0.0800\n",
      "Epoch 581, Training RMSE: 0.0786, Validation RMSE: 0.0799\n",
      "Epoch 582, Training RMSE: 0.0785, Validation RMSE: 0.0798\n",
      "Epoch 583, Training RMSE: 0.0785, Validation RMSE: 0.0798\n",
      "Epoch 584, Training RMSE: 0.0785, Validation RMSE: 0.0797\n",
      "Epoch 585, Training RMSE: 0.0784, Validation RMSE: 0.0797\n",
      "Epoch 586, Training RMSE: 0.0784, Validation RMSE: 0.0796\n",
      "Epoch 587, Training RMSE: 0.0783, Validation RMSE: 0.0796\n",
      "Epoch 588, Training RMSE: 0.0783, Validation RMSE: 0.0796\n",
      "Epoch 589, Training RMSE: 0.0782, Validation RMSE: 0.0795\n",
      "Epoch 590, Training RMSE: 0.0782, Validation RMSE: 0.0795\n",
      "Epoch 591, Training RMSE: 0.0781, Validation RMSE: 0.0794\n",
      "Epoch 592, Training RMSE: 0.0781, Validation RMSE: 0.0794\n",
      "Epoch 593, Training RMSE: 0.0781, Validation RMSE: 0.0793\n",
      "Epoch 594, Training RMSE: 0.0780, Validation RMSE: 0.0793\n",
      "Epoch 595, Training RMSE: 0.0780, Validation RMSE: 0.0792\n",
      "Epoch 596, Training RMSE: 0.0779, Validation RMSE: 0.0792\n",
      "Epoch 597, Training RMSE: 0.0779, Validation RMSE: 0.0792\n",
      "Epoch 598, Training RMSE: 0.0778, Validation RMSE: 0.0791\n",
      "Epoch 599, Training RMSE: 0.0778, Validation RMSE: 0.0791\n",
      "Epoch 600, Training RMSE: 0.0778, Validation RMSE: 0.0790\n",
      "Epoch 601, Training RMSE: 0.0777, Validation RMSE: 0.0790\n",
      "Epoch 602, Training RMSE: 0.0777, Validation RMSE: 0.0789\n",
      "Epoch 603, Training RMSE: 0.0776, Validation RMSE: 0.0789\n",
      "Epoch 604, Training RMSE: 0.0776, Validation RMSE: 0.0789\n",
      "Epoch 605, Training RMSE: 0.0775, Validation RMSE: 0.0788\n",
      "Epoch 606, Training RMSE: 0.0775, Validation RMSE: 0.0788\n",
      "Epoch 607, Training RMSE: 0.0775, Validation RMSE: 0.0787\n",
      "Epoch 608, Training RMSE: 0.0774, Validation RMSE: 0.0787\n",
      "Epoch 609, Training RMSE: 0.0774, Validation RMSE: 0.0786\n",
      "Epoch 610, Training RMSE: 0.0773, Validation RMSE: 0.0786\n",
      "Epoch 611, Training RMSE: 0.0773, Validation RMSE: 0.0786\n",
      "Epoch 612, Training RMSE: 0.0772, Validation RMSE: 0.0785\n",
      "Epoch 613, Training RMSE: 0.0772, Validation RMSE: 0.0785\n",
      "Epoch 614, Training RMSE: 0.0772, Validation RMSE: 0.0784\n",
      "Epoch 615, Training RMSE: 0.0771, Validation RMSE: 0.0784\n",
      "Epoch 616, Training RMSE: 0.0771, Validation RMSE: 0.0784\n",
      "Epoch 617, Training RMSE: 0.0771, Validation RMSE: 0.0784\n",
      "Epoch 618, Training RMSE: 0.0771, Validation RMSE: 0.0784\n",
      "Epoch 619, Training RMSE: 0.0771, Validation RMSE: 0.0784\n",
      "Epoch 620, Training RMSE: 0.0772, Validation RMSE: 0.0785\n",
      "Epoch 621, Training RMSE: 0.0772, Validation RMSE: 0.0784\n",
      "Epoch 622, Training RMSE: 0.0771, Validation RMSE: 0.0784\n",
      "Epoch 623, Training RMSE: 0.0769, Validation RMSE: 0.0782\n",
      "Epoch 624, Training RMSE: 0.0768, Validation RMSE: 0.0781\n",
      "Epoch 625, Training RMSE: 0.0768, Validation RMSE: 0.0780\n",
      "Epoch 626, Training RMSE: 0.0768, Validation RMSE: 0.0780\n",
      "Epoch 627, Training RMSE: 0.0768, Validation RMSE: 0.0781\n",
      "Epoch 628, Training RMSE: 0.0767, Validation RMSE: 0.0780\n",
      "Epoch 629, Training RMSE: 0.0766, Validation RMSE: 0.0779\n",
      "Epoch 630, Training RMSE: 0.0766, Validation RMSE: 0.0778\n",
      "Epoch 631, Training RMSE: 0.0765, Validation RMSE: 0.0778\n",
      "Epoch 632, Training RMSE: 0.0765, Validation RMSE: 0.0778\n",
      "Epoch 633, Training RMSE: 0.0765, Validation RMSE: 0.0777\n",
      "Epoch 634, Training RMSE: 0.0764, Validation RMSE: 0.0777\n",
      "Epoch 635, Training RMSE: 0.0764, Validation RMSE: 0.0777\n",
      "Epoch 636, Training RMSE: 0.0763, Validation RMSE: 0.0776\n",
      "Epoch 637, Training RMSE: 0.0763, Validation RMSE: 0.0776\n",
      "Epoch 638, Training RMSE: 0.0763, Validation RMSE: 0.0775\n",
      "Epoch 639, Training RMSE: 0.0762, Validation RMSE: 0.0775\n",
      "Epoch 640, Training RMSE: 0.0762, Validation RMSE: 0.0775\n",
      "Epoch 641, Training RMSE: 0.0762, Validation RMSE: 0.0774\n",
      "Epoch 642, Training RMSE: 0.0761, Validation RMSE: 0.0774\n",
      "Epoch 643, Training RMSE: 0.0761, Validation RMSE: 0.0773\n",
      "Epoch 644, Training RMSE: 0.0760, Validation RMSE: 0.0773\n",
      "Epoch 645, Training RMSE: 0.0760, Validation RMSE: 0.0773\n",
      "Epoch 646, Training RMSE: 0.0760, Validation RMSE: 0.0773\n",
      "Epoch 647, Training RMSE: 0.0759, Validation RMSE: 0.0772\n",
      "Epoch 648, Training RMSE: 0.0759, Validation RMSE: 0.0772\n",
      "Epoch 649, Training RMSE: 0.0759, Validation RMSE: 0.0771\n",
      "Epoch 650, Training RMSE: 0.0758, Validation RMSE: 0.0771\n",
      "Epoch 651, Training RMSE: 0.0758, Validation RMSE: 0.0771\n",
      "Epoch 652, Training RMSE: 0.0758, Validation RMSE: 0.0770\n",
      "Epoch 653, Training RMSE: 0.0757, Validation RMSE: 0.0770\n",
      "Epoch 654, Training RMSE: 0.0757, Validation RMSE: 0.0770\n",
      "Epoch 655, Training RMSE: 0.0756, Validation RMSE: 0.0769\n",
      "Epoch 656, Training RMSE: 0.0756, Validation RMSE: 0.0769\n",
      "Epoch 657, Training RMSE: 0.0756, Validation RMSE: 0.0768\n",
      "Epoch 658, Training RMSE: 0.0755, Validation RMSE: 0.0768\n",
      "Epoch 659, Training RMSE: 0.0755, Validation RMSE: 0.0768\n",
      "Epoch 660, Training RMSE: 0.0755, Validation RMSE: 0.0768\n",
      "Epoch 661, Training RMSE: 0.0754, Validation RMSE: 0.0767\n",
      "Epoch 662, Training RMSE: 0.0754, Validation RMSE: 0.0767\n",
      "Epoch 663, Training RMSE: 0.0754, Validation RMSE: 0.0766\n",
      "Epoch 664, Training RMSE: 0.0753, Validation RMSE: 0.0766\n",
      "Epoch 665, Training RMSE: 0.0753, Validation RMSE: 0.0766\n",
      "Epoch 666, Training RMSE: 0.0753, Validation RMSE: 0.0765\n",
      "Epoch 667, Training RMSE: 0.0752, Validation RMSE: 0.0765\n",
      "Epoch 668, Training RMSE: 0.0752, Validation RMSE: 0.0765\n",
      "Epoch 669, Training RMSE: 0.0752, Validation RMSE: 0.0764\n",
      "Epoch 670, Training RMSE: 0.0751, Validation RMSE: 0.0764\n",
      "Epoch 671, Training RMSE: 0.0751, Validation RMSE: 0.0764\n",
      "Epoch 672, Training RMSE: 0.0751, Validation RMSE: 0.0763\n",
      "Epoch 673, Training RMSE: 0.0750, Validation RMSE: 0.0763\n",
      "Epoch 674, Training RMSE: 0.0750, Validation RMSE: 0.0763\n",
      "Epoch 675, Training RMSE: 0.0750, Validation RMSE: 0.0762\n",
      "Epoch 676, Training RMSE: 0.0749, Validation RMSE: 0.0762\n",
      "Epoch 677, Training RMSE: 0.0749, Validation RMSE: 0.0762\n",
      "Epoch 678, Training RMSE: 0.0749, Validation RMSE: 0.0761\n",
      "Epoch 679, Training RMSE: 0.0748, Validation RMSE: 0.0761\n",
      "Epoch 680, Training RMSE: 0.0748, Validation RMSE: 0.0761\n",
      "Epoch 681, Training RMSE: 0.0748, Validation RMSE: 0.0760\n",
      "Epoch 682, Training RMSE: 0.0747, Validation RMSE: 0.0760\n",
      "Epoch 683, Training RMSE: 0.0747, Validation RMSE: 0.0760\n",
      "Epoch 684, Training RMSE: 0.0747, Validation RMSE: 0.0759\n",
      "Epoch 685, Training RMSE: 0.0746, Validation RMSE: 0.0759\n",
      "Epoch 686, Training RMSE: 0.0746, Validation RMSE: 0.0759\n",
      "Epoch 687, Training RMSE: 0.0746, Validation RMSE: 0.0759\n",
      "Epoch 688, Training RMSE: 0.0745, Validation RMSE: 0.0758\n",
      "Epoch 689, Training RMSE: 0.0745, Validation RMSE: 0.0758\n",
      "Epoch 690, Training RMSE: 0.0745, Validation RMSE: 0.0758\n",
      "Epoch 691, Training RMSE: 0.0744, Validation RMSE: 0.0757\n",
      "Epoch 692, Training RMSE: 0.0744, Validation RMSE: 0.0757\n",
      "Epoch 693, Training RMSE: 0.0744, Validation RMSE: 0.0757\n",
      "Epoch 694, Training RMSE: 0.0743, Validation RMSE: 0.0756\n",
      "Epoch 695, Training RMSE: 0.0743, Validation RMSE: 0.0756\n",
      "Epoch 696, Training RMSE: 0.0743, Validation RMSE: 0.0756\n",
      "Epoch 697, Training RMSE: 0.0743, Validation RMSE: 0.0756\n",
      "Epoch 698, Training RMSE: 0.0742, Validation RMSE: 0.0755\n",
      "Epoch 699, Training RMSE: 0.0742, Validation RMSE: 0.0755\n",
      "Epoch 700, Training RMSE: 0.0742, Validation RMSE: 0.0755\n",
      "Epoch 701, Training RMSE: 0.0742, Validation RMSE: 0.0755\n",
      "Epoch 702, Training RMSE: 0.0743, Validation RMSE: 0.0755\n",
      "Epoch 703, Training RMSE: 0.0744, Validation RMSE: 0.0757\n",
      "Epoch 704, Training RMSE: 0.0746, Validation RMSE: 0.0759\n",
      "Epoch 705, Training RMSE: 0.0748, Validation RMSE: 0.0762\n",
      "Epoch 706, Training RMSE: 0.0749, Validation RMSE: 0.0762\n",
      "Epoch 707, Training RMSE: 0.0746, Validation RMSE: 0.0760\n",
      "Epoch 708, Training RMSE: 0.0741, Validation RMSE: 0.0754\n",
      "Epoch 709, Training RMSE: 0.0739, Validation RMSE: 0.0752\n",
      "Epoch 710, Training RMSE: 0.0741, Validation RMSE: 0.0755\n",
      "Epoch 711, Training RMSE: 0.0743, Validation RMSE: 0.0756\n",
      "Epoch 712, Training RMSE: 0.0741, Validation RMSE: 0.0755\n",
      "Epoch 713, Training RMSE: 0.0738, Validation RMSE: 0.0751\n",
      "Epoch 714, Training RMSE: 0.0738, Validation RMSE: 0.0751\n",
      "Epoch 715, Training RMSE: 0.0740, Validation RMSE: 0.0753\n",
      "Epoch 716, Training RMSE: 0.0739, Validation RMSE: 0.0752\n",
      "Epoch 717, Training RMSE: 0.0737, Validation RMSE: 0.0751\n",
      "Epoch 718, Training RMSE: 0.0736, Validation RMSE: 0.0750\n",
      "Epoch 719, Training RMSE: 0.0737, Validation RMSE: 0.0751\n",
      "Epoch 720, Training RMSE: 0.0737, Validation RMSE: 0.0751\n",
      "Epoch 721, Training RMSE: 0.0736, Validation RMSE: 0.0749\n",
      "Epoch 722, Training RMSE: 0.0735, Validation RMSE: 0.0749\n",
      "Epoch 723, Training RMSE: 0.0736, Validation RMSE: 0.0749\n",
      "Epoch 724, Training RMSE: 0.0736, Validation RMSE: 0.0749\n",
      "Epoch 725, Training RMSE: 0.0735, Validation RMSE: 0.0748\n",
      "Epoch 726, Training RMSE: 0.0734, Validation RMSE: 0.0748\n",
      "Epoch 727, Training RMSE: 0.0734, Validation RMSE: 0.0748\n",
      "Epoch 728, Training RMSE: 0.0734, Validation RMSE: 0.0748\n",
      "Epoch 729, Training RMSE: 0.0734, Validation RMSE: 0.0747\n",
      "Epoch 730, Training RMSE: 0.0733, Validation RMSE: 0.0746\n",
      "Epoch 731, Training RMSE: 0.0733, Validation RMSE: 0.0747\n",
      "Epoch 732, Training RMSE: 0.0733, Validation RMSE: 0.0746\n",
      "Epoch 733, Training RMSE: 0.0732, Validation RMSE: 0.0746\n",
      "Epoch 734, Training RMSE: 0.0732, Validation RMSE: 0.0745\n",
      "Epoch 735, Training RMSE: 0.0732, Validation RMSE: 0.0745\n",
      "Epoch 736, Training RMSE: 0.0732, Validation RMSE: 0.0745\n",
      "Epoch 737, Training RMSE: 0.0731, Validation RMSE: 0.0745\n",
      "Epoch 738, Training RMSE: 0.0731, Validation RMSE: 0.0744\n",
      "Epoch 739, Training RMSE: 0.0731, Validation RMSE: 0.0744\n",
      "Epoch 740, Training RMSE: 0.0730, Validation RMSE: 0.0744\n",
      "Epoch 741, Training RMSE: 0.0730, Validation RMSE: 0.0744\n",
      "Epoch 742, Training RMSE: 0.0730, Validation RMSE: 0.0743\n",
      "Epoch 743, Training RMSE: 0.0729, Validation RMSE: 0.0743\n",
      "Epoch 744, Training RMSE: 0.0729, Validation RMSE: 0.0743\n",
      "Epoch 745, Training RMSE: 0.0729, Validation RMSE: 0.0743\n",
      "Epoch 746, Training RMSE: 0.0729, Validation RMSE: 0.0743\n",
      "Epoch 747, Training RMSE: 0.0728, Validation RMSE: 0.0742\n",
      "Epoch 748, Training RMSE: 0.0728, Validation RMSE: 0.0742\n",
      "Epoch 749, Training RMSE: 0.0728, Validation RMSE: 0.0742\n",
      "Epoch 750, Training RMSE: 0.0728, Validation RMSE: 0.0741\n",
      "Epoch 751, Training RMSE: 0.0727, Validation RMSE: 0.0741\n",
      "Epoch 752, Training RMSE: 0.0727, Validation RMSE: 0.0741\n",
      "Epoch 753, Training RMSE: 0.0727, Validation RMSE: 0.0741\n",
      "Epoch 754, Training RMSE: 0.0727, Validation RMSE: 0.0740\n",
      "Epoch 755, Training RMSE: 0.0726, Validation RMSE: 0.0740\n",
      "Epoch 756, Training RMSE: 0.0726, Validation RMSE: 0.0740\n",
      "Epoch 757, Training RMSE: 0.0726, Validation RMSE: 0.0740\n",
      "Epoch 758, Training RMSE: 0.0726, Validation RMSE: 0.0739\n",
      "Epoch 759, Training RMSE: 0.0725, Validation RMSE: 0.0739\n",
      "Epoch 760, Training RMSE: 0.0725, Validation RMSE: 0.0739\n",
      "Epoch 761, Training RMSE: 0.0725, Validation RMSE: 0.0739\n",
      "Epoch 762, Training RMSE: 0.0725, Validation RMSE: 0.0738\n",
      "Epoch 763, Training RMSE: 0.0724, Validation RMSE: 0.0738\n",
      "Epoch 764, Training RMSE: 0.0724, Validation RMSE: 0.0738\n",
      "Epoch 765, Training RMSE: 0.0724, Validation RMSE: 0.0738\n",
      "Epoch 766, Training RMSE: 0.0723, Validation RMSE: 0.0737\n",
      "Epoch 767, Training RMSE: 0.0723, Validation RMSE: 0.0737\n",
      "Epoch 768, Training RMSE: 0.0723, Validation RMSE: 0.0737\n",
      "Epoch 769, Training RMSE: 0.0723, Validation RMSE: 0.0737\n",
      "Epoch 770, Training RMSE: 0.0722, Validation RMSE: 0.0736\n",
      "Epoch 771, Training RMSE: 0.0722, Validation RMSE: 0.0736\n",
      "Epoch 772, Training RMSE: 0.0722, Validation RMSE: 0.0736\n",
      "Epoch 773, Training RMSE: 0.0722, Validation RMSE: 0.0736\n",
      "Epoch 774, Training RMSE: 0.0721, Validation RMSE: 0.0735\n",
      "Epoch 775, Training RMSE: 0.0721, Validation RMSE: 0.0735\n",
      "Epoch 776, Training RMSE: 0.0721, Validation RMSE: 0.0735\n",
      "Epoch 777, Training RMSE: 0.0721, Validation RMSE: 0.0735\n",
      "Epoch 778, Training RMSE: 0.0720, Validation RMSE: 0.0734\n",
      "Epoch 779, Training RMSE: 0.0720, Validation RMSE: 0.0734\n",
      "Epoch 780, Training RMSE: 0.0720, Validation RMSE: 0.0734\n",
      "Epoch 781, Training RMSE: 0.0720, Validation RMSE: 0.0734\n",
      "Epoch 782, Training RMSE: 0.0720, Validation RMSE: 0.0733\n",
      "Epoch 783, Training RMSE: 0.0719, Validation RMSE: 0.0733\n",
      "Epoch 784, Training RMSE: 0.0719, Validation RMSE: 0.0733\n",
      "Epoch 785, Training RMSE: 0.0719, Validation RMSE: 0.0733\n",
      "Epoch 786, Training RMSE: 0.0719, Validation RMSE: 0.0733\n",
      "Epoch 787, Training RMSE: 0.0719, Validation RMSE: 0.0734\n",
      "Epoch 788, Training RMSE: 0.0720, Validation RMSE: 0.0733\n",
      "Epoch 789, Training RMSE: 0.0720, Validation RMSE: 0.0734\n",
      "Epoch 790, Training RMSE: 0.0720, Validation RMSE: 0.0734\n",
      "Epoch 791, Training RMSE: 0.0719, Validation RMSE: 0.0734\n",
      "Epoch 792, Training RMSE: 0.0718, Validation RMSE: 0.0732\n",
      "Epoch 793, Training RMSE: 0.0717, Validation RMSE: 0.0731\n",
      "Epoch 794, Training RMSE: 0.0717, Validation RMSE: 0.0731\n",
      "Epoch 795, Training RMSE: 0.0717, Validation RMSE: 0.0731\n",
      "Epoch 796, Training RMSE: 0.0717, Validation RMSE: 0.0731\n",
      "Epoch 797, Training RMSE: 0.0717, Validation RMSE: 0.0730\n",
      "Epoch 798, Training RMSE: 0.0716, Validation RMSE: 0.0730\n",
      "Epoch 799, Training RMSE: 0.0716, Validation RMSE: 0.0729\n",
      "Epoch 800, Training RMSE: 0.0715, Validation RMSE: 0.0729\n",
      "Epoch 801, Training RMSE: 0.0715, Validation RMSE: 0.0729\n",
      "Epoch 802, Training RMSE: 0.0715, Validation RMSE: 0.0729\n",
      "Epoch 803, Training RMSE: 0.0715, Validation RMSE: 0.0729\n",
      "Epoch 804, Training RMSE: 0.0714, Validation RMSE: 0.0728\n",
      "Epoch 805, Training RMSE: 0.0714, Validation RMSE: 0.0728\n",
      "Epoch 806, Training RMSE: 0.0714, Validation RMSE: 0.0728\n",
      "Epoch 807, Training RMSE: 0.0714, Validation RMSE: 0.0727\n",
      "Epoch 808, Training RMSE: 0.0713, Validation RMSE: 0.0727\n",
      "Epoch 809, Training RMSE: 0.0713, Validation RMSE: 0.0727\n",
      "Epoch 810, Training RMSE: 0.0713, Validation RMSE: 0.0727\n",
      "Epoch 811, Training RMSE: 0.0713, Validation RMSE: 0.0727\n",
      "Epoch 812, Training RMSE: 0.0712, Validation RMSE: 0.0726\n",
      "Epoch 813, Training RMSE: 0.0712, Validation RMSE: 0.0726\n",
      "Epoch 814, Training RMSE: 0.0712, Validation RMSE: 0.0726\n",
      "Epoch 815, Training RMSE: 0.0712, Validation RMSE: 0.0726\n",
      "Epoch 816, Training RMSE: 0.0711, Validation RMSE: 0.0725\n",
      "Epoch 817, Training RMSE: 0.0711, Validation RMSE: 0.0725\n",
      "Epoch 818, Training RMSE: 0.0711, Validation RMSE: 0.0725\n",
      "Epoch 819, Training RMSE: 0.0711, Validation RMSE: 0.0725\n",
      "Epoch 820, Training RMSE: 0.0710, Validation RMSE: 0.0724\n",
      "Epoch 821, Training RMSE: 0.0710, Validation RMSE: 0.0724\n",
      "Epoch 822, Training RMSE: 0.0710, Validation RMSE: 0.0724\n",
      "Epoch 823, Training RMSE: 0.0710, Validation RMSE: 0.0724\n",
      "Epoch 824, Training RMSE: 0.0709, Validation RMSE: 0.0724\n",
      "Epoch 825, Training RMSE: 0.0709, Validation RMSE: 0.0723\n",
      "Epoch 826, Training RMSE: 0.0709, Validation RMSE: 0.0723\n",
      "Epoch 827, Training RMSE: 0.0709, Validation RMSE: 0.0723\n",
      "Epoch 828, Training RMSE: 0.0709, Validation RMSE: 0.0723\n",
      "Epoch 829, Training RMSE: 0.0708, Validation RMSE: 0.0722\n",
      "Epoch 830, Training RMSE: 0.0708, Validation RMSE: 0.0722\n",
      "Epoch 831, Training RMSE: 0.0708, Validation RMSE: 0.0722\n",
      "Epoch 832, Training RMSE: 0.0708, Validation RMSE: 0.0722\n",
      "Epoch 833, Training RMSE: 0.0707, Validation RMSE: 0.0722\n",
      "Epoch 834, Training RMSE: 0.0707, Validation RMSE: 0.0721\n",
      "Epoch 835, Training RMSE: 0.0707, Validation RMSE: 0.0721\n",
      "Epoch 836, Training RMSE: 0.0707, Validation RMSE: 0.0721\n",
      "Epoch 837, Training RMSE: 0.0706, Validation RMSE: 0.0721\n",
      "Epoch 838, Training RMSE: 0.0706, Validation RMSE: 0.0720\n",
      "Epoch 839, Training RMSE: 0.0706, Validation RMSE: 0.0720\n",
      "Epoch 840, Training RMSE: 0.0706, Validation RMSE: 0.0720\n",
      "Epoch 841, Training RMSE: 0.0706, Validation RMSE: 0.0720\n",
      "Epoch 842, Training RMSE: 0.0705, Validation RMSE: 0.0720\n",
      "Epoch 843, Training RMSE: 0.0705, Validation RMSE: 0.0719\n",
      "Epoch 844, Training RMSE: 0.0705, Validation RMSE: 0.0719\n",
      "Epoch 845, Training RMSE: 0.0705, Validation RMSE: 0.0719\n",
      "Epoch 846, Training RMSE: 0.0704, Validation RMSE: 0.0719\n",
      "Epoch 847, Training RMSE: 0.0704, Validation RMSE: 0.0718\n",
      "Epoch 848, Training RMSE: 0.0704, Validation RMSE: 0.0718\n",
      "Epoch 849, Training RMSE: 0.0704, Validation RMSE: 0.0718\n",
      "Epoch 850, Training RMSE: 0.0704, Validation RMSE: 0.0718\n",
      "Epoch 851, Training RMSE: 0.0703, Validation RMSE: 0.0718\n",
      "Epoch 852, Training RMSE: 0.0703, Validation RMSE: 0.0717\n",
      "Epoch 853, Training RMSE: 0.0703, Validation RMSE: 0.0717\n",
      "Epoch 854, Training RMSE: 0.0703, Validation RMSE: 0.0717\n",
      "Epoch 855, Training RMSE: 0.0702, Validation RMSE: 0.0717\n",
      "Epoch 856, Training RMSE: 0.0702, Validation RMSE: 0.0717\n",
      "Epoch 857, Training RMSE: 0.0702, Validation RMSE: 0.0716\n",
      "Epoch 858, Training RMSE: 0.0702, Validation RMSE: 0.0716\n",
      "Epoch 859, Training RMSE: 0.0702, Validation RMSE: 0.0716\n",
      "Epoch 860, Training RMSE: 0.0702, Validation RMSE: 0.0716\n",
      "Epoch 861, Training RMSE: 0.0701, Validation RMSE: 0.0716\n",
      "Epoch 862, Training RMSE: 0.0701, Validation RMSE: 0.0716\n",
      "Epoch 863, Training RMSE: 0.0702, Validation RMSE: 0.0716\n",
      "Epoch 864, Training RMSE: 0.0702, Validation RMSE: 0.0717\n",
      "Epoch 865, Training RMSE: 0.0702, Validation RMSE: 0.0716\n",
      "Epoch 866, Training RMSE: 0.0703, Validation RMSE: 0.0718\n",
      "Epoch 867, Training RMSE: 0.0704, Validation RMSE: 0.0718\n",
      "Epoch 868, Training RMSE: 0.0704, Validation RMSE: 0.0719\n",
      "Epoch 869, Training RMSE: 0.0704, Validation RMSE: 0.0718\n",
      "Epoch 870, Training RMSE: 0.0703, Validation RMSE: 0.0718\n",
      "Epoch 871, Training RMSE: 0.0701, Validation RMSE: 0.0715\n",
      "Epoch 872, Training RMSE: 0.0699, Validation RMSE: 0.0714\n",
      "Epoch 873, Training RMSE: 0.0699, Validation RMSE: 0.0713\n",
      "Epoch 874, Training RMSE: 0.0699, Validation RMSE: 0.0714\n",
      "Epoch 875, Training RMSE: 0.0700, Validation RMSE: 0.0715\n",
      "Epoch 876, Training RMSE: 0.0700, Validation RMSE: 0.0714\n",
      "Epoch 877, Training RMSE: 0.0699, Validation RMSE: 0.0714\n",
      "Epoch 878, Training RMSE: 0.0698, Validation RMSE: 0.0712\n",
      "Epoch 879, Training RMSE: 0.0698, Validation RMSE: 0.0712\n",
      "Epoch 880, Training RMSE: 0.0698, Validation RMSE: 0.0712\n",
      "Epoch 881, Training RMSE: 0.0698, Validation RMSE: 0.0712\n",
      "Epoch 882, Training RMSE: 0.0698, Validation RMSE: 0.0712\n",
      "Epoch 883, Training RMSE: 0.0697, Validation RMSE: 0.0711\n",
      "Epoch 884, Training RMSE: 0.0697, Validation RMSE: 0.0711\n",
      "Epoch 885, Training RMSE: 0.0696, Validation RMSE: 0.0711\n",
      "Epoch 886, Training RMSE: 0.0696, Validation RMSE: 0.0711\n",
      "Epoch 887, Training RMSE: 0.0696, Validation RMSE: 0.0711\n",
      "Epoch 888, Training RMSE: 0.0696, Validation RMSE: 0.0711\n",
      "Epoch 889, Training RMSE: 0.0696, Validation RMSE: 0.0710\n",
      "Epoch 890, Training RMSE: 0.0695, Validation RMSE: 0.0710\n",
      "Epoch 891, Training RMSE: 0.0695, Validation RMSE: 0.0710\n",
      "Epoch 892, Training RMSE: 0.0695, Validation RMSE: 0.0710\n",
      "Epoch 893, Training RMSE: 0.0695, Validation RMSE: 0.0709\n",
      "Epoch 894, Training RMSE: 0.0695, Validation RMSE: 0.0709\n",
      "Epoch 895, Training RMSE: 0.0694, Validation RMSE: 0.0709\n",
      "Epoch 896, Training RMSE: 0.0694, Validation RMSE: 0.0709\n",
      "Epoch 897, Training RMSE: 0.0694, Validation RMSE: 0.0708\n",
      "Epoch 898, Training RMSE: 0.0694, Validation RMSE: 0.0708\n",
      "Epoch 899, Training RMSE: 0.0694, Validation RMSE: 0.0708\n",
      "Epoch 900, Training RMSE: 0.0693, Validation RMSE: 0.0708\n",
      "Epoch 901, Training RMSE: 0.0693, Validation RMSE: 0.0708\n",
      "Epoch 902, Training RMSE: 0.0693, Validation RMSE: 0.0707\n",
      "Epoch 903, Training RMSE: 0.0693, Validation RMSE: 0.0707\n",
      "Epoch 904, Training RMSE: 0.0692, Validation RMSE: 0.0707\n",
      "Epoch 905, Training RMSE: 0.0692, Validation RMSE: 0.0707\n",
      "Epoch 906, Training RMSE: 0.0692, Validation RMSE: 0.0707\n",
      "Epoch 907, Training RMSE: 0.0692, Validation RMSE: 0.0707\n",
      "Epoch 908, Training RMSE: 0.0692, Validation RMSE: 0.0707\n",
      "Epoch 909, Training RMSE: 0.0691, Validation RMSE: 0.0706\n",
      "Epoch 910, Training RMSE: 0.0691, Validation RMSE: 0.0706\n",
      "Epoch 911, Training RMSE: 0.0691, Validation RMSE: 0.0706\n",
      "Epoch 912, Training RMSE: 0.0691, Validation RMSE: 0.0706\n",
      "Epoch 913, Training RMSE: 0.0691, Validation RMSE: 0.0706\n",
      "Epoch 914, Training RMSE: 0.0690, Validation RMSE: 0.0705\n",
      "Epoch 915, Training RMSE: 0.0690, Validation RMSE: 0.0705\n",
      "Epoch 916, Training RMSE: 0.0690, Validation RMSE: 0.0705\n",
      "Epoch 917, Training RMSE: 0.0690, Validation RMSE: 0.0705\n",
      "Epoch 918, Training RMSE: 0.0690, Validation RMSE: 0.0705\n",
      "Epoch 919, Training RMSE: 0.0689, Validation RMSE: 0.0704\n",
      "Epoch 920, Training RMSE: 0.0689, Validation RMSE: 0.0704\n",
      "Epoch 921, Training RMSE: 0.0689, Validation RMSE: 0.0704\n",
      "Epoch 922, Training RMSE: 0.0689, Validation RMSE: 0.0704\n",
      "Epoch 923, Training RMSE: 0.0689, Validation RMSE: 0.0704\n",
      "Epoch 924, Training RMSE: 0.0689, Validation RMSE: 0.0703\n",
      "Epoch 925, Training RMSE: 0.0688, Validation RMSE: 0.0703\n",
      "Epoch 926, Training RMSE: 0.0688, Validation RMSE: 0.0703\n",
      "Epoch 927, Training RMSE: 0.0688, Validation RMSE: 0.0703\n",
      "Epoch 928, Training RMSE: 0.0688, Validation RMSE: 0.0703\n",
      "Epoch 929, Training RMSE: 0.0688, Validation RMSE: 0.0703\n",
      "Epoch 930, Training RMSE: 0.0688, Validation RMSE: 0.0703\n",
      "Epoch 931, Training RMSE: 0.0688, Validation RMSE: 0.0703\n",
      "Epoch 932, Training RMSE: 0.0688, Validation RMSE: 0.0703\n",
      "Epoch 933, Training RMSE: 0.0688, Validation RMSE: 0.0703\n",
      "Epoch 934, Training RMSE: 0.0688, Validation RMSE: 0.0703\n",
      "Epoch 935, Training RMSE: 0.0689, Validation RMSE: 0.0704\n",
      "Epoch 936, Training RMSE: 0.0690, Validation RMSE: 0.0705\n",
      "Epoch 937, Training RMSE: 0.0691, Validation RMSE: 0.0705\n",
      "Epoch 938, Training RMSE: 0.0691, Validation RMSE: 0.0706\n",
      "Epoch 939, Training RMSE: 0.0690, Validation RMSE: 0.0705\n",
      "Epoch 940, Training RMSE: 0.0689, Validation RMSE: 0.0704\n",
      "Epoch 941, Training RMSE: 0.0687, Validation RMSE: 0.0702\n",
      "Epoch 942, Training RMSE: 0.0686, Validation RMSE: 0.0701\n",
      "Epoch 943, Training RMSE: 0.0685, Validation RMSE: 0.0700\n",
      "Epoch 944, Training RMSE: 0.0686, Validation RMSE: 0.0701\n",
      "Epoch 945, Training RMSE: 0.0686, Validation RMSE: 0.0701\n",
      "Epoch 946, Training RMSE: 0.0686, Validation RMSE: 0.0701\n",
      "Epoch 947, Training RMSE: 0.0686, Validation RMSE: 0.0701\n",
      "Epoch 948, Training RMSE: 0.0685, Validation RMSE: 0.0700\n",
      "Epoch 949, Training RMSE: 0.0685, Validation RMSE: 0.0700\n",
      "Epoch 950, Training RMSE: 0.0684, Validation RMSE: 0.0699\n",
      "Epoch 951, Training RMSE: 0.0684, Validation RMSE: 0.0699\n",
      "Epoch 952, Training RMSE: 0.0684, Validation RMSE: 0.0699\n",
      "Epoch 953, Training RMSE: 0.0684, Validation RMSE: 0.0699\n",
      "Epoch 954, Training RMSE: 0.0684, Validation RMSE: 0.0699\n",
      "Epoch 955, Training RMSE: 0.0683, Validation RMSE: 0.0698\n",
      "Epoch 956, Training RMSE: 0.0683, Validation RMSE: 0.0698\n",
      "Epoch 957, Training RMSE: 0.0683, Validation RMSE: 0.0698\n",
      "Epoch 958, Training RMSE: 0.0683, Validation RMSE: 0.0698\n",
      "Epoch 959, Training RMSE: 0.0682, Validation RMSE: 0.0697\n",
      "Epoch 960, Training RMSE: 0.0682, Validation RMSE: 0.0697\n",
      "Epoch 961, Training RMSE: 0.0682, Validation RMSE: 0.0697\n",
      "Epoch 962, Training RMSE: 0.0682, Validation RMSE: 0.0697\n",
      "Epoch 963, Training RMSE: 0.0682, Validation RMSE: 0.0697\n",
      "Epoch 964, Training RMSE: 0.0682, Validation RMSE: 0.0697\n",
      "Epoch 965, Training RMSE: 0.0681, Validation RMSE: 0.0696\n",
      "Epoch 966, Training RMSE: 0.0681, Validation RMSE: 0.0696\n",
      "Epoch 967, Training RMSE: 0.0681, Validation RMSE: 0.0696\n",
      "Epoch 968, Training RMSE: 0.0681, Validation RMSE: 0.0696\n",
      "Epoch 969, Training RMSE: 0.0681, Validation RMSE: 0.0696\n",
      "Epoch 970, Training RMSE: 0.0681, Validation RMSE: 0.0696\n",
      "Epoch 971, Training RMSE: 0.0680, Validation RMSE: 0.0695\n",
      "Epoch 972, Training RMSE: 0.0680, Validation RMSE: 0.0695\n",
      "Epoch 973, Training RMSE: 0.0680, Validation RMSE: 0.0695\n",
      "Epoch 974, Training RMSE: 0.0680, Validation RMSE: 0.0695\n",
      "Epoch 975, Training RMSE: 0.0680, Validation RMSE: 0.0695\n",
      "Epoch 976, Training RMSE: 0.0679, Validation RMSE: 0.0695\n",
      "Epoch 977, Training RMSE: 0.0679, Validation RMSE: 0.0695\n",
      "Epoch 978, Training RMSE: 0.0679, Validation RMSE: 0.0694\n",
      "Epoch 979, Training RMSE: 0.0679, Validation RMSE: 0.0694\n",
      "Epoch 980, Training RMSE: 0.0679, Validation RMSE: 0.0694\n",
      "Epoch 981, Training RMSE: 0.0679, Validation RMSE: 0.0694\n",
      "Epoch 982, Training RMSE: 0.0678, Validation RMSE: 0.0694\n",
      "Epoch 983, Training RMSE: 0.0678, Validation RMSE: 0.0693\n",
      "Epoch 984, Training RMSE: 0.0678, Validation RMSE: 0.0693\n",
      "Epoch 985, Training RMSE: 0.0678, Validation RMSE: 0.0693\n",
      "Epoch 986, Training RMSE: 0.0678, Validation RMSE: 0.0693\n",
      "Epoch 987, Training RMSE: 0.0678, Validation RMSE: 0.0693\n",
      "Epoch 988, Training RMSE: 0.0677, Validation RMSE: 0.0693\n",
      "Epoch 989, Training RMSE: 0.0677, Validation RMSE: 0.0692\n",
      "Epoch 990, Training RMSE: 0.0677, Validation RMSE: 0.0692\n",
      "Epoch 991, Training RMSE: 0.0677, Validation RMSE: 0.0692\n",
      "Epoch 992, Training RMSE: 0.0677, Validation RMSE: 0.0692\n",
      "Epoch 993, Training RMSE: 0.0677, Validation RMSE: 0.0692\n",
      "Epoch 994, Training RMSE: 0.0676, Validation RMSE: 0.0692\n",
      "Epoch 995, Training RMSE: 0.0676, Validation RMSE: 0.0691\n",
      "Epoch 996, Training RMSE: 0.0676, Validation RMSE: 0.0691\n",
      "Epoch 997, Training RMSE: 0.0676, Validation RMSE: 0.0691\n",
      "Epoch 998, Training RMSE: 0.0676, Validation RMSE: 0.0691\n",
      "Epoch 999, Training RMSE: 0.0676, Validation RMSE: 0.0691\n",
      "Epoch 1000, Training RMSE: 0.0675, Validation RMSE: 0.0691\n",
      "Epoch 1001, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1002, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1003, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1004, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1005, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1006, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1007, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1008, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1009, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1010, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1011, Training RMSE: 0.0675, Validation RMSE: 0.0690\n",
      "Epoch 1012, Training RMSE: 0.0676, Validation RMSE: 0.0691\n",
      "Epoch 1013, Training RMSE: 0.0676, Validation RMSE: 0.0691\n",
      "Epoch 1014, Training RMSE: 0.0677, Validation RMSE: 0.0693\n",
      "Epoch 1015, Training RMSE: 0.0678, Validation RMSE: 0.0693\n",
      "Epoch 1016, Training RMSE: 0.0679, Validation RMSE: 0.0694\n",
      "Epoch 1017, Training RMSE: 0.0678, Validation RMSE: 0.0693\n",
      "Epoch 1018, Training RMSE: 0.0676, Validation RMSE: 0.0692\n",
      "Epoch 1019, Training RMSE: 0.0674, Validation RMSE: 0.0689\n",
      "Epoch 1020, Training RMSE: 0.0672, Validation RMSE: 0.0688\n",
      "Epoch 1021, Training RMSE: 0.0672, Validation RMSE: 0.0688\n",
      "Epoch 1022, Training RMSE: 0.0673, Validation RMSE: 0.0688\n",
      "Epoch 1023, Training RMSE: 0.0674, Validation RMSE: 0.0689\n",
      "Epoch 1024, Training RMSE: 0.0674, Validation RMSE: 0.0689\n",
      "Epoch 1025, Training RMSE: 0.0673, Validation RMSE: 0.0689\n",
      "Epoch 1026, Training RMSE: 0.0672, Validation RMSE: 0.0687\n",
      "Epoch 1027, Training RMSE: 0.0671, Validation RMSE: 0.0687\n",
      "Epoch 1028, Training RMSE: 0.0671, Validation RMSE: 0.0687\n",
      "Epoch 1029, Training RMSE: 0.0672, Validation RMSE: 0.0687\n",
      "Epoch 1030, Training RMSE: 0.0672, Validation RMSE: 0.0687\n",
      "Epoch 1031, Training RMSE: 0.0672, Validation RMSE: 0.0687\n",
      "Epoch 1032, Training RMSE: 0.0671, Validation RMSE: 0.0687\n",
      "Epoch 1033, Training RMSE: 0.0671, Validation RMSE: 0.0686\n",
      "Epoch 1034, Training RMSE: 0.0670, Validation RMSE: 0.0686\n",
      "Epoch 1035, Training RMSE: 0.0670, Validation RMSE: 0.0686\n",
      "Epoch 1036, Training RMSE: 0.0670, Validation RMSE: 0.0686\n",
      "Epoch 1037, Training RMSE: 0.0670, Validation RMSE: 0.0686\n",
      "Epoch 1038, Training RMSE: 0.0670, Validation RMSE: 0.0685\n",
      "Epoch 1039, Training RMSE: 0.0670, Validation RMSE: 0.0685\n",
      "Epoch 1040, Training RMSE: 0.0669, Validation RMSE: 0.0685\n",
      "Epoch 1041, Training RMSE: 0.0669, Validation RMSE: 0.0685\n",
      "Epoch 1042, Training RMSE: 0.0669, Validation RMSE: 0.0685\n",
      "Epoch 1043, Training RMSE: 0.0669, Validation RMSE: 0.0684\n",
      "Epoch 1044, Training RMSE: 0.0669, Validation RMSE: 0.0685\n",
      "Epoch 1045, Training RMSE: 0.0669, Validation RMSE: 0.0684\n",
      "Epoch 1046, Training RMSE: 0.0668, Validation RMSE: 0.0684\n",
      "Epoch 1047, Training RMSE: 0.0668, Validation RMSE: 0.0684\n",
      "Epoch 1048, Training RMSE: 0.0668, Validation RMSE: 0.0684\n",
      "Epoch 1049, Training RMSE: 0.0668, Validation RMSE: 0.0684\n",
      "Epoch 1050, Training RMSE: 0.0668, Validation RMSE: 0.0683\n",
      "Epoch 1051, Training RMSE: 0.0668, Validation RMSE: 0.0683\n",
      "Epoch 1052, Training RMSE: 0.0668, Validation RMSE: 0.0683\n",
      "Epoch 1053, Training RMSE: 0.0667, Validation RMSE: 0.0683\n",
      "Epoch 1054, Training RMSE: 0.0667, Validation RMSE: 0.0683\n",
      "Epoch 1055, Training RMSE: 0.0667, Validation RMSE: 0.0683\n",
      "Epoch 1056, Training RMSE: 0.0667, Validation RMSE: 0.0682\n",
      "Epoch 1057, Training RMSE: 0.0667, Validation RMSE: 0.0682\n",
      "Epoch 1058, Training RMSE: 0.0667, Validation RMSE: 0.0682\n",
      "Epoch 1059, Training RMSE: 0.0667, Validation RMSE: 0.0682\n",
      "Epoch 1060, Training RMSE: 0.0666, Validation RMSE: 0.0682\n",
      "Epoch 1061, Training RMSE: 0.0666, Validation RMSE: 0.0682\n",
      "Epoch 1062, Training RMSE: 0.0666, Validation RMSE: 0.0682\n",
      "Epoch 1063, Training RMSE: 0.0666, Validation RMSE: 0.0681\n",
      "Epoch 1064, Training RMSE: 0.0666, Validation RMSE: 0.0681\n",
      "Epoch 1065, Training RMSE: 0.0666, Validation RMSE: 0.0681\n",
      "Epoch 1066, Training RMSE: 0.0665, Validation RMSE: 0.0681\n",
      "Epoch 1067, Training RMSE: 0.0665, Validation RMSE: 0.0681\n",
      "Epoch 1068, Training RMSE: 0.0665, Validation RMSE: 0.0681\n",
      "Epoch 1069, Training RMSE: 0.0665, Validation RMSE: 0.0681\n",
      "Epoch 1070, Training RMSE: 0.0665, Validation RMSE: 0.0680\n",
      "Epoch 1071, Training RMSE: 0.0665, Validation RMSE: 0.0680\n",
      "Epoch 1072, Training RMSE: 0.0665, Validation RMSE: 0.0680\n",
      "Epoch 1073, Training RMSE: 0.0664, Validation RMSE: 0.0680\n",
      "Epoch 1074, Training RMSE: 0.0664, Validation RMSE: 0.0680\n",
      "Epoch 1075, Training RMSE: 0.0664, Validation RMSE: 0.0680\n",
      "Epoch 1076, Training RMSE: 0.0664, Validation RMSE: 0.0680\n",
      "Epoch 1077, Training RMSE: 0.0664, Validation RMSE: 0.0680\n",
      "Epoch 1078, Training RMSE: 0.0664, Validation RMSE: 0.0680\n",
      "Epoch 1079, Training RMSE: 0.0664, Validation RMSE: 0.0679\n",
      "Epoch 1080, Training RMSE: 0.0664, Validation RMSE: 0.0679\n",
      "Epoch 1081, Training RMSE: 0.0664, Validation RMSE: 0.0679\n",
      "Epoch 1082, Training RMSE: 0.0664, Validation RMSE: 0.0679\n",
      "Epoch 1083, Training RMSE: 0.0664, Validation RMSE: 0.0679\n",
      "Epoch 1084, Training RMSE: 0.0664, Validation RMSE: 0.0679\n",
      "Epoch 1085, Training RMSE: 0.0664, Validation RMSE: 0.0680\n",
      "Epoch 1086, Training RMSE: 0.0664, Validation RMSE: 0.0680\n",
      "Epoch 1087, Training RMSE: 0.0665, Validation RMSE: 0.0681\n",
      "Epoch 1088, Training RMSE: 0.0665, Validation RMSE: 0.0681\n",
      "Epoch 1089, Training RMSE: 0.0666, Validation RMSE: 0.0682\n",
      "Epoch 1090, Training RMSE: 0.0666, Validation RMSE: 0.0682\n",
      "Epoch 1091, Training RMSE: 0.0666, Validation RMSE: 0.0682\n",
      "Epoch 1092, Training RMSE: 0.0665, Validation RMSE: 0.0680\n",
      "Epoch 1093, Training RMSE: 0.0664, Validation RMSE: 0.0680\n",
      "Epoch 1094, Training RMSE: 0.0663, Validation RMSE: 0.0678\n",
      "Epoch 1095, Training RMSE: 0.0662, Validation RMSE: 0.0678\n",
      "Epoch 1096, Training RMSE: 0.0662, Validation RMSE: 0.0678\n",
      "Epoch 1097, Training RMSE: 0.0662, Validation RMSE: 0.0678\n",
      "Epoch 1098, Training RMSE: 0.0662, Validation RMSE: 0.0678\n",
      "Epoch 1099, Training RMSE: 0.0662, Validation RMSE: 0.0677\n",
      "Epoch 1100, Training RMSE: 0.0661, Validation RMSE: 0.0677\n",
      "Epoch 1101, Training RMSE: 0.0661, Validation RMSE: 0.0677\n",
      "Epoch 1102, Training RMSE: 0.0661, Validation RMSE: 0.0677\n",
      "Epoch 1103, Training RMSE: 0.0661, Validation RMSE: 0.0677\n",
      "Epoch 1104, Training RMSE: 0.0661, Validation RMSE: 0.0677\n",
      "Epoch 1105, Training RMSE: 0.0661, Validation RMSE: 0.0676\n",
      "Epoch 1106, Training RMSE: 0.0660, Validation RMSE: 0.0676\n",
      "Epoch 1107, Training RMSE: 0.0660, Validation RMSE: 0.0676\n",
      "Epoch 1108, Training RMSE: 0.0660, Validation RMSE: 0.0676\n",
      "Epoch 1109, Training RMSE: 0.0660, Validation RMSE: 0.0676\n",
      "Epoch 1110, Training RMSE: 0.0660, Validation RMSE: 0.0675\n",
      "Epoch 1111, Training RMSE: 0.0660, Validation RMSE: 0.0676\n",
      "Epoch 1112, Training RMSE: 0.0660, Validation RMSE: 0.0675\n",
      "Epoch 1113, Training RMSE: 0.0659, Validation RMSE: 0.0675\n",
      "Epoch 1114, Training RMSE: 0.0659, Validation RMSE: 0.0675\n",
      "Epoch 1115, Training RMSE: 0.0659, Validation RMSE: 0.0675\n",
      "Epoch 1116, Training RMSE: 0.0659, Validation RMSE: 0.0675\n",
      "Epoch 1117, Training RMSE: 0.0659, Validation RMSE: 0.0675\n",
      "Epoch 1118, Training RMSE: 0.0658, Validation RMSE: 0.0675\n",
      "Epoch 1119, Training RMSE: 0.0658, Validation RMSE: 0.0674\n",
      "Epoch 1120, Training RMSE: 0.0658, Validation RMSE: 0.0674\n",
      "Epoch 1121, Training RMSE: 0.0658, Validation RMSE: 0.0674\n",
      "Epoch 1122, Training RMSE: 0.0658, Validation RMSE: 0.0674\n",
      "Epoch 1123, Training RMSE: 0.0658, Validation RMSE: 0.0674\n",
      "Epoch 1124, Training RMSE: 0.0658, Validation RMSE: 0.0674\n",
      "Epoch 1125, Training RMSE: 0.0658, Validation RMSE: 0.0674\n",
      "Epoch 1126, Training RMSE: 0.0657, Validation RMSE: 0.0673\n",
      "Epoch 1127, Training RMSE: 0.0657, Validation RMSE: 0.0673\n",
      "Epoch 1128, Training RMSE: 0.0657, Validation RMSE: 0.0673\n",
      "Epoch 1129, Training RMSE: 0.0657, Validation RMSE: 0.0673\n",
      "Epoch 1130, Training RMSE: 0.0657, Validation RMSE: 0.0673\n",
      "Epoch 1131, Training RMSE: 0.0657, Validation RMSE: 0.0673\n",
      "Epoch 1132, Training RMSE: 0.0657, Validation RMSE: 0.0673\n",
      "Epoch 1133, Training RMSE: 0.0657, Validation RMSE: 0.0672\n",
      "Epoch 1134, Training RMSE: 0.0656, Validation RMSE: 0.0673\n",
      "Epoch 1135, Training RMSE: 0.0656, Validation RMSE: 0.0672\n",
      "Epoch 1136, Training RMSE: 0.0656, Validation RMSE: 0.0672\n",
      "Epoch 1137, Training RMSE: 0.0656, Validation RMSE: 0.0672\n",
      "Epoch 1138, Training RMSE: 0.0656, Validation RMSE: 0.0672\n",
      "Epoch 1139, Training RMSE: 0.0656, Validation RMSE: 0.0672\n",
      "Epoch 1140, Training RMSE: 0.0656, Validation RMSE: 0.0672\n",
      "Epoch 1141, Training RMSE: 0.0656, Validation RMSE: 0.0672\n",
      "Epoch 1142, Training RMSE: 0.0656, Validation RMSE: 0.0673\n",
      "Epoch 1143, Training RMSE: 0.0656, Validation RMSE: 0.0672\n",
      "Epoch 1144, Training RMSE: 0.0657, Validation RMSE: 0.0673\n",
      "Epoch 1145, Training RMSE: 0.0657, Validation RMSE: 0.0673\n",
      "Epoch 1146, Training RMSE: 0.0657, Validation RMSE: 0.0674\n",
      "Epoch 1147, Training RMSE: 0.0658, Validation RMSE: 0.0674\n",
      "Epoch 1148, Training RMSE: 0.0658, Validation RMSE: 0.0675\n",
      "Epoch 1149, Training RMSE: 0.0658, Validation RMSE: 0.0674\n",
      "Epoch 1150, Training RMSE: 0.0658, Validation RMSE: 0.0674\n",
      "Epoch 1151, Training RMSE: 0.0657, Validation RMSE: 0.0673\n",
      "Epoch 1152, Training RMSE: 0.0656, Validation RMSE: 0.0672\n",
      "Epoch 1153, Training RMSE: 0.0654, Validation RMSE: 0.0670\n",
      "Epoch 1154, Training RMSE: 0.0654, Validation RMSE: 0.0670\n",
      "Epoch 1155, Training RMSE: 0.0654, Validation RMSE: 0.0670\n",
      "Epoch 1156, Training RMSE: 0.0654, Validation RMSE: 0.0670\n",
      "Epoch 1157, Training RMSE: 0.0654, Validation RMSE: 0.0671\n",
      "Epoch 1158, Training RMSE: 0.0654, Validation RMSE: 0.0670\n",
      "Epoch 1159, Training RMSE: 0.0654, Validation RMSE: 0.0671\n",
      "Epoch 1160, Training RMSE: 0.0654, Validation RMSE: 0.0670\n",
      "Epoch 1161, Training RMSE: 0.0654, Validation RMSE: 0.0670\n",
      "Epoch 1162, Training RMSE: 0.0653, Validation RMSE: 0.0669\n",
      "Epoch 1163, Training RMSE: 0.0653, Validation RMSE: 0.0669\n",
      "Epoch 1164, Training RMSE: 0.0653, Validation RMSE: 0.0669\n",
      "Epoch 1165, Training RMSE: 0.0652, Validation RMSE: 0.0669\n",
      "Epoch 1166, Training RMSE: 0.0653, Validation RMSE: 0.0669\n",
      "Epoch 1167, Training RMSE: 0.0653, Validation RMSE: 0.0669\n",
      "Epoch 1168, Training RMSE: 0.0653, Validation RMSE: 0.0669\n",
      "Epoch 1169, Training RMSE: 0.0652, Validation RMSE: 0.0669\n",
      "Epoch 1170, Training RMSE: 0.0652, Validation RMSE: 0.0669\n",
      "Epoch 1171, Training RMSE: 0.0652, Validation RMSE: 0.0668\n",
      "Epoch 1172, Training RMSE: 0.0652, Validation RMSE: 0.0668\n",
      "Epoch 1173, Training RMSE: 0.0651, Validation RMSE: 0.0668\n",
      "Epoch 1174, Training RMSE: 0.0651, Validation RMSE: 0.0668\n",
      "Epoch 1175, Training RMSE: 0.0651, Validation RMSE: 0.0668\n",
      "Epoch 1176, Training RMSE: 0.0651, Validation RMSE: 0.0668\n",
      "Epoch 1177, Training RMSE: 0.0651, Validation RMSE: 0.0668\n",
      "Epoch 1178, Training RMSE: 0.0651, Validation RMSE: 0.0667\n",
      "Epoch 1179, Training RMSE: 0.0651, Validation RMSE: 0.0668\n",
      "Epoch 1180, Training RMSE: 0.0651, Validation RMSE: 0.0667\n",
      "Epoch 1181, Training RMSE: 0.0651, Validation RMSE: 0.0667\n",
      "Epoch 1182, Training RMSE: 0.0651, Validation RMSE: 0.0667\n",
      "Epoch 1183, Training RMSE: 0.0650, Validation RMSE: 0.0667\n",
      "Epoch 1184, Training RMSE: 0.0650, Validation RMSE: 0.0667\n",
      "Epoch 1185, Training RMSE: 0.0650, Validation RMSE: 0.0667\n",
      "Epoch 1186, Training RMSE: 0.0650, Validation RMSE: 0.0666\n",
      "Epoch 1187, Training RMSE: 0.0650, Validation RMSE: 0.0666\n",
      "Epoch 1188, Training RMSE: 0.0650, Validation RMSE: 0.0666\n",
      "Epoch 1189, Training RMSE: 0.0650, Validation RMSE: 0.0666\n",
      "Epoch 1190, Training RMSE: 0.0649, Validation RMSE: 0.0666\n",
      "Epoch 1191, Training RMSE: 0.0649, Validation RMSE: 0.0666\n",
      "Epoch 1192, Training RMSE: 0.0649, Validation RMSE: 0.0666\n",
      "Epoch 1193, Training RMSE: 0.0649, Validation RMSE: 0.0666\n",
      "Epoch 1194, Training RMSE: 0.0649, Validation RMSE: 0.0666\n",
      "Epoch 1195, Training RMSE: 0.0649, Validation RMSE: 0.0665\n",
      "Epoch 1196, Training RMSE: 0.0649, Validation RMSE: 0.0665\n",
      "Epoch 1197, Training RMSE: 0.0649, Validation RMSE: 0.0665\n",
      "Epoch 1198, Training RMSE: 0.0649, Validation RMSE: 0.0665\n",
      "Epoch 1199, Training RMSE: 0.0648, Validation RMSE: 0.0665\n",
      "Epoch 1200, Training RMSE: 0.0648, Validation RMSE: 0.0665\n",
      "Epoch 1201, Training RMSE: 0.0648, Validation RMSE: 0.0665\n",
      "Epoch 1202, Training RMSE: 0.0648, Validation RMSE: 0.0665\n",
      "Epoch 1203, Training RMSE: 0.0648, Validation RMSE: 0.0665\n",
      "Epoch 1204, Training RMSE: 0.0648, Validation RMSE: 0.0665\n",
      "Epoch 1205, Training RMSE: 0.0648, Validation RMSE: 0.0665\n",
      "Epoch 1206, Training RMSE: 0.0648, Validation RMSE: 0.0665\n",
      "Epoch 1207, Training RMSE: 0.0648, Validation RMSE: 0.0665\n",
      "Epoch 1208, Training RMSE: 0.0649, Validation RMSE: 0.0665\n",
      "Epoch 1209, Training RMSE: 0.0649, Validation RMSE: 0.0665\n",
      "Epoch 1210, Training RMSE: 0.0649, Validation RMSE: 0.0666\n",
      "Epoch 1211, Training RMSE: 0.0650, Validation RMSE: 0.0666\n",
      "Epoch 1212, Training RMSE: 0.0650, Validation RMSE: 0.0667\n",
      "Epoch 1213, Training RMSE: 0.0651, Validation RMSE: 0.0667\n",
      "Epoch 1214, Training RMSE: 0.0651, Validation RMSE: 0.0668\n",
      "Epoch 1215, Training RMSE: 0.0650, Validation RMSE: 0.0667\n",
      "Epoch 1216, Training RMSE: 0.0650, Validation RMSE: 0.0667\n",
      "Epoch 1217, Training RMSE: 0.0649, Validation RMSE: 0.0665\n",
      "Epoch 1218, Training RMSE: 0.0648, Validation RMSE: 0.0665\n",
      "Epoch 1219, Training RMSE: 0.0647, Validation RMSE: 0.0664\n",
      "Epoch 1220, Training RMSE: 0.0647, Validation RMSE: 0.0664\n",
      "Epoch 1221, Training RMSE: 0.0647, Validation RMSE: 0.0664\n",
      "Epoch 1222, Training RMSE: 0.0647, Validation RMSE: 0.0664\n",
      "Epoch 1223, Training RMSE: 0.0647, Validation RMSE: 0.0664\n",
      "Epoch 1224, Training RMSE: 0.0647, Validation RMSE: 0.0664\n",
      "Epoch 1225, Training RMSE: 0.0647, Validation RMSE: 0.0664\n",
      "Epoch 1226, Training RMSE: 0.0646, Validation RMSE: 0.0663\n",
      "Epoch 1227, Training RMSE: 0.0645, Validation RMSE: 0.0663\n",
      "Epoch 1228, Training RMSE: 0.0645, Validation RMSE: 0.0662\n",
      "Epoch 1229, Training RMSE: 0.0645, Validation RMSE: 0.0663\n",
      "Epoch 1230, Training RMSE: 0.0646, Validation RMSE: 0.0663\n",
      "Epoch 1231, Training RMSE: 0.0646, Validation RMSE: 0.0663\n",
      "Epoch 1232, Training RMSE: 0.0646, Validation RMSE: 0.0663\n",
      "Epoch 1233, Training RMSE: 0.0645, Validation RMSE: 0.0662\n",
      "Epoch 1234, Training RMSE: 0.0645, Validation RMSE: 0.0662\n",
      "Epoch 1235, Training RMSE: 0.0644, Validation RMSE: 0.0662\n",
      "Epoch 1236, Training RMSE: 0.0644, Validation RMSE: 0.0661\n",
      "Epoch 1237, Training RMSE: 0.0644, Validation RMSE: 0.0661\n",
      "Epoch 1238, Training RMSE: 0.0644, Validation RMSE: 0.0662\n",
      "Epoch 1239, Training RMSE: 0.0644, Validation RMSE: 0.0661\n",
      "Epoch 1240, Training RMSE: 0.0644, Validation RMSE: 0.0661\n",
      "Epoch 1241, Training RMSE: 0.0644, Validation RMSE: 0.0661\n",
      "Epoch 1242, Training RMSE: 0.0644, Validation RMSE: 0.0661\n",
      "Epoch 1243, Training RMSE: 0.0644, Validation RMSE: 0.0661\n",
      "Epoch 1244, Training RMSE: 0.0643, Validation RMSE: 0.0661\n",
      "Epoch 1245, Training RMSE: 0.0643, Validation RMSE: 0.0661\n",
      "Epoch 1246, Training RMSE: 0.0643, Validation RMSE: 0.0661\n",
      "Epoch 1247, Training RMSE: 0.0643, Validation RMSE: 0.0661\n",
      "Epoch 1248, Training RMSE: 0.0643, Validation RMSE: 0.0661\n",
      "Epoch 1249, Training RMSE: 0.0643, Validation RMSE: 0.0660\n",
      "Epoch 1250, Training RMSE: 0.0643, Validation RMSE: 0.0660\n",
      "Epoch 1251, Training RMSE: 0.0643, Validation RMSE: 0.0660\n",
      "Epoch 1252, Training RMSE: 0.0643, Validation RMSE: 0.0660\n",
      "Epoch 1253, Training RMSE: 0.0643, Validation RMSE: 0.0660\n",
      "Epoch 1254, Training RMSE: 0.0642, Validation RMSE: 0.0660\n",
      "Epoch 1255, Training RMSE: 0.0642, Validation RMSE: 0.0660\n",
      "Epoch 1256, Training RMSE: 0.0642, Validation RMSE: 0.0660\n",
      "Epoch 1257, Training RMSE: 0.0642, Validation RMSE: 0.0660\n",
      "Epoch 1258, Training RMSE: 0.0642, Validation RMSE: 0.0660\n",
      "Epoch 1259, Training RMSE: 0.0642, Validation RMSE: 0.0659\n",
      "Epoch 1260, Training RMSE: 0.0642, Validation RMSE: 0.0659\n",
      "Epoch 1261, Training RMSE: 0.0642, Validation RMSE: 0.0659\n",
      "Epoch 1262, Training RMSE: 0.0642, Validation RMSE: 0.0659\n",
      "Epoch 1263, Training RMSE: 0.0642, Validation RMSE: 0.0659\n",
      "Epoch 1264, Training RMSE: 0.0641, Validation RMSE: 0.0659\n",
      "Epoch 1265, Training RMSE: 0.0641, Validation RMSE: 0.0659\n",
      "Epoch 1266, Training RMSE: 0.0641, Validation RMSE: 0.0659\n",
      "Epoch 1267, Training RMSE: 0.0641, Validation RMSE: 0.0659\n",
      "Epoch 1268, Training RMSE: 0.0641, Validation RMSE: 0.0659\n",
      "Epoch 1269, Training RMSE: 0.0641, Validation RMSE: 0.0659\n",
      "Epoch 1270, Training RMSE: 0.0641, Validation RMSE: 0.0659\n",
      "Epoch 1271, Training RMSE: 0.0641, Validation RMSE: 0.0659\n",
      "Epoch 1272, Training RMSE: 0.0642, Validation RMSE: 0.0659\n",
      "Epoch 1273, Training RMSE: 0.0642, Validation RMSE: 0.0659\n",
      "Epoch 1274, Training RMSE: 0.0643, Validation RMSE: 0.0661\n",
      "Epoch 1275, Training RMSE: 0.0644, Validation RMSE: 0.0661\n",
      "Epoch 1276, Training RMSE: 0.0646, Validation RMSE: 0.0664\n",
      "Epoch 1277, Training RMSE: 0.0647, Validation RMSE: 0.0664\n",
      "Epoch 1278, Training RMSE: 0.0649, Validation RMSE: 0.0668\n",
      "Epoch 1279, Training RMSE: 0.0649, Validation RMSE: 0.0666\n",
      "Epoch 1280, Training RMSE: 0.0647, Validation RMSE: 0.0665\n",
      "Epoch 1281, Training RMSE: 0.0643, Validation RMSE: 0.0661\n",
      "Epoch 1282, Training RMSE: 0.0641, Validation RMSE: 0.0659\n",
      "Epoch 1283, Training RMSE: 0.0640, Validation RMSE: 0.0658\n",
      "Epoch 1284, Training RMSE: 0.0641, Validation RMSE: 0.0659\n",
      "Epoch 1285, Training RMSE: 0.0643, Validation RMSE: 0.0661\n",
      "Epoch 1286, Training RMSE: 0.0643, Validation RMSE: 0.0660\n",
      "Epoch 1287, Training RMSE: 0.0641, Validation RMSE: 0.0660\n",
      "Epoch 1288, Training RMSE: 0.0640, Validation RMSE: 0.0658\n",
      "Epoch 1289, Training RMSE: 0.0640, Validation RMSE: 0.0658\n",
      "Epoch 1290, Training RMSE: 0.0640, Validation RMSE: 0.0658\n",
      "Epoch 1291, Training RMSE: 0.0640, Validation RMSE: 0.0658\n",
      "Epoch 1292, Training RMSE: 0.0640, Validation RMSE: 0.0658\n",
      "Epoch 1293, Training RMSE: 0.0639, Validation RMSE: 0.0657\n",
      "Epoch 1294, Training RMSE: 0.0639, Validation RMSE: 0.0657\n",
      "Epoch 1295, Training RMSE: 0.0640, Validation RMSE: 0.0657\n",
      "Epoch 1296, Training RMSE: 0.0639, Validation RMSE: 0.0657\n",
      "Epoch 1297, Training RMSE: 0.0639, Validation RMSE: 0.0657\n",
      "Epoch 1298, Training RMSE: 0.0639, Validation RMSE: 0.0656\n",
      "Epoch 1299, Training RMSE: 0.0638, Validation RMSE: 0.0656\n",
      "Epoch 1300, Training RMSE: 0.0639, Validation RMSE: 0.0656\n",
      "Epoch 1301, Training RMSE: 0.0639, Validation RMSE: 0.0656\n",
      "Epoch 1302, Training RMSE: 0.0638, Validation RMSE: 0.0656\n",
      "Epoch 1303, Training RMSE: 0.0638, Validation RMSE: 0.0656\n",
      "Epoch 1304, Training RMSE: 0.0638, Validation RMSE: 0.0656\n",
      "Epoch 1305, Training RMSE: 0.0638, Validation RMSE: 0.0656\n",
      "Epoch 1306, Training RMSE: 0.0638, Validation RMSE: 0.0656\n",
      "Epoch 1307, Training RMSE: 0.0638, Validation RMSE: 0.0656\n",
      "Epoch 1308, Training RMSE: 0.0638, Validation RMSE: 0.0655\n",
      "Epoch 1309, Training RMSE: 0.0637, Validation RMSE: 0.0655\n",
      "Epoch 1310, Training RMSE: 0.0637, Validation RMSE: 0.0655\n",
      "Epoch 1311, Training RMSE: 0.0637, Validation RMSE: 0.0655\n",
      "Epoch 1312, Training RMSE: 0.0637, Validation RMSE: 0.0655\n",
      "Epoch 1313, Training RMSE: 0.0637, Validation RMSE: 0.0655\n",
      "Epoch 1314, Training RMSE: 0.0637, Validation RMSE: 0.0655\n",
      "Epoch 1315, Training RMSE: 0.0637, Validation RMSE: 0.0655\n",
      "Epoch 1316, Training RMSE: 0.0637, Validation RMSE: 0.0655\n",
      "Epoch 1317, Training RMSE: 0.0637, Validation RMSE: 0.0655\n",
      "Epoch 1318, Training RMSE: 0.0637, Validation RMSE: 0.0654\n",
      "Epoch 1319, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1320, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1321, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1322, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1323, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1324, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1325, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1326, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1327, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1328, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1329, Training RMSE: 0.0636, Validation RMSE: 0.0654\n",
      "Epoch 1330, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1331, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1332, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1333, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1334, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1335, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1336, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1337, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1338, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1339, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1340, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1341, Training RMSE: 0.0634, Validation RMSE: 0.0653\n",
      "Epoch 1342, Training RMSE: 0.0634, Validation RMSE: 0.0653\n",
      "Epoch 1343, Training RMSE: 0.0634, Validation RMSE: 0.0653\n",
      "Epoch 1344, Training RMSE: 0.0634, Validation RMSE: 0.0652\n",
      "Epoch 1345, Training RMSE: 0.0634, Validation RMSE: 0.0652\n",
      "Epoch 1346, Training RMSE: 0.0634, Validation RMSE: 0.0652\n",
      "Epoch 1347, Training RMSE: 0.0634, Validation RMSE: 0.0652\n",
      "Epoch 1348, Training RMSE: 0.0634, Validation RMSE: 0.0652\n",
      "Epoch 1349, Training RMSE: 0.0634, Validation RMSE: 0.0652\n",
      "Epoch 1350, Training RMSE: 0.0634, Validation RMSE: 0.0652\n",
      "Epoch 1351, Training RMSE: 0.0634, Validation RMSE: 0.0652\n",
      "Epoch 1352, Training RMSE: 0.0633, Validation RMSE: 0.0652\n",
      "Epoch 1353, Training RMSE: 0.0633, Validation RMSE: 0.0652\n",
      "Epoch 1354, Training RMSE: 0.0633, Validation RMSE: 0.0652\n",
      "Epoch 1355, Training RMSE: 0.0633, Validation RMSE: 0.0652\n",
      "Epoch 1356, Training RMSE: 0.0633, Validation RMSE: 0.0652\n",
      "Epoch 1357, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1358, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1359, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1360, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1361, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1362, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1363, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1364, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1365, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1366, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1367, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1368, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1369, Training RMSE: 0.0633, Validation RMSE: 0.0652\n",
      "Epoch 1370, Training RMSE: 0.0634, Validation RMSE: 0.0652\n",
      "Epoch 1371, Training RMSE: 0.0634, Validation RMSE: 0.0653\n",
      "Epoch 1372, Training RMSE: 0.0635, Validation RMSE: 0.0654\n",
      "Epoch 1373, Training RMSE: 0.0637, Validation RMSE: 0.0656\n",
      "Epoch 1374, Training RMSE: 0.0638, Validation RMSE: 0.0656\n",
      "Epoch 1375, Training RMSE: 0.0639, Validation RMSE: 0.0658\n",
      "Epoch 1376, Training RMSE: 0.0639, Validation RMSE: 0.0657\n",
      "Epoch 1377, Training RMSE: 0.0637, Validation RMSE: 0.0656\n",
      "Epoch 1378, Training RMSE: 0.0635, Validation RMSE: 0.0653\n",
      "Epoch 1379, Training RMSE: 0.0632, Validation RMSE: 0.0651\n",
      "Epoch 1380, Training RMSE: 0.0631, Validation RMSE: 0.0650\n",
      "Epoch 1381, Training RMSE: 0.0632, Validation RMSE: 0.0650\n",
      "Epoch 1382, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1383, Training RMSE: 0.0633, Validation RMSE: 0.0652\n",
      "Epoch 1384, Training RMSE: 0.0633, Validation RMSE: 0.0652\n",
      "Epoch 1385, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1386, Training RMSE: 0.0632, Validation RMSE: 0.0651\n",
      "Epoch 1387, Training RMSE: 0.0631, Validation RMSE: 0.0649\n",
      "Epoch 1388, Training RMSE: 0.0631, Validation RMSE: 0.0650\n",
      "Epoch 1389, Training RMSE: 0.0631, Validation RMSE: 0.0650\n",
      "Epoch 1390, Training RMSE: 0.0631, Validation RMSE: 0.0650\n",
      "Epoch 1391, Training RMSE: 0.0631, Validation RMSE: 0.0650\n",
      "Epoch 1392, Training RMSE: 0.0631, Validation RMSE: 0.0649\n",
      "Epoch 1393, Training RMSE: 0.0631, Validation RMSE: 0.0650\n",
      "Epoch 1394, Training RMSE: 0.0631, Validation RMSE: 0.0649\n",
      "Epoch 1395, Training RMSE: 0.0630, Validation RMSE: 0.0649\n",
      "Epoch 1396, Training RMSE: 0.0630, Validation RMSE: 0.0649\n",
      "Epoch 1397, Training RMSE: 0.0630, Validation RMSE: 0.0649\n",
      "Epoch 1398, Training RMSE: 0.0630, Validation RMSE: 0.0649\n",
      "Epoch 1399, Training RMSE: 0.0630, Validation RMSE: 0.0649\n",
      "Epoch 1400, Training RMSE: 0.0630, Validation RMSE: 0.0649\n",
      "Epoch 1401, Training RMSE: 0.0630, Validation RMSE: 0.0648\n",
      "Epoch 1402, Training RMSE: 0.0630, Validation RMSE: 0.0649\n",
      "Epoch 1403, Training RMSE: 0.0630, Validation RMSE: 0.0648\n",
      "Epoch 1404, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1405, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1406, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1407, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1408, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1409, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1410, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1411, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1412, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1413, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1414, Training RMSE: 0.0629, Validation RMSE: 0.0648\n",
      "Epoch 1415, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1416, Training RMSE: 0.0628, Validation RMSE: 0.0648\n",
      "Epoch 1417, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1418, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1419, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1420, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1421, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1422, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1423, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1424, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1425, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1426, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1427, Training RMSE: 0.0628, Validation RMSE: 0.0647\n",
      "Epoch 1428, Training RMSE: 0.0627, Validation RMSE: 0.0647\n",
      "Epoch 1429, Training RMSE: 0.0627, Validation RMSE: 0.0647\n",
      "Epoch 1430, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1431, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1432, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1433, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1434, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1435, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1436, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1437, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1438, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1439, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1440, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1441, Training RMSE: 0.0626, Validation RMSE: 0.0646\n",
      "Epoch 1442, Training RMSE: 0.0626, Validation RMSE: 0.0646\n",
      "Epoch 1443, Training RMSE: 0.0626, Validation RMSE: 0.0646\n",
      "Epoch 1444, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1445, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1446, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1447, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1448, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1449, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1450, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1451, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1452, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1453, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1454, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1455, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1456, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1457, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1458, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1459, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1460, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1461, Training RMSE: 0.0625, Validation RMSE: 0.0644\n",
      "Epoch 1462, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1463, Training RMSE: 0.0625, Validation RMSE: 0.0644\n",
      "Epoch 1464, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1465, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1466, Training RMSE: 0.0626, Validation RMSE: 0.0646\n",
      "Epoch 1467, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1468, Training RMSE: 0.0629, Validation RMSE: 0.0649\n",
      "Epoch 1469, Training RMSE: 0.0630, Validation RMSE: 0.0649\n",
      "Epoch 1470, Training RMSE: 0.0633, Validation RMSE: 0.0653\n",
      "Epoch 1471, Training RMSE: 0.0634, Validation RMSE: 0.0653\n",
      "Epoch 1472, Training RMSE: 0.0635, Validation RMSE: 0.0655\n",
      "Epoch 1473, Training RMSE: 0.0633, Validation RMSE: 0.0651\n",
      "Epoch 1474, Training RMSE: 0.0629, Validation RMSE: 0.0649\n",
      "Epoch 1475, Training RMSE: 0.0626, Validation RMSE: 0.0645\n",
      "Epoch 1476, Training RMSE: 0.0624, Validation RMSE: 0.0644\n",
      "Epoch 1477, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1478, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1479, Training RMSE: 0.0628, Validation RMSE: 0.0648\n",
      "Epoch 1480, Training RMSE: 0.0627, Validation RMSE: 0.0646\n",
      "Epoch 1481, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1482, Training RMSE: 0.0624, Validation RMSE: 0.0644\n",
      "Epoch 1483, Training RMSE: 0.0624, Validation RMSE: 0.0643\n",
      "Epoch 1484, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1485, Training RMSE: 0.0625, Validation RMSE: 0.0644\n",
      "Epoch 1486, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1487, Training RMSE: 0.0624, Validation RMSE: 0.0643\n",
      "Epoch 1488, Training RMSE: 0.0623, Validation RMSE: 0.0643\n",
      "Epoch 1489, Training RMSE: 0.0624, Validation RMSE: 0.0643\n",
      "Epoch 1490, Training RMSE: 0.0624, Validation RMSE: 0.0643\n",
      "Epoch 1491, Training RMSE: 0.0624, Validation RMSE: 0.0644\n",
      "Epoch 1492, Training RMSE: 0.0624, Validation RMSE: 0.0643\n",
      "Epoch 1493, Training RMSE: 0.0623, Validation RMSE: 0.0643\n",
      "Epoch 1494, Training RMSE: 0.0623, Validation RMSE: 0.0642\n",
      "Epoch 1495, Training RMSE: 0.0623, Validation RMSE: 0.0642\n",
      "Epoch 1496, Training RMSE: 0.0623, Validation RMSE: 0.0643\n",
      "Epoch 1497, Training RMSE: 0.0623, Validation RMSE: 0.0642\n",
      "Epoch 1498, Training RMSE: 0.0623, Validation RMSE: 0.0643\n",
      "Epoch 1499, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1500, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1501, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1502, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1503, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1504, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1505, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1506, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1507, Training RMSE: 0.0622, Validation RMSE: 0.0641\n",
      "Epoch 1508, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1509, Training RMSE: 0.0622, Validation RMSE: 0.0641\n",
      "Epoch 1510, Training RMSE: 0.0622, Validation RMSE: 0.0641\n",
      "Epoch 1511, Training RMSE: 0.0622, Validation RMSE: 0.0641\n",
      "Epoch 1512, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1513, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1514, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1515, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1516, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1517, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1518, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1519, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1520, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1521, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1522, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1523, Training RMSE: 0.0621, Validation RMSE: 0.0640\n",
      "Epoch 1524, Training RMSE: 0.0621, Validation RMSE: 0.0640\n",
      "Epoch 1525, Training RMSE: 0.0621, Validation RMSE: 0.0640\n",
      "Epoch 1526, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1527, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1528, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1529, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1530, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1531, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1532, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1533, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1534, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1535, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1536, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1537, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1538, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1539, Training RMSE: 0.0620, Validation RMSE: 0.0639\n",
      "Epoch 1540, Training RMSE: 0.0620, Validation RMSE: 0.0639\n",
      "Epoch 1541, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1542, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1543, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1544, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1545, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1546, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1547, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1548, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1549, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1550, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1551, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1552, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1553, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1554, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1555, Training RMSE: 0.0619, Validation RMSE: 0.0638\n",
      "Epoch 1556, Training RMSE: 0.0619, Validation RMSE: 0.0638\n",
      "Epoch 1557, Training RMSE: 0.0618, Validation RMSE: 0.0638\n",
      "Epoch 1558, Training RMSE: 0.0618, Validation RMSE: 0.0638\n",
      "Epoch 1559, Training RMSE: 0.0618, Validation RMSE: 0.0638\n",
      "Epoch 1560, Training RMSE: 0.0618, Validation RMSE: 0.0638\n",
      "Epoch 1561, Training RMSE: 0.0619, Validation RMSE: 0.0638\n",
      "Epoch 1562, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1563, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1564, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1565, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1566, Training RMSE: 0.0621, Validation RMSE: 0.0641\n",
      "Epoch 1567, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1568, Training RMSE: 0.0623, Validation RMSE: 0.0643\n",
      "Epoch 1569, Training RMSE: 0.0624, Validation RMSE: 0.0644\n",
      "Epoch 1570, Training RMSE: 0.0625, Validation RMSE: 0.0645\n",
      "Epoch 1571, Training RMSE: 0.0624, Validation RMSE: 0.0644\n",
      "Epoch 1572, Training RMSE: 0.0622, Validation RMSE: 0.0642\n",
      "Epoch 1573, Training RMSE: 0.0620, Validation RMSE: 0.0640\n",
      "Epoch 1574, Training RMSE: 0.0618, Validation RMSE: 0.0639\n",
      "Epoch 1575, Training RMSE: 0.0618, Validation RMSE: 0.0638\n",
      "Epoch 1576, Training RMSE: 0.0618, Validation RMSE: 0.0639\n",
      "Epoch 1577, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1578, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1579, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1580, Training RMSE: 0.0618, Validation RMSE: 0.0638\n",
      "Epoch 1581, Training RMSE: 0.0618, Validation RMSE: 0.0638\n",
      "Epoch 1582, Training RMSE: 0.0618, Validation RMSE: 0.0638\n",
      "Epoch 1583, Training RMSE: 0.0618, Validation RMSE: 0.0638\n",
      "Epoch 1584, Training RMSE: 0.0618, Validation RMSE: 0.0637\n",
      "Epoch 1585, Training RMSE: 0.0617, Validation RMSE: 0.0637\n",
      "Epoch 1586, Training RMSE: 0.0617, Validation RMSE: 0.0637\n",
      "Epoch 1587, Training RMSE: 0.0617, Validation RMSE: 0.0637\n",
      "Epoch 1588, Training RMSE: 0.0617, Validation RMSE: 0.0638\n",
      "Epoch 1589, Training RMSE: 0.0617, Validation RMSE: 0.0637\n",
      "Epoch 1590, Training RMSE: 0.0617, Validation RMSE: 0.0638\n",
      "Epoch 1591, Training RMSE: 0.0617, Validation RMSE: 0.0637\n",
      "Epoch 1592, Training RMSE: 0.0616, Validation RMSE: 0.0637\n",
      "Epoch 1593, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1594, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1595, Training RMSE: 0.0616, Validation RMSE: 0.0637\n",
      "Epoch 1596, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1597, Training RMSE: 0.0616, Validation RMSE: 0.0637\n",
      "Epoch 1598, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1599, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1600, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1601, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1602, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1603, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1604, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1605, Training RMSE: 0.0615, Validation RMSE: 0.0636\n",
      "Epoch 1606, Training RMSE: 0.0615, Validation RMSE: 0.0636\n",
      "Epoch 1607, Training RMSE: 0.0615, Validation RMSE: 0.0636\n",
      "Epoch 1608, Training RMSE: 0.0615, Validation RMSE: 0.0636\n",
      "Epoch 1609, Training RMSE: 0.0615, Validation RMSE: 0.0636\n",
      "Epoch 1610, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1611, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1612, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1613, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1614, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1615, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1616, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1617, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1618, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1619, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1620, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1621, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1622, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1623, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1624, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1625, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1626, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1627, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1628, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1629, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1630, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1631, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1632, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1633, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1634, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1635, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1636, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1637, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1638, Training RMSE: 0.0613, Validation RMSE: 0.0634\n",
      "Epoch 1639, Training RMSE: 0.0613, Validation RMSE: 0.0634\n",
      "Epoch 1640, Training RMSE: 0.0613, Validation RMSE: 0.0634\n",
      "Epoch 1641, Training RMSE: 0.0613, Validation RMSE: 0.0634\n",
      "Epoch 1642, Training RMSE: 0.0613, Validation RMSE: 0.0634\n",
      "Epoch 1643, Training RMSE: 0.0613, Validation RMSE: 0.0634\n",
      "Epoch 1644, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1645, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1646, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1647, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1648, Training RMSE: 0.0615, Validation RMSE: 0.0635\n",
      "Epoch 1649, Training RMSE: 0.0616, Validation RMSE: 0.0637\n",
      "Epoch 1650, Training RMSE: 0.0617, Validation RMSE: 0.0637\n",
      "Epoch 1651, Training RMSE: 0.0618, Validation RMSE: 0.0639\n",
      "Epoch 1652, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1653, Training RMSE: 0.0619, Validation RMSE: 0.0640\n",
      "Epoch 1654, Training RMSE: 0.0619, Validation RMSE: 0.0639\n",
      "Epoch 1655, Training RMSE: 0.0618, Validation RMSE: 0.0639\n",
      "Epoch 1656, Training RMSE: 0.0616, Validation RMSE: 0.0636\n",
      "Epoch 1657, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1658, Training RMSE: 0.0612, Validation RMSE: 0.0633\n",
      "Epoch 1659, Training RMSE: 0.0612, Validation RMSE: 0.0633\n",
      "Epoch 1660, Training RMSE: 0.0613, Validation RMSE: 0.0634\n",
      "Epoch 1661, Training RMSE: 0.0614, Validation RMSE: 0.0634\n",
      "Epoch 1662, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1663, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1664, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1665, Training RMSE: 0.0613, Validation RMSE: 0.0633\n",
      "Epoch 1666, Training RMSE: 0.0612, Validation RMSE: 0.0633\n",
      "Epoch 1667, Training RMSE: 0.0612, Validation RMSE: 0.0632\n",
      "Epoch 1668, Training RMSE: 0.0612, Validation RMSE: 0.0632\n",
      "Epoch 1669, Training RMSE: 0.0612, Validation RMSE: 0.0633\n",
      "Epoch 1670, Training RMSE: 0.0613, Validation RMSE: 0.0633\n",
      "Epoch 1671, Training RMSE: 0.0613, Validation RMSE: 0.0633\n",
      "Epoch 1672, Training RMSE: 0.0612, Validation RMSE: 0.0633\n",
      "Epoch 1673, Training RMSE: 0.0612, Validation RMSE: 0.0633\n",
      "Epoch 1674, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1675, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1676, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1677, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1678, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1679, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1680, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1681, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1682, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1683, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1684, Training RMSE: 0.0611, Validation RMSE: 0.0631\n",
      "Epoch 1685, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1686, Training RMSE: 0.0611, Validation RMSE: 0.0631\n",
      "Epoch 1687, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1688, Training RMSE: 0.0611, Validation RMSE: 0.0631\n",
      "Epoch 1689, Training RMSE: 0.0611, Validation RMSE: 0.0632\n",
      "Epoch 1690, Training RMSE: 0.0611, Validation RMSE: 0.0631\n",
      "Epoch 1691, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1692, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1693, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1694, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1695, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1696, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1697, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1698, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1699, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1700, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1701, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1702, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1703, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1704, Training RMSE: 0.0610, Validation RMSE: 0.0631\n",
      "Epoch 1705, Training RMSE: 0.0610, Validation RMSE: 0.0630\n",
      "Epoch 1706, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1707, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1708, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1709, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1710, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1711, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1712, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1713, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1714, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1715, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1716, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1717, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1718, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1719, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1720, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1721, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1722, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1723, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1724, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1725, Training RMSE: 0.0608, Validation RMSE: 0.0630\n",
      "Epoch 1726, Training RMSE: 0.0608, Validation RMSE: 0.0630\n",
      "Epoch 1727, Training RMSE: 0.0608, Validation RMSE: 0.0630\n",
      "Epoch 1728, Training RMSE: 0.0608, Validation RMSE: 0.0630\n",
      "Epoch 1729, Training RMSE: 0.0608, Validation RMSE: 0.0630\n",
      "Epoch 1730, Training RMSE: 0.0608, Validation RMSE: 0.0630\n",
      "Epoch 1731, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1732, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1733, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1734, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1735, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1736, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1737, Training RMSE: 0.0609, Validation RMSE: 0.0631\n",
      "Epoch 1738, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1739, Training RMSE: 0.0609, Validation RMSE: 0.0631\n",
      "Epoch 1740, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1741, Training RMSE: 0.0609, Validation RMSE: 0.0631\n",
      "Epoch 1742, Training RMSE: 0.0609, Validation RMSE: 0.0629\n",
      "Epoch 1743, Training RMSE: 0.0608, Validation RMSE: 0.0630\n",
      "Epoch 1744, Training RMSE: 0.0608, Validation RMSE: 0.0629\n",
      "Epoch 1745, Training RMSE: 0.0608, Validation RMSE: 0.0630\n",
      "Epoch 1746, Training RMSE: 0.0608, Validation RMSE: 0.0629\n",
      "Epoch 1747, Training RMSE: 0.0608, Validation RMSE: 0.0629\n",
      "Epoch 1748, Training RMSE: 0.0608, Validation RMSE: 0.0629\n",
      "Epoch 1749, Training RMSE: 0.0608, Validation RMSE: 0.0629\n",
      "Epoch 1750, Training RMSE: 0.0608, Validation RMSE: 0.0629\n",
      "Epoch 1751, Training RMSE: 0.0608, Validation RMSE: 0.0630\n",
      "Epoch 1752, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1753, Training RMSE: 0.0609, Validation RMSE: 0.0631\n",
      "Epoch 1754, Training RMSE: 0.0610, Validation RMSE: 0.0632\n",
      "Epoch 1755, Training RMSE: 0.0611, Validation RMSE: 0.0633\n",
      "Epoch 1756, Training RMSE: 0.0612, Validation RMSE: 0.0633\n",
      "Epoch 1757, Training RMSE: 0.0613, Validation RMSE: 0.0635\n",
      "Epoch 1758, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1759, Training RMSE: 0.0615, Validation RMSE: 0.0637\n",
      "Epoch 1760, Training RMSE: 0.0614, Validation RMSE: 0.0635\n",
      "Epoch 1761, Training RMSE: 0.0612, Validation RMSE: 0.0635\n",
      "Epoch 1762, Training RMSE: 0.0610, Validation RMSE: 0.0630\n",
      "Epoch 1763, Training RMSE: 0.0607, Validation RMSE: 0.0629\n",
      "Epoch 1764, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1765, Training RMSE: 0.0607, Validation RMSE: 0.0628\n",
      "Epoch 1766, Training RMSE: 0.0608, Validation RMSE: 0.0630\n",
      "Epoch 1767, Training RMSE: 0.0609, Validation RMSE: 0.0630\n",
      "Epoch 1768, Training RMSE: 0.0609, Validation RMSE: 0.0631\n",
      "Epoch 1769, Training RMSE: 0.0608, Validation RMSE: 0.0629\n",
      "Epoch 1770, Training RMSE: 0.0607, Validation RMSE: 0.0629\n",
      "Epoch 1771, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1772, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1773, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1774, Training RMSE: 0.0607, Validation RMSE: 0.0628\n",
      "Epoch 1775, Training RMSE: 0.0607, Validation RMSE: 0.0629\n",
      "Epoch 1776, Training RMSE: 0.0607, Validation RMSE: 0.0628\n",
      "Epoch 1777, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1778, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1779, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1780, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1781, Training RMSE: 0.0606, Validation RMSE: 0.0627\n",
      "Epoch 1782, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1783, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1784, Training RMSE: 0.0606, Validation RMSE: 0.0627\n",
      "Epoch 1785, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1786, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1787, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1788, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1789, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1790, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1791, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1792, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1793, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1794, Training RMSE: 0.0605, Validation RMSE: 0.0626\n",
      "Epoch 1795, Training RMSE: 0.0605, Validation RMSE: 0.0626\n",
      "Epoch 1796, Training RMSE: 0.0605, Validation RMSE: 0.0626\n",
      "Epoch 1797, Training RMSE: 0.0605, Validation RMSE: 0.0626\n",
      "Epoch 1798, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1799, Training RMSE: 0.0605, Validation RMSE: 0.0626\n",
      "Epoch 1800, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1801, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1802, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1803, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1804, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1805, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1806, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1807, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1808, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1809, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1810, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1811, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1812, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1813, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1814, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1815, Training RMSE: 0.0604, Validation RMSE: 0.0625\n",
      "Epoch 1816, Training RMSE: 0.0604, Validation RMSE: 0.0625\n",
      "Epoch 1817, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1818, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1819, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1820, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1821, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1822, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1823, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1824, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1825, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1826, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1827, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1828, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1829, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1830, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1831, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1832, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1833, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1834, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1835, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1836, Training RMSE: 0.0603, Validation RMSE: 0.0624\n",
      "Epoch 1837, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1838, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1839, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1840, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1841, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1842, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1843, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1844, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1845, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1846, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1847, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1848, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1849, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1850, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1851, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1852, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1853, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1854, Training RMSE: 0.0605, Validation RMSE: 0.0626\n",
      "Epoch 1855, Training RMSE: 0.0605, Validation RMSE: 0.0628\n",
      "Epoch 1856, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1857, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1858, Training RMSE: 0.0606, Validation RMSE: 0.0629\n",
      "Epoch 1859, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1860, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1861, Training RMSE: 0.0606, Validation RMSE: 0.0627\n",
      "Epoch 1862, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1863, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1864, Training RMSE: 0.0605, Validation RMSE: 0.0628\n",
      "Epoch 1865, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1866, Training RMSE: 0.0603, Validation RMSE: 0.0626\n",
      "Epoch 1867, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1868, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1869, Training RMSE: 0.0601, Validation RMSE: 0.0624\n",
      "Epoch 1870, Training RMSE: 0.0602, Validation RMSE: 0.0624\n",
      "Epoch 1871, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1872, Training RMSE: 0.0603, Validation RMSE: 0.0624\n",
      "Epoch 1873, Training RMSE: 0.0602, Validation RMSE: 0.0625\n",
      "Epoch 1874, Training RMSE: 0.0602, Validation RMSE: 0.0623\n",
      "Epoch 1875, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1876, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1877, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1878, Training RMSE: 0.0601, Validation RMSE: 0.0624\n",
      "Epoch 1879, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1880, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1881, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1882, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1883, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1884, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1885, Training RMSE: 0.0601, Validation RMSE: 0.0623\n",
      "Epoch 1886, Training RMSE: 0.0600, Validation RMSE: 0.0623\n",
      "Epoch 1887, Training RMSE: 0.0600, Validation RMSE: 0.0623\n",
      "Epoch 1888, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1889, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1890, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1891, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1892, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1893, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1894, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1895, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1896, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1897, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1898, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1899, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1900, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1901, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1902, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1903, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1904, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1905, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1906, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1907, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1908, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1909, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1910, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1911, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1912, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1913, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1914, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1915, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1916, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1917, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1918, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1919, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1920, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1921, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1922, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1923, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1924, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1925, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1926, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1927, Training RMSE: 0.0599, Validation RMSE: 0.0621\n",
      "Epoch 1928, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1929, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1930, Training RMSE: 0.0600, Validation RMSE: 0.0623\n",
      "Epoch 1931, Training RMSE: 0.0601, Validation RMSE: 0.0624\n",
      "Epoch 1932, Training RMSE: 0.0603, Validation RMSE: 0.0625\n",
      "Epoch 1933, Training RMSE: 0.0604, Validation RMSE: 0.0626\n",
      "Epoch 1934, Training RMSE: 0.0605, Validation RMSE: 0.0628\n",
      "Epoch 1935, Training RMSE: 0.0606, Validation RMSE: 0.0628\n",
      "Epoch 1936, Training RMSE: 0.0606, Validation RMSE: 0.0629\n",
      "Epoch 1937, Training RMSE: 0.0605, Validation RMSE: 0.0627\n",
      "Epoch 1938, Training RMSE: 0.0602, Validation RMSE: 0.0625\n",
      "Epoch 1939, Training RMSE: 0.0600, Validation RMSE: 0.0622\n",
      "Epoch 1940, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1941, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1942, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1943, Training RMSE: 0.0600, Validation RMSE: 0.0623\n",
      "Epoch 1944, Training RMSE: 0.0600, Validation RMSE: 0.0623\n",
      "Epoch 1945, Training RMSE: 0.0600, Validation RMSE: 0.0623\n",
      "Epoch 1946, Training RMSE: 0.0599, Validation RMSE: 0.0622\n",
      "Epoch 1947, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1948, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1949, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1950, Training RMSE: 0.0598, Validation RMSE: 0.0620\n",
      "Epoch 1951, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1952, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1953, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1954, Training RMSE: 0.0598, Validation RMSE: 0.0621\n",
      "Epoch 1955, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1956, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1957, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1958, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1959, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1960, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1961, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1962, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1963, Training RMSE: 0.0597, Validation RMSE: 0.0620\n",
      "Epoch 1964, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1965, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1966, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1967, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1968, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1969, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1970, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1971, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1972, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1973, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1974, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1975, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1976, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1977, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1978, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1979, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1980, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1981, Training RMSE: 0.0596, Validation RMSE: 0.0619\n",
      "Epoch 1982, Training RMSE: 0.0596, Validation RMSE: 0.0618\n",
      "Epoch 1983, Training RMSE: 0.0596, Validation RMSE: 0.0618\n",
      "Epoch 1984, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1985, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1986, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1987, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1988, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1989, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1990, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1991, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1992, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1993, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1994, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1995, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1996, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1997, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1998, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 1999, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Epoch 2000, Training RMSE: 0.0595, Validation RMSE: 0.0618\n",
      "Best Training RMSE after training: 0.05947504058191821\n",
      "Best Validation RMSE after training: 0.061777622911965015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/guoqiang/tmp/ipykernel_70632/310533576.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file))\n"
     ]
    }
   ],
   "source": [
    "model = train_nn_model_pytorch(x_all, y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b725d79-80c6-4c77-88fa-3fd923a6d11e",
   "metadata": {},
   "source": [
    "# Sklearn ANN using x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb7dd920-608e-4967-a9db-c0ec51fdd143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def train_nn_model_sklearn(x_all, y_all, hidden_layers=(100, 100), max_iter=1000, patience=10, random_state=42):\n",
    "    # Split the data into training and validation sets (10% for validation)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_all, y_all, test_size=0.1, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Initialize scalers for inputs and outputs\n",
    "    x_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "\n",
    "    # Normalize x_train and x_val\n",
    "    x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "    x_val_scaled = x_scaler.transform(x_val)\n",
    "\n",
    "    # Normalize y_train and y_val\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Initialize the MLPRegressor with early stopping\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layers,\n",
    "        max_iter=max_iter,\n",
    "        early_stopping=True,  # Enables internal validation-based early stopping\n",
    "        validation_fraction=0.1,  # Internal validation on 10% of the training set for early stopping\n",
    "        n_iter_no_change=patience,  # Stops if no improvement for 'patience' epochs\n",
    "        random_state=random_state,\n",
    "        solver=\"adam\"\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "    # Calculate RMSE on the validation set using model predictions\n",
    "    y_train_pred_scaled = model.predict(x_train_scaled)\n",
    "    y_val_pred_scaled = model.predict(x_val_scaled)\n",
    "\n",
    "    # Inverse transform the predicted and actual values to calculate RMSE in original scale\n",
    "    y_train_pred = y_scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_val_pred = y_scaler.inverse_transform(y_val_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_train_true = y_train\n",
    "    y_val_true = y_val\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
    "\n",
    "    # Output the best training and validation RMSE after training finishes\n",
    "    print(f'Best Training RMSE: {train_rmse}')\n",
    "    print(f'Best Validation RMSE: {val_rmse}')\n",
    "\n",
    "    # Return the trained model and scalers for further predictions\n",
    "    return model, x_scaler, y_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5b4b430-2ec3-46c4-946b-591342955bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training RMSE: 0.04352850613978999\n",
      "Best Validation RMSE: 0.048807726740472394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLPRegressor(early_stopping=True, hidden_layer_sizes=(100, 100), max_iter=1000,\n",
       "              random_state=42),\n",
       " StandardScaler(),\n",
       " StandardScaler())"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn_model_sklearn(x_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6f561-f0ea-49c6-9cec-bfcb004a1bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PytorchEnv]",
   "language": "python",
   "name": "conda-env-PytorchEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
